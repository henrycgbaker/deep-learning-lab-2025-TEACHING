# GRAD-E1394: Deep Learning Labs 2025

Graduate-level lab materials covering deep learning fundamentals through modern architectures. Each lab builds practical skills with hands-on PyTorch implementations.

## Labs

| Lab | Notebook | Topics |
|-----|----------|--------|
| 1 | [lab_1_python_foundations.ipynb](lab_1_python_foundations.ipynb) | Type hints, NumPy, PyTorch tensors, autograd |
| 2 | [lab_2_intro_to_ffnns.ipynb](lab_2_intro_to_ffnns.ipynb) | Feed-forward networks, backpropagation, MNIST |
| 3 | [lab_3_tensorboard_vanishing_gradients_hyperparams.ipynb](lab_3_tensorboard_vanishing_gradients_hyperparams.ipynb) | TensorBoard, vanishing gradients, hyperparameter tuning |
| 4 | [lab_4_introduction_to_cnns.ipynb](lab_4_introduction_to_cnns.ipynb) | Convolutional layers, pooling, image classification |
| 5 | [lab_5_advanced_cnns.ipynb](lab_5_advanced_cnns.ipynb) | ResNet, skip connections, transfer learning, class imbalance |
| 6 | [lab_6_rnn_foundations.ipynb](lab_6_rnn_foundations.ipynb) | Vanilla RNNs, BPTT, character-level language models |
| 7 | [lab_7_rnn_adv.ipynb](lab_7_rnn_adv.ipynb) | LSTM, GRU, gating mechanisms |
| 8 | [lab_8_seq2seq_attention.ipynb](lab_8_seq2seq_attention.ipynb) | Encoder-decoder, attention mechanisms, translation |
| 9 | [lab_9_transformers.ipynb](lab_9_transformers.ipynb) | Self-attention, transformers, positional encoding |
| 10 | [lab_10_mlops_finetuning_evaluation.ipynb](lab_10_mlops_finetuning_evaluation.ipynb) | HuggingFace, fine-tuning, MLOps practices |

## Problem Sets

Theory problem sets with solutions are available in [`problem_sets/`](problem_sets/).

## Prerequisites

- Python programming (functions, classes, list comprehensions)
- Linear algebra (matrices, vectors, dot products)
- Calculus (derivatives, chain rule)
- Basic probability and statistics

## Frameworks

- **PyTorch** — primary deep learning framework
- **HuggingFace Transformers** — pre-trained models (Labs 9-10)
- **TensorBoard** — training visualisation
