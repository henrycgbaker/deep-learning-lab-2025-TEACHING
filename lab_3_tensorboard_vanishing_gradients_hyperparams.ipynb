{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: TensorBoard, Vanishing Gradients & Hyperparameter Tuning\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "1. Use TensorBoard to monitor and visualise training metrics\n",
    "2. Explain mathematically why vanishing gradients occur in deep networks\n",
    "3. Diagnose vanishing gradients using gradient norm analysis\n",
    "4. Apply solutions: ReLU, batch normalisation, careful initialisation\n",
    "5. Use Optuna for automated hyperparameter optimisation\n",
    "\n",
    "**Prerequisites:** Lab 2 (FFNNs, backpropagation basics)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#",
    " ",
    "=",
    "=",
    "=",
    "=",
    " ",
    "E",
    "n",
    "v",
    "i",
    "r",
    "o",
    "n",
    "m",
    "e",
    "n",
    "t",
    " ",
    "S",
    "e",
    "t",
    "u",
    "p",
    " ",
    "=",
    "=",
    "=",
    "=",
    "\n",
    "#",
    " ",
    "D",
    "e",
    "t",
    "e",
    "c",
    "t",
    "s",
    " ",
    "C",
    "o",
    "l",
    "a",
    "b",
    " ",
    "v",
    "s",
    " ",
    "l",
    "o",
    "c",
    "a",
    "l",
    " ",
    "a",
    "n",
    "d",
    " ",
    "p",
    "r",
    "o",
    "v",
    "i",
    "d",
    "e",
    "s",
    " ",
    "c",
    "r",
    "o",
    "s",
    "s",
    "-",
    "p",
    "l",
    "a",
    "t",
    "f",
    "o",
    "r",
    "m",
    " ",
    "u",
    "t",
    "i",
    "l",
    "i",
    "t",
    "i",
    "e",
    "s",
    "\n",
    "\n",
    "i",
    "m",
    "p",
    "o",
    "r",
    "t",
    " ",
    "o",
    "s",
    "\n",
    "i",
    "m",
    "p",
    "o",
    "r",
    "t",
    " ",
    "s",
    "y",
    "s",
    "\n",
    "\n",
    "#",
    " ",
    "D",
    "e",
    "t",
    "e",
    "c",
    "t",
    " ",
    "e",
    "n",
    "v",
    "i",
    "r",
    "o",
    "n",
    "m",
    "e",
    "n",
    "t",
    "\n",
    "I",
    "N",
    "_",
    "C",
    "O",
    "L",
    "A",
    "B",
    " ",
    "=",
    " ",
    "'",
    "g",
    "o",
    "o",
    "g",
    "l",
    "e",
    ".",
    "c",
    "o",
    "l",
    "a",
    "b",
    "'",
    " ",
    "i",
    "n",
    " ",
    "s",
    "y",
    "s",
    ".",
    "m",
    "o",
    "d",
    "u",
    "l",
    "e",
    "s",
    "\n",
    "\n",
    "i",
    "f",
    " ",
    "I",
    "N",
    "_",
    "C",
    "O",
    "L",
    "A",
    "B",
    ":",
    "\n",
    " ",
    " ",
    " ",
    " ",
    "p",
    "r",
    "i",
    "n",
    "t",
    "(",
    "\"",
    "\u2713",
    " ",
    "R",
    "u",
    "n",
    "n",
    "i",
    "n",
    "g",
    " ",
    "o",
    "n",
    " ",
    "G",
    "o",
    "o",
    "g",
    "l",
    "e",
    " ",
    "C",
    "o",
    "l",
    "a",
    "b",
    "\"",
    ")",
    "\n",
    "e",
    "l",
    "s",
    "e",
    ":",
    "\n",
    " ",
    " ",
    " ",
    " ",
    "p",
    "r",
    "i",
    "n",
    "t",
    "(",
    "\"",
    "\u2713",
    " ",
    "R",
    "u",
    "n",
    "n",
    "i",
    "n",
    "g",
    " ",
    "l",
    "o",
    "c",
    "a",
    "l",
    "l",
    "y",
    "\"",
    ")",
    "\n",
    "\n",
    "d",
    "e",
    "f",
    " ",
    "d",
    "o",
    "w",
    "n",
    "l",
    "o",
    "a",
    "d",
    "_",
    "f",
    "i",
    "l",
    "e",
    "(",
    "u",
    "r",
    "l",
    ":",
    " ",
    "s",
    "t",
    "r",
    ",",
    " ",
    "f",
    "i",
    "l",
    "e",
    "n",
    "a",
    "m",
    "e",
    ":",
    " ",
    "s",
    "t",
    "r",
    ")",
    " ",
    "-",
    ">",
    " ",
    "s",
    "t",
    "r",
    ":",
    "\n",
    " ",
    " ",
    " ",
    " ",
    "\"",
    "\"",
    "\"",
    "D",
    "o",
    "w",
    "n",
    "l",
    "o",
    "a",
    "d",
    " ",
    "f",
    "i",
    "l",
    "e",
    " ",
    "i",
    "f",
    " ",
    "i",
    "t",
    " ",
    "d",
    "o",
    "e",
    "s",
    "n",
    "'",
    "t",
    " ",
    "e",
    "x",
    "i",
    "s",
    "t",
    ".",
    " ",
    "W",
    "o",
    "r",
    "k",
    "s",
    " ",
    "o",
    "n",
    " ",
    "b",
    "o",
    "t",
    "h",
    " ",
    "C",
    "o",
    "l",
    "a",
    "b",
    " ",
    "a",
    "n",
    "d",
    " ",
    "l",
    "o",
    "c",
    "a",
    "l",
    ".",
    "\"",
    "\"",
    "\"",
    "\n",
    " ",
    " ",
    " ",
    " ",
    "i",
    "f",
    " ",
    "o",
    "s",
    ".",
    "p",
    "a",
    "t",
    "h",
    ".",
    "e",
    "x",
    "i",
    "s",
    "t",
    "s",
    "(",
    "f",
    "i",
    "l",
    "e",
    "n",
    "a",
    "m",
    "e",
    ")",
    ":",
    "\n",
    " ",
    " ",
    " ",
    " ",
    " ",
    " ",
    " ",
    " ",
    "p",
    "r",
    "i",
    "n",
    "t",
    "(",
    "f",
    "\"",
    "\u2713",
    " ",
    "{",
    "f",
    "i",
    "l",
    "e",
    "n",
    "a",
    "m",
    "e",
    "}",
    " ",
    "a",
    "l",
    "r",
    "e",
    "a",
    "d",
    "y",
    " ",
    "e",
    "x",
    "i",
    "s",
    "t",
    "s",
    "\"",
    ")",
    "\n",
    " ",
    " ",
    " ",
    " ",
    " ",
    " ",
    " ",
    " ",
    "r",
    "e",
    "t",
    "u",
    "r",
    "n",
    " ",
    "f",
    "i",
    "l",
    "e",
    "n",
    "a",
    "m",
    "e",
    "\n",
    " ",
    " ",
    " ",
    " ",
    "\n",
    " ",
    " ",
    " ",
    " ",
    "p",
    "r",
    "i",
    "n",
    "t",
    "(",
    "f",
    "\"",
    "D",
    "o",
    "w",
    "n",
    "l",
    "o",
    "a",
    "d",
    "i",
    "n",
    "g",
    " ",
    "{",
    "f",
    "i",
    "l",
    "e",
    "n",
    "a",
    "m",
    "e",
    "}",
    ".",
    ".",
    ".",
    "\"",
    ")",
    "\n",
    " ",
    " ",
    " ",
    " ",
    "i",
    "f",
    " ",
    "I",
    "N",
    "_",
    "C",
    "O",
    "L",
    "A",
    "B",
    ":",
    "\n",
    " ",
    " ",
    " ",
    " ",
    " ",
    " ",
    " ",
    " ",
    "i",
    "m",
    "p",
    "o",
    "r",
    "t",
    " ",
    "s",
    "u",
    "b",
    "p",
    "r",
    "o",
    "c",
    "e",
    "s",
    "s",
    "\n",
    " ",
    " ",
    " ",
    " ",
    " ",
    " ",
    " ",
    " ",
    "s",
    "u",
    "b",
    "p",
    "r",
    "o",
    "c",
    "e",
    "s",
    "s",
    ".",
    "r",
    "u",
    "n",
    "(",
    "[",
    "'",
    "w",
    "g",
    "e",
    "t",
    "'",
    ",",
    " ",
    "'",
    "-",
    "q",
    "'",
    ",",
    " ",
    "u",
    "r",
    "l",
    ",",
    " ",
    "'",
    "-",
    "O",
    "'",
    ",",
    " ",
    "f",
    "i",
    "l",
    "e",
    "n",
    "a",
    "m",
    "e",
    "]",
    ",",
    " ",
    "c",
    "h",
    "e",
    "c",
    "k",
    "=",
    "T",
    "r",
    "u",
    "e",
    ")",
    "\n",
    " ",
    " ",
    " ",
    " ",
    "e",
    "l",
    "s",
    "e",
    ":",
    "\n",
    " ",
    " ",
    " ",
    " ",
    " ",
    " ",
    " ",
    " ",
    "i",
    "m",
    "p",
    "o",
    "r",
    "t",
    " ",
    "u",
    "r",
    "l",
    "l",
    "i",
    "b",
    ".",
    "r",
    "e",
    "q",
    "u",
    "e",
    "s",
    "t",
    "\n",
    " ",
    " ",
    " ",
    " ",
    " ",
    " ",
    " ",
    " ",
    "u",
    "r",
    "l",
    "l",
    "i",
    "b",
    ".",
    "r",
    "e",
    "q",
    "u",
    "e",
    "s",
    "t",
    ".",
    "u",
    "r",
    "l",
    "r",
    "e",
    "t",
    "r",
    "i",
    "e",
    "v",
    "e",
    "(",
    "u",
    "r",
    "l",
    ",",
    " ",
    "f",
    "i",
    "l",
    "e",
    "n",
    "a",
    "m",
    "e",
    ")",
    "\n",
    " ",
    " ",
    " ",
    " ",
    "p",
    "r",
    "i",
    "n",
    "t",
    "(",
    "f",
    "\"",
    "\u2713",
    " ",
    "D",
    "o",
    "w",
    "n",
    "l",
    "o",
    "a",
    "d",
    "e",
    "d",
    " ",
    "{",
    "f",
    "i",
    "l",
    "e",
    "n",
    "a",
    "m",
    "e",
    "}",
    "\"",
    ")",
    "\n",
    " ",
    " ",
    " ",
    " ",
    "r",
    "e",
    "t",
    "u",
    "r",
    "n",
    " ",
    "f",
    "i",
    "l",
    "e",
    "n",
    "a",
    "m",
    "e"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#",
    " ",
    "=",
    "=",
    "=",
    "=",
    " ",
    "D",
    "e",
    "v",
    "i",
    "c",
    "e",
    " ",
    "S",
    "e",
    "t",
    "u",
    "p",
    " ",
    "=",
    "=",
    "=",
    "=",
    "\n",
    "i",
    "m",
    "p",
    "o",
    "r",
    "t",
    " ",
    "t",
    "o",
    "r",
    "c",
    "h",
    "\n",
    "\n",
    "d",
    "e",
    "f",
    " ",
    "g",
    "e",
    "t",
    "_",
    "d",
    "e",
    "v",
    "i",
    "c",
    "e",
    "(",
    ")",
    ":",
    "\n",
    " ",
    " ",
    " ",
    " ",
    "\"",
    "\"",
    "\"",
    "G",
    "e",
    "t",
    " ",
    "b",
    "e",
    "s",
    "t",
    " ",
    "a",
    "v",
    "a",
    "i",
    "l",
    "a",
    "b",
    "l",
    "e",
    " ",
    "d",
    "e",
    "v",
    "i",
    "c",
    "e",
    ":",
    " ",
    "C",
    "U",
    "D",
    "A",
    " ",
    ">",
    " ",
    "M",
    "P",
    "S",
    " ",
    ">",
    " ",
    "C",
    "P",
    "U",
    ".",
    "\"",
    "\"",
    "\"",
    "\n",
    " ",
    " ",
    " ",
    " ",
    "i",
    "f",
    " ",
    "t",
    "o",
    "r",
    "c",
    "h",
    ".",
    "c",
    "u",
    "d",
    "a",
    ".",
    "i",
    "s",
    "_",
    "a",
    "v",
    "a",
    "i",
    "l",
    "a",
    "b",
    "l",
    "e",
    "(",
    ")",
    ":",
    "\n",
    " ",
    " ",
    " ",
    " ",
    " ",
    " ",
    " ",
    " ",
    "d",
    "e",
    "v",
    "i",
    "c",
    "e",
    " ",
    "=",
    " ",
    "t",
    "o",
    "r",
    "c",
    "h",
    ".",
    "d",
    "e",
    "v",
    "i",
    "c",
    "e",
    "(",
    "'",
    "c",
    "u",
    "d",
    "a",
    "'",
    ")",
    "\n",
    " ",
    " ",
    " ",
    " ",
    " ",
    " ",
    " ",
    " ",
    "p",
    "r",
    "i",
    "n",
    "t",
    "(",
    "f",
    "\"",
    "\u2713",
    " ",
    "U",
    "s",
    "i",
    "n",
    "g",
    " ",
    "C",
    "U",
    "D",
    "A",
    " ",
    "G",
    "P",
    "U",
    ":",
    " ",
    "{",
    "t",
    "o",
    "r",
    "c",
    "h",
    ".",
    "c",
    "u",
    "d",
    "a",
    ".",
    "g",
    "e",
    "t",
    "_",
    "d",
    "e",
    "v",
    "i",
    "c",
    "e",
    "_",
    "n",
    "a",
    "m",
    "e",
    "(",
    "0",
    ")",
    "}",
    "\"",
    ")",
    "\n",
    " ",
    " ",
    " ",
    " ",
    "e",
    "l",
    "i",
    "f",
    " ",
    "h",
    "a",
    "s",
    "a",
    "t",
    "t",
    "r",
    "(",
    "t",
    "o",
    "r",
    "c",
    "h",
    ".",
    "b",
    "a",
    "c",
    "k",
    "e",
    "n",
    "d",
    "s",
    ",",
    " ",
    "'",
    "m",
    "p",
    "s",
    "'",
    ")",
    " ",
    "a",
    "n",
    "d",
    " ",
    "t",
    "o",
    "r",
    "c",
    "h",
    ".",
    "b",
    "a",
    "c",
    "k",
    "e",
    "n",
    "d",
    "s",
    ".",
    "m",
    "p",
    "s",
    ".",
    "i",
    "s",
    "_",
    "a",
    "v",
    "a",
    "i",
    "l",
    "a",
    "b",
    "l",
    "e",
    "(",
    ")",
    ":",
    "\n",
    " ",
    " ",
    " ",
    " ",
    " ",
    " ",
    " ",
    " ",
    "d",
    "e",
    "v",
    "i",
    "c",
    "e",
    " ",
    "=",
    " ",
    "t",
    "o",
    "r",
    "c",
    "h",
    ".",
    "d",
    "e",
    "v",
    "i",
    "c",
    "e",
    "(",
    "'",
    "m",
    "p",
    "s",
    "'",
    ")",
    "\n",
    " ",
    " ",
    " ",
    " ",
    " ",
    " ",
    " ",
    " ",
    "p",
    "r",
    "i",
    "n",
    "t",
    "(",
    "\"",
    "\u2713",
    " ",
    "U",
    "s",
    "i",
    "n",
    "g",
    " ",
    "A",
    "p",
    "p",
    "l",
    "e",
    " ",
    "M",
    "P",
    "S",
    " ",
    "(",
    "M",
    "e",
    "t",
    "a",
    "l",
    ")",
    "\"",
    ")",
    "\n",
    " ",
    " ",
    " ",
    " ",
    "e",
    "l",
    "s",
    "e",
    ":",
    "\n",
    " ",
    " ",
    " ",
    " ",
    " ",
    " ",
    " ",
    " ",
    "d",
    "e",
    "v",
    "i",
    "c",
    "e",
    " ",
    "=",
    " ",
    "t",
    "o",
    "r",
    "c",
    "h",
    ".",
    "d",
    "e",
    "v",
    "i",
    "c",
    "e",
    "(",
    "'",
    "c",
    "p",
    "u",
    "'",
    ")",
    "\n",
    " ",
    " ",
    " ",
    " ",
    " ",
    " ",
    " ",
    " ",
    "p",
    "r",
    "i",
    "n",
    "t",
    "(",
    "\"",
    "\u2713",
    " ",
    "U",
    "s",
    "i",
    "n",
    "g",
    " ",
    "C",
    "P",
    "U",
    "\"",
    ")",
    "\n",
    " ",
    " ",
    " ",
    " ",
    "r",
    "e",
    "t",
    "u",
    "r",
    "n",
    " ",
    "d",
    "e",
    "v",
    "i",
    "c",
    "e",
    "\n",
    "\n",
    "D",
    "E",
    "V",
    "I",
    "C",
    "E",
    " ",
    "=",
    " ",
    "g",
    "e",
    "t",
    "_",
    "d",
    "e",
    "v",
    "i",
    "c",
    "e",
    "(",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Loading the data\n",
    "\n",
    "Again, we will be using PyTorch library/framework for this lab, following on from last week"
   ],
   "metadata": {
    "id": "THR6SWeqMYR7"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# import basic libs\n",
    "import pandas as pd, numpy as np\n",
    "import matplotlib.pyplot as plt, seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.display import display, clear_output\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# import torch (whole lib & specific modules)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "torch.set_default_dtype(torch.double)\n",
    "\n",
    "seed = 42"
   ],
   "metadata": {
    "id": "vW8t4MQUpOfM"
   },
   "execution_count": 23,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# import dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "training_data = datasets.FashionMNIST( # MNIST for image classification\n",
    "    root=\"data\", # specifies directory\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(), # converts images from PIL format (or numpy array) to PyTorch Tensor (fundamental data structure for PyTorch)\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False, #\u00a0NB\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ],
   "metadata": {
    "id": "ce8u3BGUxceR"
   },
   "execution_count": 24,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# there is unncessary amount of data!\n",
    "print(f\"len of training dataset: {len(training_data)}, len of testing dataset: {len(test_data)}\")"
   ],
   "metadata": {
    "id": "cwYTKZJ_xmCp",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "39fb9b80-5665-4a6d-9563-242b6bfa4e08"
   },
   "execution_count": 25,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "len of training dataset: 60000, len of testing dataset: 10000\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# extract a subset of the datasets\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "training_data, _ = random_split(training_data, # arg: original dataset to split\n",
    "                                [1/10, 9/10], # split props\n",
    "                                generator=torch.manual_seed(seed)) # reproducibility\n",
    "\n",
    "test_data, _ = random_split(test_data, [1/10, 9/10], generator=torch.manual_seed(seed))\n",
    "\n",
    "print(f\"len of training dataset: {len(training_data)}, len of testing dataset: {len(test_data)}\")"
   ],
   "metadata": {
    "id": "M4vLfzSCu7vU",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "24d8d8bc-5e1e-4754-ad45-fa13aeab8554"
   },
   "execution_count": 26,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "len of training dataset: 6000, len of testing dataset: 1000\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Constructing the model using a generic class\n",
    "\n",
    "As last week, we construct a lazy sequential model with a customized number of `hidden_layers`, of dimensions provided by list object `hidden_size`.\n",
    "\n",
    "Whereas last week predefined a classification task -> only required `input_dims` as an arg; here we define a more generic architecture that can be either regr / classification -> requires `out_feat_size` that can take any hidden dims size."
   ],
   "metadata": {
    "id": "GT5-DpSRT1FR"
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "UiiTXAKh2QzX"
   }
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class LazySequential(nn.Sequential):\n",
    "    \"\"\"Flexible sequential model with configurable hidden layers and activations.\"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 in_feat_size: int,\n",
    "                 out_feat_size: int,\n",
    "                 hidden_sizes: list,\n",
    "                 activation_fn: str = 'ReLU'):\n",
    "        \n",
    "        layers = [nn.Flatten()]  # Flatten input image to vector\n",
    "        \n",
    "        # Add hidden layers with activations\n",
    "        for idx, size in enumerate(hidden_sizes):\n",
    "            in_features = in_feat_size if idx == 0 else hidden_sizes[idx-1]\n",
    "            layers.append(nn.Linear(in_features, size))\n",
    "            layers.append(getattr(nn, activation_fn)())\n",
    "        \n",
    "        # Output layer (no activation - handled by loss function)\n",
    "        layers.append(nn.Linear(hidden_sizes[-1], out_feat_size))\n",
    "        \n",
    "        super().__init__(*layers)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "So now we have a generic class for definiing architecture upto the output activation function! We haven't applied a final activation function, because in PyTorch this is handled by the selected loss function (here, `CrossEntropyLoss`)\n",
    "\n",
    "Next, we need to define the training function; here our loss function will contain the final activations fn - this modularity allows us to define a generic architecture to be put to multiple end use cases.\n",
    "\n",
    "We will also feed our per-batch training loss (already calculated in the training loop) to a TensorBoard for visualisation."
   ],
   "metadata": {
    "id": "MRfGvJ0CjjsW"
   }
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import datetime\n",
    "\n",
    "def torch_train(model,\n",
    "                data,\n",
    "                optimizer='Adam',\n",
    "                loss_fn='CrossEntropyLoss',  # Renamed to avoid shadowing\n",
    "                batch_size=64,\n",
    "                epochs=1,\n",
    "                shuffle=False,\n",
    "                logdir=None):\n",
    "    \"\"\"Train a PyTorch model with TensorBoard logging.\"\"\"\n",
    "    \n",
    "    criterion = getattr(nn, loss_fn)()\n",
    "    opt = getattr(optim, optimizer)(model.parameters())\n",
    "    dataloader = DataLoader(data, batch_size=batch_size, shuffle=shuffle)\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    # Create unique log directory\n",
    "    if logdir is None:\n",
    "        current_time = datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "        logdir = f'runs/{current_time}/train'\n",
    "    \n",
    "    logger = SummaryWriter(logdir)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx, (X, Y) in enumerate(tqdm(dataloader, desc=f'Epoch {epoch+1}')):\n",
    "            opt.zero_grad()\n",
    "            pred = model(X)\n",
    "            loss = criterion(pred.squeeze(-1), Y.long())\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            \n",
    "            global_step = batch_idx + len(dataloader) * epoch\n",
    "            logger.add_scalar('Training loss per batch', loss.item(), global_step)\n",
    "    \n",
    "    logger.close()\n",
    "    return model\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Instantiate our models"
   ],
   "metadata": {
    "id": "0J16wWfcvcub"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# define 2 architectures\n",
    "hidden_layers1 = [2**10, 2**5, 2**3]\n",
    "hidden_layers2 = [2**10, 2**7, 2**5]\n",
    "\n",
    "# model1\n",
    "model1 = LazySequential(in_feat_size=28*28,\n",
    "                        out_feat_size=10,\n",
    "                        hidden_sizes=hidden_layers1)\n",
    "model1 = torch_train(model1,\n",
    "                     training_data,\n",
    "                     logdir='runs/models1_2/model1')\n",
    "\n",
    "# model 2\n",
    "model2 = LazySequential(in_feat_size=28*28, out_feat_size=10, hidden_sizes=hidden_layers2)\n",
    "model2 = torch_train(model2, training_data, logdir='runs/models1_2/model2')"
   ],
   "metadata": {
    "id": "p9Y_xgNtne3E"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TensorBoard(s)\n",
    "\n",
    "[Tensorboard](https://www.tensorflow.org/tensorboard) is a visualisation tool provided by TensorFlow that allows you to visualise our ML training process -> helps us understand how our model is training and to diagnose issues. Can incl metrics like:\n",
    "- loss\n",
    "- accuracy,\n",
    "- model graphs,\n",
    "- other data (all customisable).\n",
    "\n",
    "TensorBoard is just the visualisation dashboard; we need to generate and send the metrics to it. Tensorboard integrates w/ libs PyTorch (here: SummaryWriter) to calculate and log the specific metrics we want to see during your training process (i.e. we need to calculate the logged metrics in our training / eval loops)."
   ],
   "metadata": {
    "id": "gD2XQwPVYEhR"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# colab magic command -> loads IPython extension\n",
    "# tells tensorboard to look for log files in in `runs/models1_2` dir\n",
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs/models1_2"
   ],
   "metadata": {
    "id": "nlwVX9_33xQC"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "This TensorBoard shows us the Training Loss that we calculated as a by-product of our training process; but it might be more informative to know how the average epoch loss changed over time.\n",
    "\n",
    "Let's develop our TensorBoard to visualise this. We'll need to calculate this new metric and pass it to the Torch's SummaryWriter object within the training function."
   ],
   "metadata": {
    "id": "oTGSsw_7bdFh"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# @title Modifying training to log avg epoch loss and total elapsed time per epoch.\n",
    "\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def torch_train_mod(model, data, optimizer='Adam', loss='CrossEntropyLoss', batch_size=2**6, epochs=1, shuffle=False, logdir=None):\n",
    "\n",
    "    criterion = getattr(nn, loss)()\n",
    "    optimizer = getattr(optim, optimizer)(model.parameters())\n",
    "    dataloader = DataLoader(data, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    logdir = '//'.join(['runs', current_time, 'train']) if logdir is None else logdir\n",
    "    logger = SummaryWriter(logdir)\n",
    "\n",
    "\n",
    "    start = time.time() # <- NEW: returns current time as float\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0 # <- NEW: init epoch loss\n",
    "        i = 0\n",
    "        for (X, Y) in tqdm(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(X)\n",
    "            loss = criterion(pred.squeeze(-1), torch.tensor(Y))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss+=loss.item() # <- NEW: sum of epoch loss\n",
    "            i+=1\n",
    "            logger.add_scalar('Training loss per batch', loss, i + len(dataloader) * epoch )\n",
    "\n",
    "        # new:\n",
    "        logger.add_scalar('Avg epoch loss',  # <- NEW comp avg\n",
    "                        epoch_loss / len(dataloader),\n",
    "                        epoch) # before was batch\n",
    "        logger.add_scalar('Total training time', time.time() - start, epoch)\n",
    "\n",
    "    logger.flush() # forces buffered data to disk\n",
    "    logger.close()\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "id": "m36Os3uevH9q"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Online learning, minibatch and batch\n",
    "\n",
    "With our training function set up to visualise how loss changes over epoch, let's visualise three common batch learning strategies.\n",
    "\n",
    "- **Online learning**: Processes one sample at a time, updating model weights after each individual example.\n",
    "- **Minibatch learning**: Processes minibatches (obviously).\n",
    "- **Batch learning** (or Full-batch learning): Processes the entire dataset at once, computing gradients over all samples before updating weights."
   ],
   "metadata": {
    "id": "Jsyj1KDJPF62"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "<summary><b>\ud83e\udd14 Question:</b> Why does online learning show more noise but potentially faster initial progress?</summary>\n",
    "\n",
    "**Answer:** \n",
    "- **More noise**: Each gradient is computed from a single sample, so it's a high-variance estimate of the true gradient\n",
    "- **Faster initial progress**: Updates happen after every sample (many updates per epoch), allowing the model to escape poor initialisations quickly\n",
    "- **Trade-off**: The noise can help escape local minima but makes convergence to a precise minimum harder\n",
    "\n",
    "Minibatch provides a balance: lower variance than online, more updates per epoch than full-batch.\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# define our learning types\n",
    "names = ['online', 'minibatch', 'fullbatch']\n",
    "batch_sizes = [1, 2**6, len(training_data)]\n",
    "\n",
    "# make one timestamped parent directory for this batch of runs\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "parent_logdir = f\"runs/batches/{timestamp}\"\n",
    "\n",
    "# first instatiate the model, then feed that model to the training function\n",
    "for name, batch_size in zip(names, batch_sizes): # zip combnes our two lists into an iterator of tuples\n",
    "    batch_model = LazySequential(\n",
    "        in_feat_size=28*28, # model is generic across batch types\n",
    "        out_feat_size=10,\n",
    "        hidden_sizes=hidden_layers1 # Using hidden_layers1 for consistency\n",
    "    )\n",
    "    logdir = f\"{parent_logdir}/{name}\"   # <- now grouped under timestamp\n",
    "    batch_model = torch_train_mod(\n",
    "        batch_model,\n",
    "        training_data,\n",
    "        batch_size=batch_size, # this changes within the loop\n",
    "        epochs=2,\n",
    "        logdir=logdir\n",
    "    )\n",
    "\n",
    "print(\"Logs saved to:\", parent_logdir)"
   ],
   "metadata": {
    "id": "fK1dMXEaS82h"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%tensorboard --logdir {parent_logdir}"
   ],
   "metadata": {
    "id": "qKfM_hUYeeyG"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Vanishing Gradient Problem\n",
    "\n",
    "## Mathematical Foundation\n",
    "\n",
    "### Backpropagation and the Chain Rule\n",
    "\n",
    "For a network with $L$ layers, the gradient of the loss with respect to weights in layer $l$ is:\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial W^{(l)}} = \\frac{\\partial \\mathcal{L}}{\\partial a^{(L)}} \\cdot \\prod_{k=l+1}^{L} \\frac{\\partial a^{(k)}}{\\partial a^{(k-1)}} \\cdot \\frac{\\partial a^{(l)}}{\\partial W^{(l)}}$$\n",
    "\n",
    "Each term $\\frac{\\partial a^{(k)}}{\\partial a^{(k-1)}} = \\sigma'(z^{(k)}) \\cdot W^{(k)}$ involves the **activation derivative**.\n",
    "\n",
    "### Sigmoid Derivative Derivation\n",
    "\n",
    "For sigmoid: $\\sigma(z) = \\frac{1}{1 + e^{-z}}$\n",
    "\n",
    "**Step 1:** Apply quotient rule:\n",
    "$$\\sigma'(z) = \\frac{e^{-z}}{(1 + e^{-z})^2}$$\n",
    "\n",
    "**Step 2:** Rewrite in terms of $\\sigma(z)$:\n",
    "$$\\sigma'(z) = \\sigma(z)(1 - \\sigma(z))$$\n",
    "\n",
    "**Step 3:** Find maximum:\n",
    "- $\\sigma(z) \\in (0, 1)$, so $\\sigma'(z) = \\sigma(1-\\sigma)$ is maximised when $\\sigma = 0.5$\n",
    "- Maximum value: $0.5 \\times 0.5 = 0.25$\n",
    "\n",
    "$$\\boxed{\\sigma'(z) \\leq 0.25 \\text{ for all } z}$$\n",
    "\n",
    "<details>\n",
    "<summary><b>\ud83e\udd14 Question:</b> What is the maximum value of tanh'(z)? Where does it occur?</summary>\n",
    "\n",
    "**Answer:** For $\\tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}$, the derivative is $\\tanh'(z) = 1 - \\tanh^2(z)$.\n",
    "\n",
    "Maximum occurs at $z = 0$ where $\\tanh(0) = 0$, giving $\\tanh'(0) = 1$.\n",
    "\n",
    "This is better than sigmoid (max 0.25), but still causes vanishing gradients because $\\tanh'(z) < 1$ for all $z \\neq 0$.\n",
    "</details>\n",
    "\n",
    "### Gradient Attenuation Through Depth\n",
    "\n",
    "For a network with $L$ sigmoid layers:\n",
    "\n",
    "$$\\left|\\frac{\\partial \\mathcal{L}}{\\partial W^{(1)}}\\right| \\leq 0.25^{L-1} \\cdot \\left|\\frac{\\partial \\mathcal{L}}{\\partial W^{(L)}}\\right|$$\n",
    "\n",
    "| Depth (L) | Gradient Attenuation |\n",
    "|-----------|----------------------|\n",
    "| 5 | $0.25^4 = 0.004$ |\n",
    "| 10 | $0.25^9 \\approx 4 \\times 10^{-6}$ |\n",
    "| 20 | $0.25^{19} \\approx 3 \\times 10^{-12}$ |\n",
    "| 50 | $0.25^{49} \\approx 10^{-30}$ |\n",
    "\n",
    "<details>\n",
    "<summary><b>\ud83e\udd14 Question:</b> Calculate the expected gradient magnitude at layer 1 of a 50-layer sigmoid network if the gradient at layer 50 is 1.0</summary>\n",
    "\n",
    "**Answer:** $0.25^{49} \\approx 3.2 \\times 10^{-30}$\n",
    "\n",
    "This is essentially zero in floating-point arithmetic (double precision has ~15-16 significant digits). The first layer receives no meaningful gradient signal!\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# BONUS:\n",
    "\n",
    "## Why it\u2019s worse for sigmoid/tanh\n",
    "\n",
    "* Sigmoid squashes input to $[0,1]$. Most inputs fall in saturated regions ($\\sigma'(z) \\approx 0$).\n",
    "* Tanh squashes to $[-1,1]$. Better, but still saturates.\n",
    "* ReLU partially fixes it ($f'(z)=1$ when active, 0 when inactive).\n",
    "  * That prevents vanishing *for active neurons*, but \u201cdead ReLUs\u201d still give zero gradient.\n",
    "\n",
    "---\n",
    "\n",
    "## Exploding gradient (the sibling problem)\n",
    "\n",
    "If derivatives or weight magnitudes > 1, the product **explodes exponentially**.\n",
    "\n",
    "* Early gradients become enormous \u2192 unstable updates.\n",
    "* Training loss oscillates or diverges.\n",
    "\n",
    "Vanishing and exploding are two sides of the same coin: *repeated multiplication through depth*.\n"
   ],
   "metadata": {
    "id": "ZBmm0JhMp_U3"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## Back to our lab implementation:\n",
    "\n",
    "The below exercise aims to demonstrate the vanishing gradient problem. By (i)initialising the weights to zero and (ii) using a Sigmoid activation function in a deep network, we observe how gradients diminish during training, hindering effective learning."
   ],
   "metadata": {
    "id": "UwgtxhMLqOZ8"
   }
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Proper vanishing gradient demonstration\n",
    "# Use SMALL RANDOM weights (not zero) with sigmoid in deep network\n",
    "# Zero init causes symmetry breaking problem, which is DIFFERENT from vanishing gradients\n",
    "\n",
    "def init_weights_small(module):\n",
    "    \"\"\"Initialise with small random weights to demonstrate vanishing gradients.\"\"\"\n",
    "    if isinstance(module, nn.Linear):\n",
    "        nn.init.normal_(module.weight, mean=0, std=0.1)\n",
    "        nn.init.zeros_(module.bias)\n",
    "\n",
    "# Deep network with sigmoid (prone to vanishing gradients)\n",
    "vanishing_layers = [32] * 20  # 20 layers of 32 neurons each\n",
    "vanishing_model = LazySequential(\n",
    "    in_feat_size=28*28,\n",
    "    out_feat_size=10,\n",
    "    hidden_sizes=vanishing_layers,\n",
    "    activation_fn='Sigmoid'  # Sigmoid derivatives are small (max 0.25)\n",
    ")\n",
    "\n",
    "# Apply small random initialisation\n",
    "vanishing_model.apply(init_weights_small)\n",
    "\n",
    "print(f\"Model depth: {len(vanishing_layers)} hidden layers\")\n",
    "print(f\"Sigmoid max derivative: 0.25\")\n",
    "print(f\"Expected gradient attenuation: 0.25^{len(vanishing_layers)} = {0.25**len(vanishing_layers):.2e}\")\n",
    "\n",
    "# Train and observe vanishing gradients\n",
    "vanishing_model = torch_train(\n",
    "    vanishing_model, \n",
    "    training_data, \n",
    "    epochs=2, \n",
    "    logdir='runs/vanishing_gradients'\n",
    ")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "%tensorboard --logdir runs/zero_grad"
   ],
   "metadata": {
    "id": "v2stNaFSeemJ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see no trend in the loss time series above; it's purely stochastic / normally distributed. There's no meaningful learning going on when we encounter the vanishing gradient problem."
   ],
   "metadata": {
    "id": "KMJaq1FMLZi6"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we are going to modify our training function to output a heatmap of the weights to the TensorBoard (same model as before, only diff training function)."
   ],
   "metadata": {
    "id": "01Scht8QJUV-"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def weight_heatmaps(model, cmap='Reds', **fig_kwargs):\n",
    "    mat, titles = [], []\n",
    "    vmin, vmax = +np.inf, -np.inf\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "      if len(param.squeeze().shape) == 2:\n",
    "        param = param.detach().numpy()\n",
    "        param = np.abs(param)\n",
    "        mat.append(param)\n",
    "        titles.append(name)\n",
    "\n",
    "        if param.max() > vmax:\n",
    "          vmax = param.max()\n",
    "        if param.min() < vmin:\n",
    "          vmin = param.min()\n",
    "\n",
    "    fig, axes = plt.subplots(2,3)\n",
    "    top, bottom = [1,2,3], [-4,-3,-2]\n",
    "    for i, weights in enumerate((top, bottom)):\n",
    "      for j, w in enumerate(weights):\n",
    "        axes[i,j].imshow(mat[w], vmin=vmin, vmax=vmax, cmap=cmap)\n",
    "\n",
    "\n",
    "    fig.suptitle('First 3 (top row) vs. last 3 (bottom row) hidden weights')\n",
    "    return fig"
   ],
   "metadata": {
    "id": "OohqlJL7qwa8"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dc538567"
   },
   "source": [
    "This is going to show the weight matrix for selected layers.\n",
    "- Each cell in the heatmap corresponds to a single weight matrix, connecting  input features to output features in that layer.\n",
    "  - x axis: input features (units from prev layer)\n",
    "  - y axis: output features (units in current layer)\n",
    "- The colour of each cell indicates the magnitude of that weight.\n",
    "\n",
    "Next, we add it to out training function."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# train function to log heatmaps of the parameters at every epoch\n",
    "\n",
    "def torch_train_heatmap(model, data,\n",
    "                optimizer='Adam', loss='CrossEntropyLoss',\n",
    "                batch_size=2**6, epochs=1, shuffle=False, logdir=None,\n",
    "                cmap='Reds', **fig_kwargs):\n",
    "\n",
    "  criterion = getattr(nn, loss)()\n",
    "  optimizer = getattr(optim, optimizer)(model.parameters())\n",
    "  dataloader = DataLoader(data, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "  model.train()\n",
    "\n",
    "  current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "  logdir = '//'.join(['runs', current_time, 'train']) if logdir is None else logdir\n",
    "  logger = SummaryWriter(logdir)\n",
    "  start = time.time()\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "    avg_loss = 0\n",
    "    i = 0\n",
    "\n",
    "    for (X, Y) in tqdm(dataloader):\n",
    "\n",
    "        # NEW:\n",
    "        fig = weight_heatmaps(model, cmap=cmap, **fig_kwargs)\n",
    "        logger.add_figure('Weights', fig, global_step=i + len(dataloader) * epoch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pred = model(X)\n",
    "        loss = criterion(pred.squeeze(-1), torch.tensor(Y))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_loss+=loss.item()\n",
    "        i+=1\n",
    "        logger.add_scalar('Training loss', loss, i + len(dataloader) * epoch )\n",
    "\n",
    "    logger.add_scalar('Avg epoch loss', avg_loss / len(dataloader), epoch)\n",
    "    logger.add_scalar('Total training time', time.time() - start, epoch)\n",
    "\n",
    "\n",
    "  return model"
   ],
   "metadata": {
    "id": "fKfT6pTWK5ca"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "zero_grad_model = torch_train_heatmap(zero_grad_model, training_data, epochs=1, logdir='runs/heatmap')"
   ],
   "metadata": {
    "id": "yecnOVKaIGZc"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%tensorboard --logdir runs/heatmap"
   ],
   "metadata": {
    "id": "g7AR2JkEJkPg"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we see weights/activations shrink layer by layer \u2192 gradient norms get smaller and smaller \u2192 earlier layers barely update \u2192 their weights stay close to initialisation (negligible coefficients)."
   ],
   "metadata": {
    "id": "L1pisLrTN7ho"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Gradient Norm Analysis\n",
    "\n",
    "Let's quantitatively track gradient magnitudes through the network layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def track_gradient_norms(model, data, epochs=1):\n",
    "    \"\"\"Track gradient norms per layer during training.\"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    opt = optim.Adam(model.parameters())\n",
    "    dataloader = DataLoader(data, batch_size=64)\n",
    "    \n",
    "    layer_names = [name for name, _ in model.named_parameters() if 'weight' in name]\n",
    "    gradient_history = {name: [] for name in layer_names}\n",
    "    \n",
    "    model.train()\n",
    "    for X, Y in dataloader:\n",
    "        opt.zero_grad()\n",
    "        loss = criterion(model(X).squeeze(-1), Y.long())\n",
    "        loss.backward()\n",
    "        \n",
    "        # Record gradient norms\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'weight' in name and param.grad is not None:\n",
    "                gradient_history[name].append(param.grad.norm().item())\n",
    "        \n",
    "        opt.step()\n",
    "        break  # Just one batch for demonstration\n",
    "    \n",
    "    return gradient_history\n",
    "\n",
    "# Compare sigmoid vs ReLU gradient flow\n",
    "sigmoid_model = LazySequential(28*28, 10, [64]*10, 'Sigmoid')\n",
    "relu_model = LazySequential(28*28, 10, [64]*10, 'ReLU')\n",
    "\n",
    "sigmoid_grads = track_gradient_norms(sigmoid_model, training_data)\n",
    "relu_grads = track_gradient_norms(relu_model, training_data)\n",
    "\n",
    "# Plot comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Get layer indices (skip first flatten, take every other for weights)\n",
    "sigmoid_norms = [sigmoid_grads[k][0] for k in sorted(sigmoid_grads.keys())]\n",
    "relu_norms = [relu_grads[k][0] for k in sorted(relu_grads.keys())]\n",
    "\n",
    "axes[0].bar(range(len(sigmoid_norms)), sigmoid_norms, alpha=0.7)\n",
    "axes[0].set_xlabel('Layer')\n",
    "axes[0].set_ylabel('Gradient Norm')\n",
    "axes[0].set_title('Sigmoid: Gradient Norms by Layer')\n",
    "axes[0].set_yscale('log')\n",
    "\n",
    "axes[1].bar(range(len(relu_norms)), relu_norms, alpha=0.7, colour='orange')\n",
    "axes[1].set_xlabel('Layer')\n",
    "axes[1].set_ylabel('Gradient Norm')\n",
    "axes[1].set_title('ReLU: Gradient Norms by Layer')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Sigmoid gradient range: {min(sigmoid_norms):.2e} to {max(sigmoid_norms):.2e}\")\n",
    "print(f\"ReLU gradient range: {min(relu_norms):.2e} to {max(relu_norms):.2e}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Hyperparameter Tuning\n",
    "Hyperparameter tuning is the process of systematically searching for the optimal set of hyperparameters (e.g. number of layers, size of the layers, learning rate, batch size, dropout ...) of a ML model. There are four common methods of hyperparameter optimisation:  \n",
    "\n",
    "*   Manual\n",
    "*   Grid search\n",
    "*   Random search\n",
    "*   Bayesian search"
   ],
   "metadata": {
    "id": "uEenH2AkPYXQ"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning Strategies\n",
    "\n",
    "| Method | Description | Pros | Cons |\n",
    "|--------|-------------|------|------|\n",
    "| **Manual** | Trial and error | Intuitive | Time-consuming, biased |\n",
    "| **Grid Search** | Exhaustive search over parameter grid | Thorough | Exponential cost in dimensions |\n",
    "| **Random Search** | Random sampling from parameter space | Often better than grid | No learning from past trials |\n",
    "| **Bayesian (Optuna)** | Model-based optimisation | Sample-efficient | More complex |\n",
    "\n",
    "<details>\n",
    "<summary><b>\ud83e\udd14 Question:</b> Why might random search outperform grid search in high dimensions?</summary>\n",
    "\n",
    "**Answer:** In high-dimensional spaces, most hyperparameters have little effect on performance (only a few matter). Grid search wastes budget testing all combinations, while random search explores more values of the important parameters. This is known as the \"effective dimensionality\" argument (Bergstra & Bengio, 2012).\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "%pip install optuna torcheval --q"
   ],
   "metadata": {
    "id": "PYdcfVRg5XXb"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Optuna\n",
    "Optuna is an open-source hyperparameter optimisation framework -> automates the process of finding the best set of hyperparameters. Optuna systematically searches through a defined space of possible values to find the combination that yields the best performance (e.g., highest accuracy, lowest loss) on our model.\n",
    "\n",
    "- **Study**: In Optuna, a \"study\" represents an optimisation session. Think of a study as a container that manages the entire hyperparameter tuning process.\n",
    "    - We specify the optimisation direction.\n",
    "        - 'maximize' for metrics like accuracy,\n",
    "        -  'minimize' for metrics like loss.\n",
    "\n",
    "- **Trial**: a single run of our model with a specific set of hyperparameters suggested by Optuna.\n",
    "    - The objective function defines what happens in each trial (w/ different parameters).\n",
    "    - Optuna calls this function repeatedly, each time providing a trial object.\n",
    "\n",
    "- **Objective Function**: This is the function that Optuna optimises.\n",
    "    - takes a trial object as input\n",
    "    - returns the metric we want to optimise (e.g., test accuracy).\n",
    "    - Inside the objective function:\n",
    "        - we use trial object to suggest values for the hyperparameters you want to tune. Optuna uses these suggestions to explore the hyperparameter space.\n",
    "        - we build and train our model using the suggested hyperparameters.\n",
    "        - we evaluate our model's performance using the chosen metric.\n",
    "        - we return the evaluated metric."
   ],
   "metadata": {
    "id": "a_ektsiFvMNO"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import optuna\n",
    "from torcheval.metrics.functional import multiclass_accuracy\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define an objective function to be maximized.\n",
    "def objective(trial,\n",
    "              training_data,\n",
    "              test_data,\n",
    "              **train_params):\n",
    "    \"\"\"\n",
    "    Objective function for Optuna hyperparameter tuning.\n",
    "\n",
    "    Args:\n",
    "        trial (optuna.Trial): An Optuna trial object.\n",
    "        training_data (torch.utils.data.Dataset): The training dataset.\n",
    "        test_data (torch.utils.data.Dataset): The test dataset.\n",
    "        **train_params: Additional parameters to pass to the torch_train function.\n",
    "\n",
    "    Returns:\n",
    "        float: The test accuracy of the model with the suggested hyperparameters.\n",
    "    \"\"\"\n",
    "\n",
    "    # Suggest values of the hyperparameters using a trial object.\n",
    "    n_layers = trial.suggest_int('n_layers', 1, 3)\n",
    "        # ^^^ telling optuna to sample this hyperparam from defined bounds\n",
    "        # this is the magic: optuna will choose optimum sampling strategy to imporve performance based on past results\n",
    "    layers = []\n",
    "\n",
    "    for i in range(n_layers):\n",
    "        size = trial.suggest_int(f'n_units_l{i}', 4, 128)\n",
    "        # ^^^ telling optuna to sample numb of hidden units from our bounds\n",
    "        layers.append(size)\n",
    "\n",
    "    # build models\n",
    "    model = LazySequential(in_feat_size=28*28, out_feat_size=10, hidden_sizes=layers).to(torch.device('cpu'))\n",
    "    model = torch_train(model, training_data, **train_params)\n",
    "\n",
    "    # prep data\n",
    "    test_dataloader = DataLoader(test_data, batch_size= len(test_data))\n",
    "\n",
    "    # compute acc\n",
    "    X_test, Y_test = next(iter(test_dataloader))\n",
    "    Y_pred = model(X_test).argmax(dim=-1)\n",
    "    test_acc = multiclass_accuracy(Y_pred, Y_test)\n",
    "\n",
    "    return test_acc"
   ],
   "metadata": {
    "id": "7YBdKLfGyLFr"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "...with our objective function defined, let's create a study object"
   ],
   "metadata": {
    "id": "nJkVTmUyw6rR"
   }
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Create study with more trials for meaningful results\n",
    "study = optuna.create_study(direction='maximize')\n",
    "\n",
    "# Run optimisation (increase trials for better results)\n",
    "study.optimize(\n",
    "    lambda trial: objective(trial, training_data, test_data),\n",
    "    n_trials=15,  # Increased from 5 for more meaningful search\n",
    "    show_progress_bar=True\n",
    ")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Visualise optimisation history\n",
    "import optuna.visualization as vis\n",
    "\n",
    "# Plot optimisation history\n",
    "fig1 = vis.plot_optimization_history(study)\n",
    "fig1.show()\n",
    "\n",
    "# Plot parameter importances\n",
    "try:\n",
    "    fig2 = vis.plot_param_importances(study)\n",
    "    fig2.show()\n",
    "except:\n",
    "    print(\"Need more trials for parameter importance plot\")\n",
    "\n",
    "# Plot parallel coordinate\n",
    "fig3 = vis.plot_parallel_coordinate(study)\n",
    "fig3.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fe197ade"
   },
   "source": [
    "# Print the best trial's parameters and value\n",
    "print(\"Best trial:\")\n",
    "print(\"  Acc: {}\".format(study.best_trial.value))\n",
    "print(\"  Params: \")\n",
    "for key, value in study.best_trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "\n",
    "# can also access all trials\n",
    "print(\"\\nAll trials:\")\n",
    "for trial in study.trials:\n",
    "    print(\"  Trial {}:\".format(trial.number))\n",
    "    print(\"    Acc: {}\".format(trial.value))\n",
    "    print(\"    Params: {}\".format(trial.params))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Summary & Key Takeaways\n",
    "\n",
    "## TensorBoard\n",
    "- Essential tool for monitoring training progress and diagnosing issues\n",
    "- Log scalars (loss, accuracy), images, histograms, and custom figures\n",
    "- Use `SummaryWriter` to create logs, view with `%tensorboard --logdir <path>`\n",
    "\n",
    "## Vanishing Gradients\n",
    "- **Cause**: Activation derivatives < 1 compound through layers\n",
    "- **Effect**: Early layers receive tiny gradients \u2192 don't learn\n",
    "- **Diagnosis**: Track gradient norms per layer\n",
    "- **Solutions**: ReLU, batch normalisation, skip connections, careful initialisation\n",
    "\n",
    "## Batch Learning\n",
    "- **Online (batch=1)**: High variance, many updates, escapes local minima\n",
    "- **Full-batch**: Low variance, few updates, precise convergence\n",
    "- **Minibatch (32-256)**: Best of both worlds for most applications\n",
    "\n",
    "## Hyperparameter Tuning\n",
    "- Grid search: Exhaustive but expensive\n",
    "- Random search: Often surprisingly effective\n",
    "- Bayesian (Optuna): Sample-efficient, learns from past trials\n",
    "\n",
    "<details>\n",
    "<summary><b>\ud83e\udd14 Question:</b> When would you prefer manual tuning over automated methods?</summary>\n",
    "\n",
    "**Answer:** Manual tuning is preferred when:\n",
    "1. You're exploring a new problem and building intuition\n",
    "2. Training is very expensive (each trial takes hours/days)\n",
    "3. You have strong domain knowledge about reasonable parameter ranges\n",
    "4. You need to understand parameter interactions, not just find \"good\" values\n",
    "\n",
    "Automated methods excel for final optimisation once you have a reasonable baseline.\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "\n",
    "- **Vanishing Gradients**: Glorot & Bengio (2010) - *Understanding the difficulty of training deep feedforward neural networks*\n",
    "- **Initialisation**: He et al. (2015) - *Delving Deep into Rectifiers*\n",
    "- **Batch Normalisation**: Ioffe & Szegedy (2015) - *Batch Normalization: Accelerating Deep Network Training*\n",
    "- **Random Search**: Bergstra & Bengio (2012) - *Random Search for Hyper-Parameter Optimisation*\n",
    "- **Optuna**: Akiba et al. (2019) - *Optuna: A Next-generation Hyperparameter Optimisation Framework*\n"
   ]
  }
 ]
}