\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Week 1: Introduction to Deep Learning}{17}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:week1}{{1}{17}{Week 1: Introduction to Deep Learning}{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}What is Deep Learning?}{17}{section.1.1}\protected@file@percent }
\newlabel{sec:what-is-dl}{{1.1}{17}{What is Deep Learning?}{section.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Relationship between AI, machine learning, and deep learning. Deep learning is a subset of machine learning, which itself is a subset of artificial intelligence. AI encompasses any technique that enables computers to mimic human intelligence; ML focuses on systems that learn from data; DL specifically uses multi-layered neural networks.}}{18}{figure.1.1}\protected@file@percent }
\newlabel{fig:dl-venn}{{1.1}{18}{Relationship between AI, machine learning, and deep learning. Deep learning is a subset of machine learning, which itself is a subset of artificial intelligence. AI encompasses any technique that enables computers to mimic human intelligence; ML focuses on systems that learn from data; DL specifically uses multi-layered neural networks}{figure.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}The Learning Problem: Formal Setup}{18}{subsection.1.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.2}What Does ``Learning'' Mean Formally?}{19}{subsection.1.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.3}Historical Context}{20}{subsection.1.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Learning Paradigms}{21}{section.1.2}\protected@file@percent }
\newlabel{sec:learning-paradigms}{{1.2}{21}{Learning Paradigms}{section.1.2}{}}
\newlabel{rigour:self-supervised}{{1.2}{24}{Learning Paradigms}{section.1.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Machine Learning vs Deep Learning}{25}{section.1.3}\protected@file@percent }
\newlabel{sec:ml-vs-dl}{{1.3}{25}{Machine Learning vs Deep Learning}{section.1.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}Feature Engineering vs Feature Learning}{25}{subsection.1.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Hierarchical feature learning in deep networks. The network automatically learns a progression from simple to complex features: edges $\to $ textures $\to $ object parts $\to $ whole objects. This hierarchy emerges from training, not manual design.}}{26}{figure.1.2}\protected@file@percent }
\newlabel{fig:rep-learning-1}{{1.2}{26}{Hierarchical feature learning in deep networks. The network automatically learns a progression from simple to complex features: edges $\to $ textures $\to $ object parts $\to $ whole objects. This hierarchy emerges from training, not manual design}{figure.1.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.2}Why Does Deep Learning Work?}{27}{subsection.1.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.3}When to Use Which}{28}{subsection.1.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Universal Approximation Theorem}{29}{section.1.4}\protected@file@percent }
\newlabel{sec:uat}{{1.4}{29}{Universal Approximation Theorem}{section.1.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.1}Intuitive Statement}{29}{subsection.1.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.2}Why Does It Work? Proof Intuition}{31}{subsection.1.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.3}Implications and Limitations}{33}{subsection.1.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.4}Why Depth Matters}{34}{subsection.1.4.4}\protected@file@percent }
\newlabel{sec:why-depth}{{1.4.4}{34}{Why Depth Matters}{subsection.1.4.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Representation Learning}{35}{section.1.5}\protected@file@percent }
\newlabel{sec:representation-learning}{{1.5}{35}{Representation Learning}{section.1.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5.1}What is a Representation?}{35}{subsection.1.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5.2}What Makes a Good Representation?}{36}{subsection.1.5.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces Deep networks learn hierarchical representations: raw pixels $\to $ edges $\to $ textures $\to $ parts $\to $ objects. Each layer builds increasingly abstract, task-relevant features from the previous layer's output. This hierarchy emerges automatically from training on the end task.}}{38}{figure.1.3}\protected@file@percent }
\newlabel{fig:rep-learning-2}{{1.3}{38}{Deep networks learn hierarchical representations: raw pixels $\to $ edges $\to $ textures $\to $ parts $\to $ objects. Each layer builds increasingly abstract, task-relevant features from the previous layer's output. This hierarchy emerges automatically from training on the end task}{figure.1.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5.3}The Manifold Hypothesis}{38}{subsection.1.5.3}\protected@file@percent }
\newlabel{sec:manifold-hypothesis}{{1.5.3}{38}{The Manifold Hypothesis}{subsection.1.5.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.6}Modern Deep Learning Architectures}{40}{section.1.6}\protected@file@percent }
\newlabel{sec:architectures}{{1.6}{40}{Modern Deep Learning Architectures}{section.1.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6.1}Multi-Layer Perceptrons (MLPs)}{41}{subsection.1.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6.2}Convolutional Neural Networks (CNNs)}{42}{subsection.1.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6.3}Recurrent Neural Networks (RNNs)}{43}{subsection.1.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6.4}Transformers}{45}{subsection.1.6.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6.5}Graph Neural Networks (GNNs)}{47}{subsection.1.6.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.7}Deep Learning in Policy Context}{48}{section.1.7}\protected@file@percent }
\newlabel{sec:policy-context}{{1.7}{48}{Deep Learning in Policy Context}{section.1.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.7.1}Ethical Considerations}{49}{subsection.1.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.7.2}Transparency and Explainability}{50}{subsection.1.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.7.3}Safety and Robustness}{51}{subsection.1.7.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.7.4}Environmental Impact}{51}{subsection.1.7.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.8}Summary and Looking Ahead}{53}{section.1.8}\protected@file@percent }
\newlabel{sec:summary}{{1.8}{53}{Summary and Looking Ahead}{section.1.8}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Week 2: Deep Neural Networks I}{55}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:week2}{{2}{55}{Week 2: Deep Neural Networks I}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Neural Network Fundamentals}{55}{section.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Notation and Conventions}{56}{subsection.2.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}The Artificial Neuron}{57}{subsection.2.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces An artificial neuron computes a weighted sum of inputs, adds a bias, and applies a nonlinear activation function.}}{57}{figure.2.1}\protected@file@percent }
\newlabel{fig:artificial-neuron}{{2.1}{57}{An artificial neuron computes a weighted sum of inputs, adds a bias, and applies a nonlinear activation function}{figure.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Layers of Neurons}{59}{subsection.2.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Hidden unit's connection to input activation. Each hidden unit receives \textit  {all} inputs-this is why we call it a \textit  {fully-connected} (or \textit  {dense}) layer.}}{59}{figure.2.2}\protected@file@percent }
\newlabel{fig:hidden-layer}{{2.2}{59}{Hidden unit's connection to input activation. Each hidden unit receives \textit {all} inputs-this is why we call it a \textit {fully-connected} (or \textit {dense}) layer}{figure.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4}Matrix Multiplication: How Forward Propagation Works}{60}{subsection.2.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Single-Layer Neural Networks}{64}{section.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Overview of neural network architecture. The network receives inputs, transforms them through hidden layers with nonlinear activations, and produces outputs. Each layer applies a linear transformation followed by a nonlinear activation function.}}{64}{figure.2.3}\protected@file@percent }
\newlabel{fig:architecture-overview}{{2.3}{64}{Overview of neural network architecture. The network receives inputs, transforms them through hidden layers with nonlinear activations, and produces outputs. Each layer applies a linear transformation followed by a nonlinear activation function}{figure.2.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces A single-layer neural network with $d$ inputs, $H$ hidden units, and one output. Note: $W^{[1]}$ is a matrix (connecting all inputs to all hidden units); $w^{[2]}$ is a vector (connecting all hidden units to a single output).}}{65}{figure.2.4}\protected@file@percent }
\newlabel{fig:single-layer-nn}{{2.4}{65}{A single-layer neural network with $d$ inputs, $H$ hidden units, and one output. Note: $W^{[1]}$ is a matrix (connecting all inputs to all hidden units); $w^{[2]}$ is a vector (connecting all hidden units to a single output)}{figure.2.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Architecture}{65}{subsection.2.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Matrix Formulation}{67}{subsection.2.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Output Layer for Different Tasks}{67}{subsection.2.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Activation Functions}{68}{section.2.3}\protected@file@percent }
\newlabel{sec:activations}{{2.3}{68}{Activation Functions}{section.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Why Nonlinearity is Essential}{68}{subsection.2.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Purpose of Activation Functions}{69}{subsection.2.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Common Activation Functions}{69}{subsection.2.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Sigmoid (Logistic)}{69}{section*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Hyperbolic Tangent (tanh)}{70}{section*.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Rectified Linear Unit (ReLU)}{70}{section*.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Leaky ReLU and Variants}{70}{section*.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.4}Gradient Analysis and Saturation}{71}{subsection.2.3.4}\protected@file@percent }
\newlabel{subsec:gradient-analysis}{{2.3.4}{71}{Gradient Analysis and Saturation}{subsection.2.3.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Comparison of sigmoid and ReLU. ReLU is unbounded for positive inputs and exactly zero for negative inputs.}}{73}{figure.2.5}\protected@file@percent }
\newlabel{fig:sigmoid-relu}{{2.5}{73}{Comparison of sigmoid and ReLU. ReLU is unbounded for positive inputs and exactly zero for negative inputs}{figure.2.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces The hyperbolic tangent function-a scaled and shifted sigmoid, zero-centred.}}{73}{figure.2.6}\protected@file@percent }
\newlabel{fig:tanh}{{2.6}{73}{The hyperbolic tangent function-a scaled and shifted sigmoid, zero-centred}{figure.2.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces The Heaviside step function-the original ``activation'' but not differentiable.}}{74}{figure.2.7}\protected@file@percent }
\newlabel{fig:heaviside}{{2.7}{74}{The Heaviside step function-the original ``activation'' but not differentiable}{figure.2.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.5}Why ReLU Dominates}{74}{subsection.2.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.6}The Dying ReLU Problem and Solutions}{75}{subsection.2.3.6}\protected@file@percent }
\newlabel{subsec:dying-relu}{{2.3.6}{75}{The Dying ReLU Problem and Solutions}{subsection.2.3.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Output Layers and Loss Functions}{77}{section.2.4}\protected@file@percent }
\newlabel{sec:loss-functions}{{2.4}{77}{Output Layers and Loss Functions}{section.2.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Output Activations by Task}{77}{subsection.2.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Softmax Function}{78}{subsection.2.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Multi-class classification with $K$ output nodes. For $K$ output nodes, $W^{[2]}$ now has $K \times H$ dimensions; biases form a vector of $K$. Softmax ensures outputs sum to 1.}}{79}{figure.2.8}\protected@file@percent }
\newlabel{fig:multiclass}{{2.8}{79}{Multi-class classification with $K$ output nodes. For $K$ output nodes, $W^{[2]}$ now has $K \times H$ dimensions; biases form a vector of $K$. Softmax ensures outputs sum to 1}{figure.2.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces The forward propagation process: input $\to $ hidden activations $\to $ output probabilities.}}{80}{figure.2.9}\protected@file@percent }
\newlabel{fig:process}{{2.9}{80}{The forward propagation process: input $\to $ hidden activations $\to $ output probabilities}{figure.2.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.3}Loss Functions from Maximum Likelihood}{80}{subsection.2.4.3}\protected@file@percent }
\newlabel{subsec:mle-derivation}{{2.4.3}{80}{Loss Functions from Maximum Likelihood}{subsection.2.4.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{MSE Loss from Gaussian Likelihood}{81}{section*.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Cross-Entropy from Categorical Likelihood}{82}{section*.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.4}Loss Functions for Regression}{84}{subsection.2.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.5}Loss Functions for Classification}{85}{subsection.2.4.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces The $-\log (x)$ function: as predicted probability approaches 0, loss increases sharply; as it approaches 1, loss approaches 0.}}{86}{figure.2.10}\protected@file@percent }
\newlabel{fig:log-loss}{{2.10}{86}{The $-\log (x)$ function: as predicted probability approaches 0, loss increases sharply; as it approaches 1, loss approaches 0}{figure.2.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Capacity and Expressiveness}{87}{section.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}Linear Separability}{87}{subsection.2.5.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces Top: linearly separable data (a single linear decision boundary can separate the classes). Bottom: not linearly separable (XOR problem-no single line can separate the classes).}}{87}{figure.2.11}\protected@file@percent }
\newlabel{fig:linear-sep}{{2.11}{87}{Top: linearly separable data (a single linear decision boundary can separate the classes). Bottom: not linearly separable (XOR problem-no single line can separate the classes)}{figure.2.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.12}{\ignorespaces Examples of linearly separable problems. A single neuron can learn decision boundaries that separate these classes.}}{88}{figure.2.12}\protected@file@percent }
\newlabel{fig:linear-sep-examples}{{2.12}{88}{Examples of linearly separable problems. A single neuron can learn decision boundaries that separate these classes}{figure.2.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}How Hidden Layers Create Nonlinear Boundaries}{88}{subsection.2.5.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.13}{\ignorespaces Transformation of input features: hidden layers project data into a space where it becomes linearly separable.}}{89}{figure.2.13}\protected@file@percent }
\newlabel{fig:feature-transform}{{2.13}{89}{Transformation of input features: hidden layers project data into a space where it becomes linearly separable}{figure.2.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.14}{\ignorespaces A single-layer network with 4 hidden neurons can represent a non-linear function. Each neuron learns a linearly separable component; their combination creates complex boundaries.}}{89}{figure.2.14}\protected@file@percent }
\newlabel{fig:nonlinear-boundary}{{2.14}{89}{A single-layer network with 4 hidden neurons can represent a non-linear function. Each neuron learns a linearly separable component; their combination creates complex boundaries}{figure.2.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.3}Universal Approximation Theorem}{90}{subsection.2.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Gradient Descent}{91}{section.2.6}\protected@file@percent }
\newlabel{sec:gradient-descent}{{2.6}{91}{Gradient Descent}{section.2.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.1}Why Gradient Descent?}{91}{subsection.2.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.2}Gradient Descent Algorithm}{91}{subsection.2.6.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.15}{\ignorespaces Gradient descent on a 2D loss surface. The gradient is a vector of partial derivatives pointing uphill; we step in the opposite direction.}}{92}{figure.2.15}\protected@file@percent }
\newlabel{fig:grad-descent}{{2.15}{92}{Gradient descent on a 2D loss surface. The gradient is a vector of partial derivatives pointing uphill; we step in the opposite direction}{figure.2.15}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.3}Learning Rate}{95}{subsection.2.6.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.16}{\ignorespaces Even with fixed $\eta $, steps become smaller as we approach the minimum-the gradient magnitude decreases. As we get closer to the minimum, the functional values we plug in become smaller; the gradient evaluated at each successive point becomes smaller.}}{95}{figure.2.16}\protected@file@percent }
\newlabel{fig:learning-rate}{{2.16}{95}{Even with fixed $\eta $, steps become smaller as we approach the minimum-the gradient magnitude decreases. As we get closer to the minimum, the functional values we plug in become smaller; the gradient evaluated at each successive point becomes smaller}{figure.2.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.4}Convergence Analysis}{95}{subsection.2.6.4}\protected@file@percent }
\newlabel{subsec:convergence}{{2.6.4}{95}{Convergence Analysis}{subsection.2.6.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.5}Stopping Criteria}{97}{subsection.2.6.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.7}Backpropagation}{97}{section.2.7}\protected@file@percent }
\newlabel{sec:backpropagation}{{2.7}{97}{Backpropagation}{section.2.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.1}The Training Loop}{98}{subsection.2.7.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.17}{\ignorespaces The neural network training loop: forward pass $\to $ loss computation $\to $ backward pass $\to $ parameter update.}}{98}{figure.2.17}\protected@file@percent }
\newlabel{fig:training-loop}{{2.17}{98}{The neural network training loop: forward pass $\to $ loss computation $\to $ backward pass $\to $ parameter update}{figure.2.17}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.2}Computational Graphs}{98}{subsection.2.7.2}\protected@file@percent }
\newlabel{subsec:comp-graphs}{{2.7.2}{98}{Computational Graphs}{subsection.2.7.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.3}The Chain Rule}{100}{subsection.2.7.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.18}{\ignorespaces Multivariate chain rule: to compute $\frac  {\partial f}{\partial w}$, sum over all paths from $w$ to $f$.}}{101}{figure.2.18}\protected@file@percent }
\newlabel{fig:chain-rule}{{2.18}{101}{Multivariate chain rule: to compute $\frac {\partial f}{\partial w}$, sum over all paths from $w$ to $f$}{figure.2.18}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.4}Backpropagation in Matrix Form}{101}{subsection.2.7.4}\protected@file@percent }
\newlabel{subsec:backprop-matrix}{{2.7.4}{101}{Backpropagation in Matrix Form}{subsection.2.7.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.5}Computing the Gradient: Scalar Form}{103}{subsection.2.7.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.6}Worked Example: Backpropagation}{106}{subsection.2.7.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.7}Gradient Formulas for Common Cases}{108}{subsection.2.7.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.8}Parameters vs Hyperparameters}{110}{section.2.8}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.9}The Bigger Picture}{111}{section.2.9}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.19}{\ignorespaces Everything covered has been for a single-layer network with linear output. Real networks are deeper and use different output activations.}}{111}{figure.2.19}\protected@file@percent }
\newlabel{fig:big-picture}{{2.19}{111}{Everything covered has been for a single-layer network with linear output. Real networks are deeper and use different output activations}{figure.2.19}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Week 3: Deep Neural Networks II}{113}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:week3}{{3}{113}{Week 3: Deep Neural Networks II}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Backpropagation (Continued)}{114}{section.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Reminder: Single-Layer Network}{114}{subsection.3.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Single-layer neural network with one hidden layer and single output. Information flows left to right: inputs $x$ are transformed by the first layer's weights $W^{[1]}$ into hidden activations $h$, which are then transformed by $W^{[2]}$ into the output $f(x)$.}}{114}{figure.3.1}\protected@file@percent }
\newlabel{fig:single-layer-nn}{{3.1}{114}{Single-layer neural network with one hidden layer and single output. Information flows left to right: inputs $x$ are transformed by the first layer's weights $W^{[1]}$ into hidden activations $h$, which are then transformed by $W^{[2]}$ into the output $f(x)$}{figure.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Gradient via Chain Rule}{116}{subsection.3.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}Gradient Update}{117}{subsection.3.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Multivariate Chain Rule}{118}{section.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Why Multiple Paths Matter}{118}{subsection.3.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Computational graph for multivariate chain rule. Nodes represent variables; edges represent functional dependencies. To find how $z$ affects $f$, we must sum contributions through all paths from $z$ to $f$.}}{118}{figure.3.2}\protected@file@percent }
\newlabel{fig:multivariate-chain}{{3.2}{118}{Computational graph for multivariate chain rule. Nodes represent variables; edges represent functional dependencies. To find how $z$ affects $f$, we must sum contributions through all paths from $z$ to $f$}{figure.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}The Formal Rule}{119}{subsection.3.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Worked Example}{119}{subsection.3.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.4}Connection to Neural Networks}{120}{subsection.3.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Multiple Output Nodes}{120}{section.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Network with $K=2$ output nodes for binary classification. Each hidden unit connects to \textit  {all} output units, and each output unit has its own set of weights and bias.}}{121}{figure.3.3}\protected@file@percent }
\newlabel{fig:multi-output}{{3.3}{121}{Network with $K=2$ output nodes for binary classification. Each hidden unit connects to \textit {all} output units, and each output unit has its own set of weights and bias}{figure.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Network Architecture with Multiple Outputs}{121}{subsection.3.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Cross-Entropy Loss}{122}{subsection.3.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}Gradient for Multi-Class Classification}{124}{subsection.3.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.4}Softmax + Cross-Entropy Simplification}{125}{subsection.3.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Deeper Networks: Multilayer Perceptrons}{127}{section.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Why Go Deeper?}{127}{subsection.3.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Two-hidden-layer network (Multilayer Perceptron). Information flows left to right through two hidden layers before reaching the output. Each layer transforms its input through weights, biases, and nonlinear activations.}}{128}{figure.3.4}\protected@file@percent }
\newlabel{fig:mlp}{{3.4}{128}{Two-hidden-layer network (Multilayer Perceptron). Information flows left to right through two hidden layers before reaching the output. Each layer transforms its input through weights, biases, and nonlinear activations}{figure.3.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Two-Hidden-Layer Network}{128}{subsection.3.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Deep network architecture with explicit bias nodes (shown as nodes containing ``1''). Each layer has weights $W^{(l)}$ connecting to the previous layer's activations, and biases $b^{(l)}$ that add constant offsets. The sigmoid curves inside neurons represent the activation function.}}{129}{figure.3.5}\protected@file@percent }
\newlabel{fig:deep-network-biases}{{3.5}{129}{Deep network architecture with explicit bias nodes (shown as nodes containing ``1''). Each layer has weights $W^{(l)}$ connecting to the previous layer's activations, and biases $b^{(l)}$ that add constant offsets. The sigmoid curves inside neurons represent the activation function}{figure.3.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.3}Generic Gradient Form}{130}{subsection.3.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.4}Full Expansion for Two Hidden Layers}{130}{subsection.3.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Vectorisation}{131}{section.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.1}Why Vectorisation Matters}{131}{subsection.3.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.2}Vectorised Neural Network}{133}{subsection.3.5.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Vectorised network with dimension annotations. Each arrow represents a matrix multiplication: $W^{[1]}$ transforms $d$-dimensional input to $H$-dimensional hidden representation; $W^{[2]}$ transforms to $K$-dimensional output.}}{134}{figure.3.6}\protected@file@percent }
\newlabel{fig:vectorized}{{3.6}{134}{Vectorised network with dimension annotations. Each arrow represents a matrix multiplication: $W^{[1]}$ transforms $d$-dimensional input to $H$-dimensional hidden representation; $W^{[2]}$ transforms to $K$-dimensional output}{figure.3.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.3}Compact Representation (Absorbing Biases)}{135}{subsection.3.5.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces Compact representation with biases absorbed into weight matrices.}}{135}{figure.3.7}\protected@file@percent }
\newlabel{fig:vectorized-compact}{{3.7}{135}{Compact representation with biases absorbed into weight matrices}{figure.3.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.4}General $L$-Layer Network}{136}{subsection.3.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Vectorised Backpropagation}{136}{section.3.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.1}The Error Signal Concept}{136}{subsection.3.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.2}Output Layer}{137}{subsection.3.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.3}Hidden Layers (Recursive)}{138}{subsection.3.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.4}Gradient Dimensions}{140}{subsection.3.6.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces Dimensions of gradients and error signals.}}{140}{figure.3.8}\protected@file@percent }
\newlabel{fig:grad-dims}{{3.8}{140}{Dimensions of gradients and error signals}{figure.3.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.7}Mini-Batch Gradient Descent}{140}{section.3.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7.1}Stochastic Gradient Descent (SGD)}{140}{subsection.3.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7.2}Batch Gradient Descent}{141}{subsection.3.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7.3}Mini-Batch Gradient Descent}{141}{subsection.3.7.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces Dimension tracking for batch gradient descent with absorbed biases. Each matrix multiplication must have compatible inner dimensions.}}{142}{figure.3.9}\protected@file@percent }
\newlabel{fig:batch-dimensions}{{3.9}{142}{Dimension tracking for batch gradient descent with absorbed biases. Each matrix multiplication must have compatible inner dimensions}{figure.3.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.10}{\ignorespaces Comparison of convergence paths. Mini-batch has intermediate noise between SGD and batch GD.}}{144}{figure.3.10}\protected@file@percent }
\newlabel{fig:minibatch-comparison}{{3.10}{144}{Comparison of convergence paths. Mini-batch has intermediate noise between SGD and batch GD}{figure.3.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.11}{\ignorespaces Mini-batch gradient descent in the loss landscape.}}{144}{figure.3.11}\protected@file@percent }
\newlabel{fig:minibatch-landscape}{{3.11}{144}{Mini-batch gradient descent in the loss landscape}{figure.3.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.8}Training Process}{145}{section.3.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.8.1}Generalisation}{145}{subsection.3.8.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.8.2}Data Splits}{145}{subsection.3.8.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.12}{\ignorespaces Train/validation/test split: training data is used to fit the model, validation data for hyperparameter tuning and early stopping, and test data remains untouched until final evaluation.}}{146}{figure.3.12}\protected@file@percent }
\newlabel{fig:train-val-test}{{3.12}{146}{Train/validation/test split: training data is used to fit the model, validation data for hyperparameter tuning and early stopping, and test data remains untouched until final evaluation}{figure.3.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.13}{\ignorespaces Training and validation error as functions of model complexity. As complexity increases, training error decreases (the model fits training data better), but eventually validation error increases (the model overfits).}}{146}{figure.3.13}\protected@file@percent }
\newlabel{fig:model-complexity}{{3.13}{146}{Training and validation error as functions of model complexity. As complexity increases, training error decreases (the model fits training data better), but eventually validation error increases (the model overfits)}{figure.3.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.8.3}Early Stopping}{147}{subsection.3.8.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.14}{\ignorespaces Early stopping: halt training when validation error starts increasing.}}{147}{figure.3.14}\protected@file@percent }
\newlabel{fig:early-stopping}{{3.14}{147}{Early stopping: halt training when validation error starts increasing}{figure.3.14}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.9}Performance Metrics}{148}{section.3.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.9.1}Binary Classification Metrics}{148}{subsection.3.9.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.15}{\ignorespaces Precision vs recall visualised.}}{149}{figure.3.15}\protected@file@percent }
\newlabel{fig:precision-recall}{{3.15}{149}{Precision vs recall visualised}{figure.3.15}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.9.2}ROC and AUC}{150}{subsection.3.9.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.16}{\ignorespaces ROC curve showing trade-off between TPR and FPR at different thresholds.}}{150}{figure.3.16}\protected@file@percent }
\newlabel{fig:roc-auc}{{3.16}{150}{ROC curve showing trade-off between TPR and FPR at different thresholds}{figure.3.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.9.3}Multi-Class Metrics}{151}{subsection.3.9.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.10}Training Tips}{152}{section.3.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.10.1}Underfitting}{152}{subsection.3.10.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.10.2}Overfitting}{152}{subsection.3.10.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.10.3}Visualising Features}{153}{subsection.3.10.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.17}{\ignorespaces Good training: sparse, structured hidden unit activations.}}{153}{figure.3.17}\protected@file@percent }
\newlabel{fig:good-training}{{3.17}{153}{Good training: sparse, structured hidden unit activations}{figure.3.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.18}{\ignorespaces Poor training: correlated, unstructured activations ignoring input.}}{153}{figure.3.18}\protected@file@percent }
\newlabel{fig:poor-training}{{3.18}{153}{Poor training: correlated, unstructured activations ignoring input}{figure.3.18}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.10.4}Common Issues}{153}{subsection.3.10.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.19}{\ignorespaces Diverging training error indicates learning rate too high or bugs.}}{153}{figure.3.19}\protected@file@percent }
\newlabel{fig:diverge}{{3.19}{153}{Diverging training error indicates learning rate too high or bugs}{figure.3.19}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.10.5}Debugging Neural Networks}{154}{subsection.3.10.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.11}Vanishing Gradient Problem}{155}{section.3.11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.11.1}The Core Problem: Multiplying Small Numbers}{155}{subsection.3.11.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.11.2}Saturation of Sigmoid}{155}{subsection.3.11.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.20}{\ignorespaces Sigmoid function and its derivative. Derivative approaches 0 at extremes.}}{156}{figure.3.20}\protected@file@percent }
\newlabel{fig:vanishing-grad}{{3.20}{156}{Sigmoid function and its derivative. Derivative approaches 0 at extremes}{figure.3.20}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.11.3}Solution 1: ReLU Activation}{157}{subsection.3.11.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.21}{\ignorespaces ReLU activation function.}}{157}{figure.3.21}\protected@file@percent }
\newlabel{fig:relu}{{3.21}{157}{ReLU activation function}{figure.3.21}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.11.4}Solution 2: Batch Normalisation}{159}{subsection.3.11.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Why Does BatchNorm Work?}{162}{section*.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.11.5}Solution 3: Residual Networks (Skip Connections)}{163}{subsection.3.11.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.22}{\ignorespaces Residual block with skip connection.}}{163}{figure.3.22}\protected@file@percent }
\newlabel{fig:resnet}{{3.22}{163}{Residual block with skip connection}{figure.3.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.23}{\ignorespaces Skip connection bypassing one layer.}}{164}{figure.3.23}\protected@file@percent }
\newlabel{fig:skip-connection}{{3.23}{164}{Skip connection bypassing one layer}{figure.3.23}{}}
\@writefile{toc}{\contentsline {subsubsection}{Gradient Flow Analysis}{164}{section*.9}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.24}{\ignorespaces Comparison of VGG-19 (no skip connections), plain 34-layer network, and ResNet-34.}}{167}{figure.3.24}\protected@file@percent }
\newlabel{fig:resnet-comparison}{{3.24}{167}{Comparison of VGG-19 (no skip connections), plain 34-layer network, and ResNet-34}{figure.3.24}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.12}Regularisation Techniques}{168}{section.3.12}\protected@file@percent }
\newlabel{sec:regularisation}{{3.12}{168}{Regularisation Techniques}{section.3.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.12.1}Weight Decay ($L_2$ Regularisation)}{169}{subsection.3.12.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Bayesian Interpretation of $L_2$ Regularisation}{169}{section*.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.12.2}$L_1$ Regularisation (Lasso)}{170}{subsection.3.12.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.12.3}Dropout}{172}{subsection.3.12.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Why Does Dropout Work?}{173}{section*.11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.12.4}Data Augmentation}{174}{subsection.3.12.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.12.5}Regularisation Summary}{176}{subsection.3.12.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.13}Optimisation Landscape}{176}{section.3.13}\protected@file@percent }
\newlabel{sec:optimisation-landscape}{{3.13}{176}{Optimisation Landscape}{section.3.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.13.1}Non-Convexity and Critical Points}{176}{subsection.3.13.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.13.2}The Loss Surface Geometry}{177}{subsection.3.13.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.13.3}The Role of Initialisation}{178}{subsection.3.13.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.14}Optimiser Variants}{179}{section.3.14}\protected@file@percent }
\newlabel{sec:optimisers}{{3.14}{179}{Optimiser Variants}{section.3.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.14.1}Momentum}{179}{subsection.3.14.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.14.2}Adaptive Learning Rate Methods}{180}{subsection.3.14.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.14.3}Adam: Adaptive Moment Estimation}{181}{subsection.3.14.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.14.4}Learning Rate Scheduling}{183}{subsection.3.14.4}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Week 4: Convolutional Neural Networks I}{185}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:week4}{{4}{185}{Week 4: Convolutional Neural Networks I}{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Computer Vision Tasks}{186}{section.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Common computer vision tasks: classification assigns one label to the whole image; detection locates objects with bounding boxes; segmentation classifies every pixel. Each task provides progressively more detailed information about the image content.}}{187}{figure.4.1}\protected@file@percent }
\newlabel{fig:cv-tasks}{{4.1}{187}{Common computer vision tasks: classification assigns one label to the whole image; detection locates objects with bounding boxes; segmentation classifies every pixel. Each task provides progressively more detailed information about the image content}{figure.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Human vs Computer Perception}{188}{subsection.4.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Why Convolutional Layers?}{189}{section.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Challenge 1: Spatial Structure}{189}{subsection.4.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Fully connected network: every neuron in one layer connects to every neuron in the next. For images, this means flattening the 2D spatial structure into a 1D vector, losing all notion of ``which pixels are neighbours.''}}{190}{figure.4.2}\protected@file@percent }
\newlabel{fig:fully-connected}{{4.2}{190}{Fully connected network: every neuron in one layer connects to every neuron in the next. For images, this means flattening the 2D spatial structure into a 1D vector, losing all notion of ``which pixels are neighbours.''}{figure.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Challenge 2: Parameter Explosion}{192}{subsection.4.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Challenge 3: Translation Invariance}{193}{subsection.4.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Properties of CNNs}{195}{section.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Intuitive Summary: What Convolution Does}{197}{subsection.4.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Versatility Beyond Images}{197}{subsection.4.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.4}The Convolution Operation}{198}{section.4.4}\protected@file@percent }
\newlabel{sec:convolution}{{4.4}{198}{The Convolution Operation}{section.4.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}Intuition: Template Matching}{198}{subsection.4.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.2}Continuous Convolution (Mathematical Background)}{199}{subsection.4.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.3}Discrete 2D Convolution}{199}{subsection.4.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.4}Cross-Correlation (What CNNs Actually Compute)}{200}{subsection.4.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.5}Worked Example: Cross-Correlation Step by Step}{202}{subsection.4.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.6}Worked Example: True Convolution}{203}{subsection.4.4.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.7}Effect of Convolution: Feature Detection}{203}{subsection.4.4.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Convolution detecting transitions from dark to light. The kernel acts as a ``horizontal gradient detector,'' producing strong responses at vertical edges where intensity changes from left to right.}}{206}{figure.4.3}\protected@file@percent }
\newlabel{fig:convolution-example}{{4.3}{206}{Convolution detecting transitions from dark to light. The kernel acts as a ``horizontal gradient detector,'' producing strong responses at vertical edges where intensity changes from left to right}{figure.4.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Output Dimensions and Stride}{206}{section.4.5}\protected@file@percent }
\newlabel{sec:output-dims}{{4.5}{206}{Output Dimensions and Stride}{section.4.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.1}Valid Convolution (No Padding)}{207}{subsection.4.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.2}Stride}{207}{subsection.4.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.6}Padding}{209}{section.4.6}\protected@file@percent }
\newlabel{sec:padding}{{4.6}{209}{Padding}{section.4.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.1}The Border Problem}{209}{subsection.4.6.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Padding strategies: zero-padding, mirroring, and continuous extension.}}{209}{figure.4.4}\protected@file@percent }
\newlabel{fig:padding}{{4.4}{209}{Padding strategies: zero-padding, mirroring, and continuous extension}{figure.4.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.2}Padding Strategies}{210}{subsection.4.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.3}Output Dimension Formula with Padding}{210}{subsection.4.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.4}Common Padding Conventions}{211}{subsection.4.6.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.5}Benefits of Zero-Padding}{211}{subsection.4.6.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.7}Pooling Layers}{212}{section.4.7}\protected@file@percent }
\newlabel{sec:pooling}{{4.7}{212}{Pooling Layers}{section.4.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7.1}Motivation: From Local to Global}{212}{subsection.4.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7.2}Max Pooling}{213}{subsection.4.7.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces Max pooling with $2 \times 2$ window and stride 2.}}{213}{figure.4.5}\protected@file@percent }
\newlabel{fig:max-pooling}{{4.5}{213}{Max pooling with $2 \times 2$ window and stride 2}{figure.4.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7.3}Average Pooling}{214}{subsection.4.7.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7.4}Global Pooling}{215}{subsection.4.7.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7.5}Max vs Average Pooling: When to Use Each}{215}{subsection.4.7.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7.6}Local Translation Invariance}{216}{subsection.4.7.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces Pooling provides local translation invariance.}}{216}{figure.4.6}\protected@file@percent }
\newlabel{fig:translation-invariance}{{4.6}{216}{Pooling provides local translation invariance}{figure.4.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces Small shifts in feature position produce identical pooled outputs.}}{216}{figure.4.7}\protected@file@percent }
\newlabel{fig:translation-invariance-2}{{4.7}{216}{Small shifts in feature position produce identical pooled outputs}{figure.4.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.8}Multi-Channel Convolutions}{217}{section.4.8}\protected@file@percent }
\newlabel{sec:multi-channel}{{4.8}{217}{Multi-Channel Convolutions}{section.4.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.8.1}Multiple Input Channels}{217}{subsection.4.8.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces Colour image as 3D tensor: height $\times $ width $\times $ channels.}}{217}{figure.4.8}\protected@file@percent }
\newlabel{fig:multiple-channels}{{4.8}{217}{Colour image as 3D tensor: height $\times $ width $\times $ channels}{figure.4.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.9}{\ignorespaces Two-channel convolution: convolve each channel, then sum. $(1 \cdot 1 + 2 \cdot 2 + 4 \cdot 3 + 5 \cdot 4) + (0 \cdot 0 + 1 \cdot 1 + 3 \cdot 2 + 4 \cdot 3) = 56$}}{218}{figure.4.9}\protected@file@percent }
\newlabel{fig:channel-sum}{{4.9}{218}{Two-channel convolution: convolve each channel, then sum. $(1 \cdot 1 + 2 \cdot 2 + 4 \cdot 3 + 5 \cdot 4) + (0 \cdot 0 + 1 \cdot 1 + 3 \cdot 2 + 4 \cdot 3) = 56$}{figure.4.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.8.2}Multiple Output Channels (Feature Maps)}{219}{subsection.4.8.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.10}{\ignorespaces 3 input channels, 2 output channels (2 filters).}}{219}{figure.4.10}\protected@file@percent }
\newlabel{fig:3in-2out}{{4.10}{219}{3 input channels, 2 output channels (2 filters)}{figure.4.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.11}{\ignorespaces Each filter detects a different feature (e.g., edges at different orientations). Pooling aggregates to ``is this feature present?''}}{220}{figure.4.11}\protected@file@percent }
\newlabel{fig:2-out}{{4.11}{220}{Each filter detects a different feature (e.g., edges at different orientations). Pooling aggregates to ``is this feature present?''}{figure.4.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.8.3}Parameter Efficiency: Weight Sharing}{220}{subsection.4.8.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.9}Translation Equivariance and Invariance}{221}{section.4.9}\protected@file@percent }
\newlabel{sec:equivariance}{{4.9}{221}{Translation Equivariance and Invariance}{section.4.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.10}Receptive Field}{222}{section.4.10}\protected@file@percent }
\newlabel{sec:receptive-field}{{4.10}{222}{Receptive Field}{section.4.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.10.1}Receptive Field and Architecture Design}{224}{subsection.4.10.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.11}Backpropagation Through Convolutions}{224}{section.4.11}\protected@file@percent }
\newlabel{sec:conv-backprop}{{4.11}{224}{Backpropagation Through Convolutions}{section.4.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.11.1}Setup and Notation}{225}{subsection.4.11.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.11.2}Gradient with Respect to Weights}{225}{subsection.4.11.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.11.3}Gradient with Respect to Input}{226}{subsection.4.11.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.11.4}Worked Example: Backprop Through Convolution}{229}{subsection.4.11.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.11.5}Multi-Channel Backpropagation}{230}{subsection.4.11.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.11.6}Backpropagation Through Pooling}{230}{subsection.4.11.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.12}CNN Architecture: LeNet}{231}{section.4.12}\protected@file@percent }
\newlabel{sec:lenet}{{4.12}{231}{CNN Architecture: LeNet}{section.4.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.12}{\ignorespaces LeNet architecture: alternating convolution and pooling layers followed by fully connected layers.}}{231}{figure.4.12}\protected@file@percent }
\newlabel{fig:lenet}{{4.12}{231}{LeNet architecture: alternating convolution and pooling layers followed by fully connected layers}{figure.4.12}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.13}Architecture Design Principles}{233}{section.4.13}\protected@file@percent }
\newlabel{sec:design-principles}{{4.13}{233}{Architecture Design Principles}{section.4.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.13.1}Filter Size Choices}{233}{subsection.4.13.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.13.2}Depth vs Width}{234}{subsection.4.13.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.13.3}Downsampling Strategies}{234}{subsection.4.13.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.14}Training CNNs}{234}{section.4.14}\protected@file@percent }
\newlabel{sec:training}{{4.14}{234}{Training CNNs}{section.4.14}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.15}Feature Visualisation}{235}{section.4.15}\protected@file@percent }
\newlabel{sec:visualisation}{{4.15}{235}{Feature Visualisation}{section.4.15}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.15.1}What Does a CNN Learn?}{235}{subsection.4.15.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.13}{\ignorespaces Feature visualisation in GoogLeNet: progression from edges to objects.}}{236}{figure.4.13}\protected@file@percent }
\newlabel{fig:features-1}{{4.13}{236}{Feature visualisation in GoogLeNet: progression from edges to objects}{figure.4.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.14}{\ignorespaces Early layer features: simple edges and colour gradients.}}{236}{figure.4.14}\protected@file@percent }
\newlabel{fig:features-2}{{4.14}{236}{Early layer features: simple edges and colour gradients}{figure.4.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.15}{\ignorespaces Deeper layer features: complex textures and patterns.}}{237}{figure.4.15}\protected@file@percent }
\newlabel{fig:features-3}{{4.15}{237}{Deeper layer features: complex textures and patterns}{figure.4.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.16}{\ignorespaces Progression from textures to object parts to full objects.}}{237}{figure.4.16}\protected@file@percent }
\newlabel{fig:features-4}{{4.16}{237}{Progression from textures to object parts to full objects}{figure.4.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.15.2}Visualisation Techniques}{238}{subsection.4.15.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.16}Summary: CNN Building Blocks}{239}{section.4.16}\protected@file@percent }
\newlabel{sec:summary}{{4.16}{239}{Summary: CNN Building Blocks}{section.4.16}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Week 5: Convolutional Neural Networks II}{241}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:week5}{{5}{241}{Week 5: Convolutional Neural Networks II}{chapter.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Labelled Data and Augmentation}{241}{section.5.1}\protected@file@percent }
\newlabel{sec:week5-data}{{5.1}{241}{Labelled Data and Augmentation}{section.5.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}The Data Bottleneck}{242}{subsection.5.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces ImageNet: the dataset that enabled the deep learning revolution in computer vision.}}{242}{figure.5.1}\protected@file@percent }
\newlabel{fig:imagenet}{{5.1}{242}{ImageNet: the dataset that enabled the deep learning revolution in computer vision}{figure.5.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces CIFAR-10: smaller benchmark dataset (60,000 images, 10 classes).}}{244}{figure.5.2}\protected@file@percent }
\newlabel{fig:cifar}{{5.2}{244}{CIFAR-10: smaller benchmark dataset (60,000 images, 10 classes)}{figure.5.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.2}Common Datasets}{244}{subsection.5.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Popular computer vision benchmark datasets.}}{244}{figure.5.3}\protected@file@percent }
\newlabel{fig:datasets}{{5.3}{244}{Popular computer vision benchmark datasets}{figure.5.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.3}Data Labelling Strategies}{247}{subsection.5.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Self-Annotating Domain-Specific Data}{247}{section*.12}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces Annotation tools: LabelImg, VGG Image Annotator, and other open-source/paid tools for creating bounding boxes and polygons.}}{247}{figure.5.4}\protected@file@percent }
\newlabel{fig:annotation}{{5.4}{247}{Annotation tools: LabelImg, VGG Image Annotator, and other open-source/paid tools for creating bounding boxes and polygons}{figure.5.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{Who Labels the Data?}{247}{section*.13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Considerations for Data Labelling}{248}{section*.14}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces Edge cases: What distinguishes a truck from a van? Such ambiguities must be resolved before annotation begins.}}{248}{figure.5.5}\protected@file@percent }
\newlabel{fig:truck}{{5.5}{248}{Edge cases: What distinguishes a truck from a van? Such ambiguities must be resolved before annotation begins}{figure.5.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.4}Active Learning}{248}{subsection.5.1.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces Active learning workflow: model queries human for uncertain samples.}}{250}{figure.5.6}\protected@file@percent }
\newlabel{fig:active-learning}{{5.6}{250}{Active learning workflow: model queries human for uncertain samples}{figure.5.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.5}Model-Assisted Labelling}{250}{subsection.5.1.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.6}Data Augmentation}{251}{subsection.5.1.6}\protected@file@percent }
\newlabel{sec:augmentation}{{5.1.6}{251}{Data Augmentation}{subsection.5.1.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces Common augmentation transformations.}}{251}{figure.5.7}\protected@file@percent }
\newlabel{fig:augmentation}{{5.7}{251}{Common augmentation transformations}{figure.5.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{Geometric Augmentation}{251}{section*.15}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Colour Augmentation}{253}{section*.16}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.8}{\ignorespaces Colour channel manipulation on a cat image: this teaches the model to focus on structural details rather than relying on colour information.}}{253}{figure.5.8}\protected@file@percent }
\newlabel{fig:colour-aug-cat}{{5.8}{253}{Colour channel manipulation on a cat image: this teaches the model to focus on structural details rather than relying on colour information}{figure.5.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.9}{\ignorespaces Colour augmentation: adjusting brightness, contrast, saturation, and hue simulates different lighting conditions, making the model more robust to environmental variations.}}{254}{figure.5.9}\protected@file@percent }
\newlabel{fig:colour-aug}{{5.9}{254}{Colour augmentation: adjusting brightness, contrast, saturation, and hue simulates different lighting conditions, making the model more robust to environmental variations}{figure.5.9}{}}
\@writefile{toc}{\contentsline {subsubsection}{Elastic Distortion}{254}{section*.17}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.10}{\ignorespaces Random elastic distortion applied to digit ``6''.}}{255}{figure.5.10}\protected@file@percent }
\newlabel{fig:elastic}{{5.10}{255}{Random elastic distortion applied to digit ``6''}{figure.5.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.11}{\ignorespaces Smoothed random distortion: the distortion field specifies how each pixel is displaced. The smoothing (via Gaussian convolution) ensures the deformation is gentle and continuous, mimicking natural handwriting variations rather than producing jagged artifacts.}}{256}{figure.5.11}\protected@file@percent }
\newlabel{fig:smoothed-elastic}{{5.11}{256}{Smoothed random distortion: the distortion field specifies how each pixel is displaced. The smoothing (via Gaussian convolution) ensures the deformation is gentle and continuous, mimicking natural handwriting variations rather than producing jagged artifacts}{figure.5.11}{}}
\@writefile{toc}{\contentsline {subsubsection}{Advanced Augmentation Techniques}{256}{section*.18}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Modern CNN Architectures}{259}{section.5.2}\protected@file@percent }
\newlabel{sec:week5-architectures}{{5.2}{259}{Modern CNN Architectures}{section.5.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}VGG: Deep and Narrow (2014)}{259}{subsection.5.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{The Intuition Behind VGG}{259}{section*.19}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Basic CNN Block vs VGG Block}{260}{section*.20}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.12}{\ignorespaces VGG block: multiple 3$\times $3 convolutions followed by pooling. Note that pooling only occurs once per block, not after every convolution.}}{260}{figure.5.12}\protected@file@percent }
\newlabel{fig:vgg-block}{{5.12}{260}{VGG block: multiple 3$\times $3 convolutions followed by pooling. Note that pooling only occurs once per block, not after every convolution}{figure.5.12}{}}
\@writefile{toc}{\contentsline {subsubsection}{Why 3$\times $3 Filters?}{260}{section*.21}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.13}{\ignorespaces VGG architecture: deep stack of small convolutions.}}{262}{figure.5.13}\protected@file@percent }
\newlabel{fig:vgg}{{5.13}{262}{VGG architecture: deep stack of small convolutions}{figure.5.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}GoogLeNet: Inception Modules (2014)}{262}{subsection.5.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{The Multi-Scale Intuition}{262}{section*.22}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.14}{\ignorespaces Inception block: parallel branches at different scales, with outputs concatenated along the channel dimension. This allows the network to capture features at multiple spatial resolutions simultaneously.}}{263}{figure.5.14}\protected@file@percent }
\newlabel{fig:inception}{{5.14}{263}{Inception block: parallel branches at different scales, with outputs concatenated along the channel dimension. This allows the network to capture features at multiple spatial resolutions simultaneously}{figure.5.14}{}}
\@writefile{toc}{\contentsline {subsubsection}{1$\times $1 Convolutions: The Workhorse of Modern CNNs}{264}{section*.23}\protected@file@percent }
\newlabel{sec:1x1-conv}{{5.2.2}{264}{1$\times $1 Convolutions: The Workhorse of Modern CNNs}{section*.23}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.15}{\ignorespaces 1$\times $1 convolution: channel reduction/expansion.}}{264}{figure.5.15}\protected@file@percent }
\newlabel{fig:1x1-conv}{{5.15}{264}{1$\times $1 convolution: channel reduction/expansion}{figure.5.15}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.3}ResNet: Skip Connections (2015)}{266}{subsection.5.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{The Problem: Deeper Is Not Always Better}{266}{section*.24}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{The Solution: Make Identity Easy}{266}{section*.25}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.16}{\ignorespaces Residual block: the portion in dotted lines learns the residual $g(x) = f(x) - x$. The skip connection adds the original input back, so the block outputs $f(x) = g(x) + x$.}}{267}{figure.5.16}\protected@file@percent }
\newlabel{fig:residual-block}{{5.16}{267}{Residual block: the portion in dotted lines learns the residual $g(x) = f(x) - x$. The skip connection adds the original input back, so the block outputs $f(x) = g(x) + x$}{figure.5.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.17}{\ignorespaces Standard ResNet block: two 3$\times $3 convolutions with skip connection.}}{268}{figure.5.17}\protected@file@percent }
\newlabel{fig:resnet-block}{{5.17}{268}{Standard ResNet block: two 3$\times $3 convolutions with skip connection}{figure.5.17}{}}
\@writefile{toc}{\contentsline {subsubsection}{Bottleneck ResNet Block}{268}{section*.26}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Global Average Pooling}{269}{section*.27}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{ResNet-18 Architecture}{271}{section*.28}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.18}{\ignorespaces ResNet-18 architecture with global average pooling.}}{271}{figure.5.18}\protected@file@percent }
\newlabel{fig:resnet18}{{5.18}{271}{ResNet-18 architecture with global average pooling}{figure.5.18}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.4}DenseNet: Dense Connectivity (2017)}{272}{subsection.5.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.5}EfficientNet: Compound Scaling (2019)}{273}{subsection.5.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Transfer Learning and Fine-Tuning}{274}{section.5.3}\protected@file@percent }
\newlabel{sec:week5-transfer}{{5.3}{274}{Transfer Learning and Fine-Tuning}{section.5.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.1}Why Transfer Learning Works}{274}{subsection.5.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.19}{\ignorespaces Fine-tuning: copy pretrained layers, replace output layer, train on target dataset. Early layers contain generic features that transfer well; late layers are adapted to the new task.}}{275}{figure.5.19}\protected@file@percent }
\newlabel{fig:fine-tuning}{{5.19}{275}{Fine-tuning: copy pretrained layers, replace output layer, train on target dataset. Early layers contain generic features that transfer well; late layers are adapted to the new task}{figure.5.19}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.2}Feature Extraction vs Fine-Tuning}{276}{subsection.5.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.3}Domain Adaptation}{278}{subsection.5.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Object Detection}{279}{section.5.4}\protected@file@percent }
\newlabel{sec:week5-detection}{{5.4}{279}{Object Detection}{section.5.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.20}{\ignorespaces Object detection: classify and localise objects with bounding boxes. The model must identify that there are two dogs, locate each one, and draw tight bounding boxes around them.}}{279}{figure.5.20}\protected@file@percent }
\newlabel{fig:object-detection}{{5.20}{279}{Object detection: classify and localise objects with bounding boxes. The model must identify that there are two dogs, locate each one, and draw tight bounding boxes around them}{figure.5.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.21}{\ignorespaces Object detection involves finding objects, drawing tight bounding boxes, and determining object classes. Multiple objects of multiple classes can appear in a single image.}}{280}{figure.5.21}\protected@file@percent }
\newlabel{fig:object-detection-2}{{5.21}{280}{Object detection involves finding objects, drawing tight bounding boxes, and determining object classes. Multiple objects of multiple classes can appear in a single image}{figure.5.21}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.1}Why Object Detection Is Harder Than Classification}{280}{subsection.5.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.22}{\ignorespaces Detecting multiple objects of different classes in a single image. The detector must output a variable number of boxes with different class labels.}}{281}{figure.5.22}\protected@file@percent }
\newlabel{fig:multiple-objects}{{5.22}{281}{Detecting multiple objects of different classes in a single image. The detector must output a variable number of boxes with different class labels}{figure.5.22}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.2}Bounding Box Representation}{282}{subsection.5.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.3}Intersection over Union (IoU)}{282}{subsection.5.4.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.23}{\ignorespaces IoU measures overlap between predicted and ground truth boxes.}}{282}{figure.5.23}\protected@file@percent }
\newlabel{fig:iou}{{5.23}{282}{IoU measures overlap between predicted and ground truth boxes}{figure.5.23}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.4}Anchor Boxes}{283}{subsection.5.4.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.24}{\ignorespaces Anchor boxes: predefined boxes at various scales and aspect ratios, centred at grid points across the feature map.}}{284}{figure.5.24}\protected@file@percent }
\newlabel{fig:anchor-boxes}{{5.24}{284}{Anchor boxes: predefined boxes at various scales and aspect ratios, centred at grid points across the feature map}{figure.5.24}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.25}{\ignorespaces Anchor boxes at different scales and aspect ratios provide coverage for objects of varying shapes-tall and thin (people), wide and short (cars), or roughly square (faces).}}{284}{figure.5.25}\protected@file@percent }
\newlabel{fig:anchor-boxes-2}{{5.25}{284}{Anchor boxes at different scales and aspect ratios provide coverage for objects of varying shapes-tall and thin (people), wide and short (cars), or roughly square (faces)}{figure.5.25}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.5}Class Prediction and Confidence Scores}{285}{subsection.5.4.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.26}{\ignorespaces Class prediction: for each anchor box, the model predicts class probabilities and confidence scores.}}{285}{figure.5.26}\protected@file@percent }
\newlabel{fig:class-prediction}{{5.26}{285}{Class prediction: for each anchor box, the model predicts class probabilities and confidence scores}{figure.5.26}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.6}Non-Maximum Suppression (NMS)}{286}{subsection.5.4.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.27}{\ignorespaces Object detection pipeline: propose boxes, predict classes, apply NMS.}}{287}{figure.5.27}\protected@file@percent }
\newlabel{fig:detection-workflow}{{5.27}{287}{Object detection pipeline: propose boxes, predict classes, apply NMS}{figure.5.27}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.7}R-CNN Family: Region-Based Detection}{287}{subsection.5.4.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.8}YOLO: Single-Shot Detection}{289}{subsection.5.4.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.9}SSD: Single Shot MultiBox Detector}{291}{subsection.5.4.9}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.28}{\ignorespaces SSD architecture: predictions at multiple feature map scales.}}{291}{figure.5.28}\protected@file@percent }
\newlabel{fig:ssd}{{5.28}{291}{SSD architecture: predictions at multiple feature map scales}{figure.5.28}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.29}{\ignorespaces Multi-scale anchor boxes on different feature maps: higher resolution maps detect smaller objects; lower resolution maps detect larger objects.}}{292}{figure.5.29}\protected@file@percent }
\newlabel{fig:multiscale-anchors}{{5.29}{292}{Multi-scale anchor boxes on different feature maps: higher resolution maps detect smaller objects; lower resolution maps detect larger objects}{figure.5.29}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.30}{\ignorespaces Different feature maps at different scales with uniformly distributed anchor boxes. The same anchor box templates, applied at different feature map resolutions, effectively cover objects at multiple scales.}}{292}{figure.5.30}\protected@file@percent }
\newlabel{fig:anchor-boxes-4}{{5.30}{292}{Different feature maps at different scales with uniformly distributed anchor boxes. The same anchor box templates, applied at different feature map resolutions, effectively cover objects at multiple scales}{figure.5.30}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.31}{\ignorespaces Smooth L1 loss (Huber loss): combines L2 stability for small errors with L1 robustness to outliers. The transition occurs at $|x| = \delta $ (typically 1).}}{293}{figure.5.31}\protected@file@percent }
\newlabel{fig:l1-loss}{{5.31}{293}{Smooth L1 loss (Huber loss): combines L2 stability for small errors with L1 robustness to outliers. The transition occurs at $|x| = \delta $ (typically 1)}{figure.5.31}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.10}Data Augmentation for Object Detection}{294}{subsection.5.4.10}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Semantic Segmentation}{295}{section.5.5}\protected@file@percent }
\newlabel{sec:week5-segmentation}{{5.5}{295}{Semantic Segmentation}{section.5.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.32}{\ignorespaces Semantic segmentation: classify every pixel. Unlike object detection (bounding boxes), segmentation produces pixel-perfect boundaries.}}{296}{figure.5.32}\protected@file@percent }
\newlabel{fig:semantic-seg}{{5.32}{296}{Semantic segmentation: classify every pixel. Unlike object detection (bounding boxes), segmentation produces pixel-perfect boundaries}{figure.5.32}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.33}{\ignorespaces Semantic segmentation example: the input image (left) is transformed into a pixel-wise class map (right), where each colour represents a different semantic class.}}{296}{figure.5.33}\protected@file@percent }
\newlabel{fig:semantic-seg-2}{{5.33}{296}{Semantic segmentation example: the input image (left) is transformed into a pixel-wise class map (right), where each colour represents a different semantic class}{figure.5.33}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.34}{\ignorespaces Different types of image segmentation: from simple region-based approaches to semantic segmentation that assigns class labels to each pixel.}}{297}{figure.5.34}\protected@file@percent }
\newlabel{fig:image-segmentation}{{5.34}{297}{Different types of image segmentation: from simple region-based approaches to semantic segmentation that assigns class labels to each pixel}{figure.5.34}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.1}Types of Segmentation}{297}{subsection.5.5.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.35}{\ignorespaces Instance segmentation distinguishes individual objects of the same class (e.g., different people get different colours/masks).}}{298}{figure.5.35}\protected@file@percent }
\newlabel{fig:instance-seg}{{5.35}{298}{Instance segmentation distinguishes individual objects of the same class (e.g., different people get different colours/masks)}{figure.5.35}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.2}The Challenge: Spatial Resolution}{298}{subsection.5.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.3}Fully Convolutional Networks (FCN)}{299}{subsection.5.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.4}Transposed Convolution}{300}{subsection.5.5.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.36}{\ignorespaces Transposed convolution increases spatial resolution.}}{300}{figure.5.36}\protected@file@percent }
\newlabel{fig:upconv}{{5.36}{300}{Transposed convolution increases spatial resolution}{figure.5.36}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.5}U-Net Architecture}{302}{subsection.5.5.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.37}{\ignorespaces U-Net: encoder-decoder with skip connections.}}{302}{figure.5.37}\protected@file@percent }
\newlabel{fig:unet}{{5.37}{302}{U-Net: encoder-decoder with skip connections}{figure.5.37}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.6}Segmentation Loss Functions}{304}{subsection.5.5.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.6}Chapter Summary}{307}{section.5.6}\protected@file@percent }
\newlabel{sec:week5-summary}{{5.6}{307}{Chapter Summary}{section.5.6}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Week 6: Recurrent Neural Networks and Sequence Modeling}{309}{chapter.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:week6}{{6}{310}{Week 6: Recurrent Neural Networks and Sequence Modeling}{chapter.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Introduction to Sequence Modeling}{311}{section.6.1}\protected@file@percent }
\newlabel{sec:intro-sequence}{{6.1}{311}{Introduction to Sequence Modeling}{section.6.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces Log files as time series data.}}{313}{figure.6.1}\protected@file@percent }
\newlabel{fig:log-files}{{6.1}{313}{Log files as time series data}{figure.6.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.1}Challenges in Modeling Sequential Data}{313}{subsection.6.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.2}Time Series in Public Policy}{313}{subsection.6.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces Time series examples: market prices, sensor values, system logs.}}{313}{figure.6.2}\protected@file@percent }
\newlabel{fig:time-series}{{6.2}{313}{Time series examples: market prices, sensor values, system logs}{figure.6.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Sequence Modeling Tasks}{314}{section.6.2}\protected@file@percent }
\newlabel{sec:sequence-tasks}{{6.2}{314}{Sequence Modeling Tasks}{section.6.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.1}Forecasting and Predicting Next Steps}{314}{subsection.6.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces Electricity load forecasting: predicting consumption patterns is crucial for grid management and energy planning.}}{314}{figure.6.3}\protected@file@percent }
\newlabel{fig:load-forecasting}{{6.3}{314}{Electricity load forecasting: predicting consumption patterns is crucial for grid management and energy planning}{figure.6.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces Search query completion: predictive text algorithms anticipate the next words in a query based on previous user inputs.}}{315}{figure.6.4}\protected@file@percent }
\newlabel{fig:search-query}{{6.4}{315}{Search query completion: predictive text algorithms anticipate the next words in a query based on previous user inputs}{figure.6.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.2}Classification}{315}{subsection.6.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.5}{\ignorespaces Non-Intrusive Load Monitoring (NILM): uses energy consumption patterns from electrical devices to classify which appliances are active based on their unique power signatures.}}{315}{figure.6.5}\protected@file@percent }
\newlabel{fig:nilm}{{6.5}{315}{Non-Intrusive Load Monitoring (NILM): uses energy consumption patterns from electrical devices to classify which appliances are active based on their unique power signatures}{figure.6.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.6}{\ignorespaces Sound classification: identifying types of sounds (e.g., engine noise, sirens, or speech) from audio recordings by analysing frequency and amplitude patterns over time.}}{316}{figure.6.6}\protected@file@percent }
\newlabel{fig:sound-classification}{{6.6}{316}{Sound classification: identifying types of sounds (e.g., engine noise, sirens, or speech) from audio recordings by analysing frequency and amplitude patterns over time}{figure.6.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.3}Clustering}{316}{subsection.6.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.7}{\ignorespaces Clusters of load profiles of industrial customers determined by k-Means clustering. By clustering load profiles, energy companies can group customers with similar usage patterns, helping them offer tailored tariffs or demand response strategies.}}{316}{figure.6.7}\protected@file@percent }
\newlabel{fig:clustering}{{6.7}{316}{Clusters of load profiles of industrial customers determined by k-Means clustering. By clustering load profiles, energy companies can group customers with similar usage patterns, helping them offer tailored tariffs or demand response strategies}{figure.6.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.4}Pattern Matching}{316}{subsection.6.2.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.8}{\ignorespaces Pattern matching: finding heartbeat patterns in continuous signals. In medical data, pattern matching can locate heartbeat patterns within a continuous signal to monitor health conditions.}}{317}{figure.6.8}\protected@file@percent }
\newlabel{fig:pattern-matching}{{6.8}{317}{Pattern matching: finding heartbeat patterns in continuous signals. In medical data, pattern matching can locate heartbeat patterns within a continuous signal to monitor health conditions}{figure.6.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.5}Anomaly Detection}{317}{subsection.6.2.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.9}{\ignorespaces Anomaly detection in time series: identifying unusual subsequences that deviate from expected patterns.}}{317}{figure.6.9}\protected@file@percent }
\newlabel{fig:anomaly-detection}{{6.9}{317}{Anomaly detection in time series: identifying unusual subsequences that deviate from expected patterns}{figure.6.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.6}Motif Detection}{318}{subsection.6.2.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.10}{\ignorespaces Motif detection: finding frequently recurring patterns within sequences.}}{318}{figure.6.10}\protected@file@percent }
\newlabel{fig:motif-detection}{{6.10}{318}{Motif detection: finding frequently recurring patterns within sequences}{figure.6.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Approaches to Sequence Modeling}{318}{section.6.3}\protected@file@percent }
\newlabel{sec:approaches}{{6.3}{318}{Approaches to Sequence Modeling}{section.6.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.11}{\ignorespaces Feature engineering vs end-to-end learning for sequences. The choice between approaches depends on data complexity, domain knowledge availability, and computational resources.}}{318}{figure.6.11}\protected@file@percent }
\newlabel{fig:approaches}{{6.11}{318}{Feature engineering vs end-to-end learning for sequences. The choice between approaches depends on data complexity, domain knowledge availability, and computational resources}{figure.6.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.1}Feature Engineering for Text: Bag-of-Words}{319}{subsection.6.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.12}{\ignorespaces Bag-of-Words representation: each sequence is mapped to a fixed-length vector indicating word occurrence, but the model loses information about word order.}}{319}{figure.6.12}\protected@file@percent }
\newlabel{fig:bow}{{6.12}{319}{Bag-of-Words representation: each sequence is mapped to a fixed-length vector indicating word occurrence, but the model loses information about word order}{figure.6.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.2}Challenges in Raw Sequence Modelling}{320}{subsection.6.3.2}\protected@file@percent }
\newlabel{fig:challenge1}{{6.3.2}{321}{Challenges in Raw Sequence Modelling}{subsection.6.3.2}{}}
\newlabel{fig:challenge2}{{6.3.2}{321}{Challenges in Raw Sequence Modelling}{subsection.6.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.13}{\ignorespaces Challenges in raw sequence modelling: fixed-size requirements and multi-scale temporal dependencies.}}{321}{figure.6.13}\protected@file@percent }
\newlabel{fig:challenge3}{{6.13}{321}{Challenges in raw sequence modelling: fixed-size requirements and multi-scale temporal dependencies}{figure.6.13}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.4}Recurrent Neural Networks (RNNs)}{322}{section.6.4}\protected@file@percent }
\newlabel{sec:rnn}{{6.4}{322}{Recurrent Neural Networks (RNNs)}{section.6.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.1}Why Not Fully Connected Networks?}{322}{subsection.6.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.14}{\ignorespaces Fully connected networks require fixed input dimension.}}{322}{figure.6.14}\protected@file@percent }
\newlabel{fig:fc-network}{{6.14}{322}{Fully connected networks require fixed input dimension}{figure.6.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.2}The Recurrence Mechanism}{323}{subsection.6.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.15}{\ignorespaces The recurrence relationship: hidden state updated at each step.}}{323}{figure.6.15}\protected@file@percent }
\newlabel{fig:recurrence}{{6.15}{323}{The recurrence relationship: hidden state updated at each step}{figure.6.15}{}}
\newlabel{def:rnn-formal}{{6.4.2}{324}{The Recurrence Mechanism}{figure.6.15}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.3}Unrolling an RNN}{325}{subsection.6.4.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.16}{\ignorespaces Unrolled RNN: same network applied at each time step with shared parameters. Each copy of the network shares the same parameters and structure, and the hidden state $h_t$ at each time step is passed to the next time step.}}{325}{figure.6.16}\protected@file@percent }
\newlabel{fig:unrolled-rnn}{{6.16}{325}{Unrolled RNN: same network applied at each time step with shared parameters. Each copy of the network shares the same parameters and structure, and the hidden state $h_t$ at each time step is passed to the next time step}{figure.6.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.17}{\ignorespaces Alternative view of an unrolled RNN showing the flow of information through time.}}{325}{figure.6.17}\protected@file@percent }
\newlabel{fig:unrolled-rnn2}{{6.17}{325}{Alternative view of an unrolled RNN showing the flow of information through time}{figure.6.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.18}{\ignorespaces Key notation for RNN diagrams.}}{325}{figure.6.18}\protected@file@percent }
\newlabel{fig:key-notation}{{6.18}{325}{Key notation for RNN diagrams}{figure.6.18}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.4}Vanilla RNN Formulation}{326}{subsection.6.4.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.19}{\ignorespaces Vanilla RNN architecture: a single layer with recurrent connections.}}{326}{figure.6.19}\protected@file@percent }
\newlabel{fig:vanilla-rnn}{{6.19}{326}{Vanilla RNN architecture: a single layer with recurrent connections}{figure.6.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.20}{\ignorespaces Vanilla RNN unit: the basic building block of recurrent networks.}}{327}{figure.6.20}\protected@file@percent }
\newlabel{fig:vanilla-rnn-unit}{{6.20}{327}{Vanilla RNN unit: the basic building block of recurrent networks}{figure.6.20}{}}
\newlabel{def:vanilla-rnn}{{6.4.4}{327}{Vanilla RNN Formulation}{figure.6.20}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.5}RNN Architectures}{329}{subsection.6.4.5}\protected@file@percent }
\newlabel{sec:rnn-architectures}{{6.4.5}{329}{RNN Architectures}{subsection.6.4.5}{}}
\newlabel{def:bidirectional-rnn}{{6.4.5}{331}{RNN Architectures}{subsection.6.4.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.6}Output Layers and Vector Notation}{332}{subsection.6.4.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.21}{\ignorespaces RNN with hidden state showing the relationship between inputs, hidden states, and outputs.}}{332}{figure.6.21}\protected@file@percent }
\newlabel{fig:vector-notation}{{6.21}{332}{RNN with hidden state showing the relationship between inputs, hidden states, and outputs}{figure.6.21}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.5}Backpropagation Through Time (BPTT)}{334}{section.6.5}\protected@file@percent }
\newlabel{sec:bptt}{{6.5}{334}{Backpropagation Through Time (BPTT)}{section.6.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.22}{\ignorespaces Computational graph for BPTT showing dependencies across time. Boxes represent variables (not shaded) or parameters (shaded), and circles represent operators. The loss $L$ depends on the $y$s and the $o$s (activations), which are themselves dependent on the previous hidden states ($h$s), which depend on the repeated weight matrices $W$.}}{335}{figure.6.22}\protected@file@percent }
\newlabel{fig:bptt}{{6.22}{335}{Computational graph for BPTT showing dependencies across time. Boxes represent variables (not shaded) or parameters (shaded), and circles represent operators. The loss $L$ depends on the $y$s and the $o$s (activations), which are themselves dependent on the previous hidden states ($h$s), which depend on the repeated weight matrices $W$}{figure.6.22}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.1}The Computational Graph}{335}{subsection.6.5.1}\protected@file@percent }
\newlabel{eq:rnn-preact}{{6.1}{335}{The Computational Graph}{equation.6.1}{}}
\newlabel{eq:rnn-hidden}{{6.2}{335}{The Computational Graph}{equation.6.2}{}}
\newlabel{eq:rnn-output}{{6.3}{335}{The Computational Graph}{equation.6.3}{}}
\newlabel{eq:rnn-softmax}{{6.4}{335}{The Computational Graph}{equation.6.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.2}BPTT Derivation}{336}{subsection.6.5.2}\protected@file@percent }
\newlabel{thm:bptt-gradient}{{6.5.2}{336}{BPTT Derivation}{subsection.6.5.2}{}}
\newlabel{derivation:bptt-detailed}{{6.5.2}{337}{BPTT Derivation}{subsection.6.5.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.3}The Vanishing and Exploding Gradient Problem}{338}{subsection.6.5.3}\protected@file@percent }
\newlabel{sec:vanishing-gradient}{{6.5.3}{338}{The Vanishing and Exploding Gradient Problem}{subsection.6.5.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.23}{\ignorespaces Short-term context: ``the clouds are in the \textit  {sky}'' - easy for RNNs.}}{338}{figure.6.23}\protected@file@percent }
\newlabel{fig:short-term}{{6.23}{338}{Short-term context: ``the clouds are in the \textit {sky}'' - easy for RNNs}{figure.6.23}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.24}{\ignorespaces Long-term context: ``I grew up in France... I speak fluent \textit  {French}'' - difficult for vanilla RNNs.}}{338}{figure.6.24}\protected@file@percent }
\newlabel{fig:long-term}{{6.24}{338}{Long-term context: ``I grew up in France... I speak fluent \textit {French}'' - difficult for vanilla RNNs}{figure.6.24}{}}
\newlabel{thm:vanishing-gradient}{{6.5.3}{339}{The Vanishing and Exploding Gradient Problem}{figure.6.24}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.4}Truncated BPTT}{341}{subsection.6.5.4}\protected@file@percent }
\newlabel{def:truncated-bptt}{{6.5.4}{341}{Truncated BPTT}{subsection.6.5.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.6}Long Short-Term Memory (LSTM)}{341}{section.6.6}\protected@file@percent }
\newlabel{sec:lstm}{{6.6}{341}{Long Short-Term Memory (LSTM)}{section.6.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.25}{\ignorespaces LSTM architecture with gates and cell state.}}{342}{figure.6.25}\protected@file@percent }
\newlabel{fig:lstm}{{6.25}{342}{LSTM architecture with gates and cell state}{figure.6.25}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.26}{\ignorespaces LSTM unit: the core building block showing the interaction between gates and states.}}{342}{figure.6.26}\protected@file@percent }
\newlabel{fig:lstm-unit}{{6.26}{342}{LSTM unit: the core building block showing the interaction between gates and states}{figure.6.26}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.6.1}The Key Insight: Additive Updates}{342}{subsection.6.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.6.2}Cell State and Hidden State}{343}{subsection.6.6.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.27}{\ignorespaces Cell state flows through time with minimal modification, acting as a memory highway.}}{343}{figure.6.27}\protected@file@percent }
\newlabel{fig:cell-state}{{6.27}{343}{Cell state flows through time with minimal modification, acting as a memory highway}{figure.6.27}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.28}{\ignorespaces Cell state detail: the current state (both $h_t$ and $c_t$) depends on the current input value $x_t$, previous hidden state $h_{t-1}$, and previous cell state $c_{t-1}$.}}{343}{figure.6.28}\protected@file@percent }
\newlabel{fig:cell-state-2}{{6.28}{343}{Cell state detail: the current state (both $h_t$ and $c_t$) depends on the current input value $x_t$, previous hidden state $h_{t-1}$, and previous cell state $c_{t-1}$}{figure.6.28}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.6.3}The Three Gates}{344}{subsection.6.6.3}\protected@file@percent }
\newlabel{def:lstm-equations}{{6.6.3}{344}{The Three Gates}{subsection.6.6.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.29}{\ignorespaces Forget gate: sigmoid outputs 0 (forget) to 1 (retain).}}{344}{figure.6.29}\protected@file@percent }
\newlabel{fig:forget-gate}{{6.29}{344}{Forget gate: sigmoid outputs 0 (forget) to 1 (retain)}{figure.6.29}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.30}{\ignorespaces Forget gate detail: the sigmoid function determines what fraction of each cell state component to retain.}}{345}{figure.6.30}\protected@file@percent }
\newlabel{fig:forget-gate-2}{{6.30}{345}{Forget gate detail: the sigmoid function determines what fraction of each cell state component to retain}{figure.6.30}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.31}{\ignorespaces Input gate: controls addition of new information.}}{345}{figure.6.31}\protected@file@percent }
\newlabel{fig:input-gate}{{6.31}{345}{Input gate: controls addition of new information}{figure.6.31}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.32}{\ignorespaces Input gate detail: the combination of $i_t$ and $\tilde  {c}_t$ determines what new information enters the cell state.}}{345}{figure.6.32}\protected@file@percent }
\newlabel{fig:input-gate-2}{{6.32}{345}{Input gate detail: the combination of $i_t$ and $\tilde {c}_t$ determines what new information enters the cell state}{figure.6.32}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.33}{\ignorespaces Output gate: controls what cell state is exposed as the hidden state.}}{346}{figure.6.33}\protected@file@percent }
\newlabel{fig:output-gate}{{6.33}{346}{Output gate: controls what cell state is exposed as the hidden state}{figure.6.33}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.6.4}Gate Mechanisms in Detail}{347}{subsection.6.6.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.6.5}Why LSTMs Solve the Vanishing Gradient Problem}{350}{subsection.6.6.5}\protected@file@percent }
\newlabel{thm:lstm-gradient-flow}{{6.6.5}{350}{Why LSTMs Solve the Vanishing Gradient Problem}{subsection.6.6.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.6.6}LSTM Variants}{351}{subsection.6.6.6}\protected@file@percent }
\newlabel{def:peephole}{{6.6.6}{351}{LSTM Variants}{subsection.6.6.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.7}Gated Recurrent Units (GRUs)}{352}{section.6.7}\protected@file@percent }
\newlabel{sec:gru}{{6.7}{352}{Gated Recurrent Units (GRUs)}{section.6.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.34}{\ignorespaces GRU architecture: simpler than LSTM with comparable performance.}}{353}{figure.6.34}\protected@file@percent }
\newlabel{fig:gru}{{6.34}{353}{GRU architecture: simpler than LSTM with comparable performance}{figure.6.34}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.7.1}GRU Equations}{353}{subsection.6.7.1}\protected@file@percent }
\newlabel{def:gru-equations}{{6.7.1}{353}{GRU Equations}{subsection.6.7.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.7.2}GRU vs LSTM Comparison}{354}{subsection.6.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.7.3}Limitations of LSTM and GRU}{355}{subsection.6.7.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.8}Convolutional Neural Networks for Sequences}{355}{section.6.8}\protected@file@percent }
\newlabel{sec:cnn-sequences}{{6.8}{355}{Convolutional Neural Networks for Sequences}{section.6.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.35}{\ignorespaces CNN for sequences: windows of data fed through convolutional layers.}}{356}{figure.6.35}\protected@file@percent }
\newlabel{fig:cnn-sequence}{{6.35}{356}{CNN for sequences: windows of data fed through convolutional layers}{figure.6.35}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.8.1}1D Convolutions}{356}{subsection.6.8.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.36}{\ignorespaces 1D convolution operation on a sequence.}}{357}{figure.6.36}\protected@file@percent }
\newlabel{fig:1d-conv}{{6.36}{357}{1D convolution operation on a sequence}{figure.6.36}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.37}{\ignorespaces Alternative view of 1D convolution showing the sliding window operation.}}{357}{figure.6.37}\protected@file@percent }
\newlabel{fig:1d-conv-alt}{{6.37}{357}{Alternative view of 1D convolution showing the sliding window operation}{figure.6.37}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.8.2}Causal Convolutions}{358}{subsection.6.8.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.38}{\ignorespaces Causal convolutions: each output depends only on past inputs.}}{359}{figure.6.38}\protected@file@percent }
\newlabel{fig:causal-conv}{{6.38}{359}{Causal convolutions: each output depends only on past inputs}{figure.6.38}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.8.3}Dilated Convolutions}{359}{subsection.6.8.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.39}{\ignorespaces Dilated convolutions: larger receptive field without more parameters.}}{360}{figure.6.39}\protected@file@percent }
\newlabel{fig:dilated-conv}{{6.39}{360}{Dilated convolutions: larger receptive field without more parameters}{figure.6.39}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.9}Temporal Convolutional Networks}{361}{section.6.9}\protected@file@percent }
\newlabel{sec:tcn}{{6.9}{361}{Temporal Convolutional Networks}{section.6.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.40}{\ignorespaces WaveNet: dilated causal convolutions for audio generation.}}{361}{figure.6.40}\protected@file@percent }
\newlabel{fig:wavenet}{{6.40}{361}{WaveNet: dilated causal convolutions for audio generation}{figure.6.40}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.41}{\ignorespaces TCN architecture with residual connections.}}{362}{figure.6.41}\protected@file@percent }
\newlabel{fig:tcn}{{6.41}{362}{TCN architecture with residual connections}{figure.6.41}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.10}Introduction to Attention Mechanisms}{362}{section.6.10}\protected@file@percent }
\newlabel{sec:attention-intro}{{6.10}{362}{Introduction to Attention Mechanisms}{section.6.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.10.1}Motivation: The Bottleneck Problem}{363}{subsection.6.10.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.10.2}Basic Attention Mechanism}{364}{subsection.6.10.2}\protected@file@percent }
\newlabel{def:attention-qkv}{{6.10.2}{364}{Basic Attention Mechanism}{subsection.6.10.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.10.3}Self-Attention Preview}{365}{subsection.6.10.3}\protected@file@percent }
\newlabel{def:self-attention}{{6.10.3}{365}{Self-Attention Preview}{subsection.6.10.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.11}Time Series Forecasting}{366}{section.6.11}\protected@file@percent }
\newlabel{sec:time-series}{{6.11}{366}{Time Series Forecasting}{section.6.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.11.1}When to Use Deep Learning for Time Series}{366}{subsection.6.11.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.42}{\ignorespaces Tree ensembles as intermediate step before deep learning.}}{367}{figure.6.42}\protected@file@percent }
\newlabel{fig:tree-ensemble}{{6.42}{367}{Tree ensembles as intermediate step before deep learning}{figure.6.42}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.12}Transformers (Preview)}{367}{section.6.12}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.43}{\ignorespaces Transformer architecture.}}{368}{figure.6.43}\protected@file@percent }
\newlabel{fig:transformer}{{6.43}{368}{Transformer architecture}{figure.6.43}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.13}Summary}{369}{section.6.13}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Week 7: Natural Language Processing I}{371}{chapter.7}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:week7}{{7}{371}{Week 7: Natural Language Processing I}{chapter.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Text and Public Policy}{372}{section.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.1}Example Applications}{373}{subsection.7.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.1}{\ignorespaces Climate risk disclosure identification in corporate reports.}}{373}{figure.7.1}\protected@file@percent }
\newlabel{fig:climate-risk}{{7.1}{373}{Climate risk disclosure identification in corporate reports}{figure.7.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Common NLP Tasks}{374}{section.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7.3}Text as Data}{374}{section.7.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7.4}Text Preprocessing}{375}{section.7.4}\protected@file@percent }
\newlabel{sec:text-preprocessing}{{7.4}{375}{Text Preprocessing}{section.7.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4.1}Tokenisation Strategies}{375}{subsection.7.4.1}\protected@file@percent }
\newlabel{subsec:tokenisation}{{7.4.1}{375}{Tokenisation Strategies}{subsection.7.4.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{Word-Level Tokenisation}{376}{section*.29}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Character-Level Tokenisation}{377}{section*.30}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Subword Tokenisation}{377}{section*.31}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4.2}Further Preprocessing Techniques}{381}{subsection.7.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.2}{\ignorespaces Zipf's law: word frequency follows power law-few tokens occur very frequently, many tokens occur rarely.}}{381}{figure.7.2}\protected@file@percent }
\newlabel{fig:zipf}{{7.2}{381}{Zipf's law: word frequency follows power law-few tokens occur very frequently, many tokens occur rarely}{figure.7.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.5}Classical Document Representations}{382}{section.7.5}\protected@file@percent }
\newlabel{sec:doc-embeddings}{{7.5}{382}{Classical Document Representations}{section.7.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.1}Bag of Words (BoW)}{382}{subsection.7.5.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.3}{\ignorespaces Bag of Words: document as unordered collection of word counts.}}{382}{figure.7.3}\protected@file@percent }
\newlabel{fig:bow}{{7.3}{382}{Bag of Words: document as unordered collection of word counts}{figure.7.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.4}{\ignorespaces Word occurrence matrix / count vectorisation.}}{383}{figure.7.4}\protected@file@percent }
\newlabel{fig:bow-matrix}{{7.4}{383}{Word occurrence matrix / count vectorisation}{figure.7.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.2}TF-IDF}{383}{subsection.7.5.2}\protected@file@percent }
\newlabel{subsec:tfidf}{{7.5.2}{383}{TF-IDF}{subsection.7.5.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.3}Visualising Embeddings}{386}{subsection.7.5.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.5}{\ignorespaces Document embeddings: documents mapped to a space where similar topics cluster (e.g., government borrowings, energy markets).}}{386}{figure.7.5}\protected@file@percent }
\newlabel{fig:doc-embeddings}{{7.5}{386}{Document embeddings: documents mapped to a space where similar topics cluster (e.g., government borrowings, energy markets)}{figure.7.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.6}{\ignorespaces Word embeddings: words like ``development'' and ``transactions'' are closer due to contextual similarity, indicating embeddings capture semantic meaning.}}{387}{figure.7.6}\protected@file@percent }
\newlabel{fig:word-embeddings}{{7.6}{387}{Word embeddings: words like ``development'' and ``transactions'' are closer due to contextual similarity, indicating embeddings capture semantic meaning}{figure.7.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.4}Simple NLP Pipeline for Document Classification}{387}{subsection.7.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7.6}Deep Learning for NLP: Architecture}{388}{section.7.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.7}{\ignorespaces Modular NLP architecture: pretraining $\rightarrow $ architecture $\rightarrow $ application.}}{388}{figure.7.7}\protected@file@percent }
\newlabel{fig:dl-nlp}{{7.7}{388}{Modular NLP architecture: pretraining $\rightarrow $ architecture $\rightarrow $ application}{figure.7.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.7}Word Embeddings I: One-Hot Encoding}{389}{section.7.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.8}{\ignorespaces One-hot encoding: sparse, high-dimensional, no semantic similarity.}}{389}{figure.7.8}\protected@file@percent }
\newlabel{fig:one-hot}{{7.8}{389}{One-hot encoding: sparse, high-dimensional, no semantic similarity}{figure.7.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.8}Word Embeddings II: Word2Vec}{390}{section.7.8}\protected@file@percent }
\newlabel{sec:word2vec}{{7.8}{390}{Word Embeddings II: Word2Vec}{section.7.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.8.1}The Distributional Hypothesis}{391}{subsection.7.8.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.8.2}Skip-Gram Model}{391}{subsection.7.8.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.9}{\ignorespaces Skip-Gram: $P(\text  {``the''}, \text  {``man''}, \text  {``his''}, \text  {``son''} \mid \text  {``loves''})$.}}{391}{figure.7.9}\protected@file@percent }
\newlabel{fig:skip-gram}{{7.9}{391}{Skip-Gram: $P(\text {``the''}, \text {``man''}, \text {``his''}, \text {``son''} \mid \text {``loves''})$}{figure.7.9}{}}
\@writefile{toc}{\contentsline {subsubsection}{Model Setup}{391}{section*.32}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Objective Function}{393}{section*.33}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Training Process}{394}{section*.34}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.10}{\ignorespaces Prediction task: predict context words $w^{(t+j)}$ from centre word $w^{(t)}$.}}{394}{figure.7.10}\protected@file@percent }
\newlabel{fig:skipgram-task}{{7.10}{394}{Prediction task: predict context words $w^{(t+j)}$ from centre word $w^{(t)}$}{figure.7.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.11}{\ignorespaces Training pairs from ``the quick brown fox...''}}{395}{figure.7.11}\protected@file@percent }
\newlabel{fig:quick-brown-fox}{{7.11}{395}{Training pairs from ``the quick brown fox...''}{figure.7.11}{}}
\@writefile{toc}{\contentsline {subsubsection}{Network Architecture}{395}{section*.35}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.12}{\ignorespaces Skip-Gram architecture. Input = centre word $v_i$; output = context word $u_j$; embedding matrix $W$ contains centre word vectors.}}{395}{figure.7.12}\protected@file@percent }
\newlabel{fig:skip-gram-arch}{{7.12}{395}{Skip-Gram architecture. Input = centre word $v_i$; output = context word $u_j$; embedding matrix $W$ contains centre word vectors}{figure.7.12}{}}
\@writefile{toc}{\contentsline {subsubsection}{Gradient Derivation}{399}{section*.36}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Negative Sampling}{400}{section*.37}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.8.3}Continuous Bag of Words (CBOW)}{402}{subsection.7.8.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.13}{\ignorespaces CBOW architecture: average context embeddings, predict centre word.}}{403}{figure.7.13}\protected@file@percent }
\newlabel{fig:cbow}{{7.13}{403}{CBOW architecture: average context embeddings, predict centre word}{figure.7.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.8.4}Word2Vec Properties and Evaluation}{404}{subsection.7.8.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7.9}Word Embeddings III: GloVe}{404}{section.7.9}\protected@file@percent }
\newlabel{sec:glove}{{7.9}{404}{Word Embeddings III: GloVe}{section.7.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.9.1}Co-occurrence Matrix}{405}{subsection.7.9.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.9.2}GloVe Objective Derivation}{406}{subsection.7.9.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7.10}Contextual Embeddings}{407}{section.7.10}\protected@file@percent }
\newlabel{sec:contextual-embeddings}{{7.10}{407}{Contextual Embeddings}{section.7.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.10.1}ELMo: Embeddings from Language Models}{408}{subsection.7.10.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.10.2}BERT: Bidirectional Encoder Representations from Transformers}{410}{subsection.7.10.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7.11}The Transformer Architecture}{412}{section.7.11}\protected@file@percent }
\newlabel{sec:transformers}{{7.11}{412}{The Transformer Architecture}{section.7.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.11.1}Self-Attention Mechanism}{413}{subsection.7.11.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.11.2}Multi-Head Attention}{416}{subsection.7.11.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.11.3}Positional Encoding}{417}{subsection.7.11.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.11.4}Feed-Forward Networks}{418}{subsection.7.11.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.11.5}Layer Normalisation and Residual Connections}{420}{subsection.7.11.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.11.6}Complete Transformer Encoder}{421}{subsection.7.11.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.11.7}Computational Complexity}{423}{subsection.7.11.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7.12}Sentiment Analysis with RNNs}{423}{section.7.12}\protected@file@percent }
\newlabel{sec:sentiment-rnn}{{7.12}{423}{Sentiment Analysis with RNNs}{section.7.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.14}{\ignorespaces Sentiment analysis: pretrained embeddings + RNN architecture + classification.}}{424}{figure.7.14}\protected@file@percent }
\newlabel{fig:sentiment}{{7.14}{424}{Sentiment analysis: pretrained embeddings + RNN architecture + classification}{figure.7.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.12.1}Basic RNNs for Sentiment Analysis}{424}{subsection.7.12.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.15}{\ignorespaces RNN unrolled through time for sequence classification.}}{424}{figure.7.15}\protected@file@percent }
\newlabel{fig:rnn-sentiment}{{7.15}{424}{RNN unrolled through time for sequence classification}{figure.7.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.16}{\ignorespaces Character sequence modelling: processing ``machine'' character by character.}}{425}{figure.7.16}\protected@file@percent }
\newlabel{fig:rnn-char}{{7.16}{425}{Character sequence modelling: processing ``machine'' character by character}{figure.7.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.12.2}Challenges with Basic RNNs}{425}{subsection.7.12.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.12.3}LSTM and GRU for Sentiment}{425}{subsection.7.12.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.12.4}Bidirectional RNNs}{426}{subsection.7.12.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.17}{\ignorespaces Bidirectional RNN: forward and backward passes concatenated.}}{426}{figure.7.17}\protected@file@percent }
\newlabel{fig:bidirectional}{{7.17}{426}{Bidirectional RNN: forward and backward passes concatenated}{figure.7.17}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.12.5}Pretraining Task: Masked Language Modelling}{426}{subsection.7.12.5}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {7.1}{\ignorespaces Masked language modelling intuition: downstream context is informative.}}{427}{table.7.1}\protected@file@percent }
\newlabel{tab:mlm}{{7.1}{427}{Masked language modelling intuition: downstream context is informative}{table.7.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.12.6}Training with Sentiment Labels}{427}{subsection.7.12.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7.13}Regularisation in Deep Learning}{427}{section.7.13}\protected@file@percent }
\newlabel{sec:regularisation}{{7.13}{427}{Regularisation in Deep Learning}{section.7.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.13.1}Weight Sharing}{428}{subsection.7.13.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.13.2}Weight Decay ($L_2$ Regularisation)}{428}{subsection.7.13.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.13.3}Dropout}{428}{subsection.7.13.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.18}{\ignorespaces Network before dropout: all neurons active.}}{428}{figure.7.18}\protected@file@percent }
\newlabel{fig:before-dropout}{{7.18}{428}{Network before dropout: all neurons active}{figure.7.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.19}{\ignorespaces Network with dropout: random neurons zeroed.}}{429}{figure.7.19}\protected@file@percent }
\newlabel{fig:after-dropout}{{7.19}{429}{Network with dropout: random neurons zeroed}{figure.7.19}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.13.4}Dropout in NLP}{431}{subsection.7.13.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.13.5}Label Smoothing}{431}{subsection.7.13.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7.14}Summary: From Words to Transformers}{432}{section.7.14}\protected@file@percent }
\newlabel{sec:summary}{{7.14}{432}{Summary: From Words to Transformers}{section.7.14}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {8}Week 8: NLP II - Attention and Transformers}{435}{chapter.8}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:week8}{{8}{435}{Week 8: NLP II - Attention and Transformers}{chapter.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.1}Encoder-Decoder Architecture}{436}{section.8.1}\protected@file@percent }
\newlabel{sec:encoder-decoder}{{8.1}{436}{Encoder-Decoder Architecture}{section.8.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1.1}Machine Translation: The Canonical Seq2Seq Problem}{437}{subsection.8.1.1}\protected@file@percent }
\newlabel{subsec:machine-translation}{{8.1.1}{437}{Machine Translation: The Canonical Seq2Seq Problem}{subsection.8.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.1}{\ignorespaces Word alignment between English and French (adapted from Brown et al., 1990). Note the crossing alignments: ``now'' at position 5 in English maps to ``maintenant'' at position 9 in French (shown in blue), while ``implemented'' maps to a multi-word phrase. The negation ``not'' maps to the French discontinuous negation ``ne...pas''. These non-monotonic alignments are a key challenge in machine translation.}}{438}{figure.8.1}\protected@file@percent }
\newlabel{fig:word-alignment}{{8.1}{438}{Word alignment between English and French (adapted from Brown et al., 1990). Note the crossing alignments: ``now'' at position 5 in English maps to ``maintenant'' at position 9 in French (shown in blue), while ``implemented'' maps to a multi-word phrase. The negation ``not'' maps to the French discontinuous negation ``ne...pas''. These non-monotonic alignments are a key challenge in machine translation}{figure.8.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1.2}The Encoder-Decoder Framework}{439}{subsection.8.1.2}\protected@file@percent }
\newlabel{subsec:enc-dec-framework}{{8.1.2}{439}{The Encoder-Decoder Framework}{subsection.8.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.2}{\ignorespaces Encoder-decoder architecture: the encoder compresses a variable-length input into a fixed-dimensional context vector, which the decoder expands into a variable-length output. The context vector is the only pathway for information from input to output-a bottleneck that will prove problematic for long sequences.}}{440}{figure.8.2}\protected@file@percent }
\newlabel{fig:encoder-decoder-flow}{{8.2}{440}{Encoder-decoder architecture: the encoder compresses a variable-length input into a fixed-dimensional context vector, which the decoder expands into a variable-length output. The context vector is the only pathway for information from input to output-a bottleneck that will prove problematic for long sequences}{figure.8.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1.3}Autoencoder: A Special Case}{440}{subsection.8.1.3}\protected@file@percent }
\newlabel{subsec:autoencoder}{{8.1.3}{440}{Autoencoder: A Special Case}{subsection.8.1.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1.4}RNN-Based Encoder-Decoder}{442}{subsection.8.1.4}\protected@file@percent }
\newlabel{subsec:rnn-encoder-decoder}{{8.1.4}{442}{RNN-Based Encoder-Decoder}{subsection.8.1.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.3}{\ignorespaces RNN encoder-decoder for translating ``I love you'' to French. The encoder processes the English sentence left-to-right, producing hidden states $\mathbf  {h}_1, \mathbf  {h}_2, \mathbf  {h}_3$. The final hidden state $\mathbf  {h}_3$ initialises the decoder (red arrow) and serves as the context $\mathbf  {c}$. The context is fed to the decoder at every time step (dashed grey arrows). The decoder generates autoregressively: each output becomes the next input.}}{444}{figure.8.3}\protected@file@percent }
\newlabel{fig:rnn-enc-dec}{{8.3}{444}{RNN encoder-decoder for translating ``I love you'' to French. The encoder processes the English sentence left-to-right, producing hidden states $\mathbf {h}_1, \mathbf {h}_2, \mathbf {h}_3$. The final hidden state $\mathbf {h}_3$ initialises the decoder (red arrow) and serves as the context $\mathbf {c}$. The context is fed to the decoder at every time step (dashed grey arrows). The decoder generates autoregressively: each output becomes the next input}{figure.8.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1.5}Bidirectional Encoding}{445}{subsection.8.1.5}\protected@file@percent }
\newlabel{subsec:bidirectional}{{8.1.5}{445}{Bidirectional Encoding}{subsection.8.1.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1.6}Data Preprocessing for Machine Translation}{446}{subsection.8.1.6}\protected@file@percent }
\newlabel{subsec:preprocessing}{{8.1.6}{446}{Data Preprocessing for Machine Translation}{subsection.8.1.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.2}BLEU: Evaluating Machine Translation}{448}{section.8.2}\protected@file@percent }
\newlabel{sec:bleu}{{8.2}{448}{BLEU: Evaluating Machine Translation}{section.8.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.1}The Challenge of Evaluation}{448}{subsection.8.2.1}\protected@file@percent }
\newlabel{subsec:eval-challenge}{{8.2.1}{448}{The Challenge of Evaluation}{subsection.8.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.2}Computing BLEU in Practice}{453}{subsection.8.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8.3}The Attention Mechanism}{453}{section.8.3}\protected@file@percent }
\newlabel{sec:attention}{{8.3}{453}{The Attention Mechanism}{section.8.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3.1}The Problem: Information Bottleneck}{453}{subsection.8.3.1}\protected@file@percent }
\newlabel{subsec:bottleneck}{{8.3.1}{453}{The Problem: Information Bottleneck}{subsection.8.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3.2}Biological Inspiration: How Humans Attend}{454}{subsection.8.3.2}\protected@file@percent }
\newlabel{subsec:bio-inspiration}{{8.3.2}{454}{Biological Inspiration: How Humans Attend}{subsection.8.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3.3}Queries, Keys, and Values}{456}{subsection.8.3.3}\protected@file@percent }
\newlabel{subsec:qkv}{{8.3.3}{456}{Queries, Keys, and Values}{subsection.8.3.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.4}{\ignorespaces The query-key-value attention mechanism. The query is compared to all keys to produce attention weights $\alpha _i$ (dashed arrows). Each key is paired with a value (dotted arrows). The output is a weighted sum of values, where weights reflect query-key similarity.}}{458}{figure.8.4}\protected@file@percent }
\newlabel{fig:qkv-attention}{{8.4}{458}{The query-key-value attention mechanism. The query is compared to all keys to produce attention weights $\alpha _i$ (dashed arrows). Each key is paired with a value (dotted arrows). The output is a weighted sum of values, where weights reflect query-key similarity}{figure.8.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3.4}Attention Pooling: From Hard to Soft Retrieval}{458}{subsection.8.3.4}\protected@file@percent }
\newlabel{subsec:attention-pooling}{{8.3.4}{458}{Attention Pooling: From Hard to Soft Retrieval}{subsection.8.3.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.5}{\ignorespaces Kernel regression as attention. Training points (blue) have associated $y$ values. For a query point $x$ (red dashed line), the kernel (green curve) assigns weights based on distance. The prediction (red point) is a weighted average of training $y$ values, with nearby points weighted more heavily.}}{460}{figure.8.5}\protected@file@percent }
\newlabel{fig:kernel-regression}{{8.5}{460}{Kernel regression as attention. Training points (blue) have associated $y$ values. For a query point $x$ (red dashed line), the kernel (green curve) assigns weights based on distance. The prediction (red point) is a weighted average of training $y$ values, with nearby points weighted more heavily}{figure.8.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3.5}Attention Scoring Functions}{460}{subsection.8.3.5}\protected@file@percent }
\newlabel{subsec:scoring-functions}{{8.3.5}{460}{Attention Scoring Functions}{subsection.8.3.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.6}{\ignorespaces Hypothetical attention weight visualisation for English-French translation. Darker cells indicate higher attention weights. ``J'aime'' attends mainly to ``I'' and ``love''; ``apprendre'' attends to ``learning''; ``profond'' attends strongly to ``deep''. This soft alignment is learned, not hand-coded.}}{465}{figure.8.6}\protected@file@percent }
\newlabel{fig:attention-heatmap}{{8.6}{465}{Hypothetical attention weight visualisation for English-French translation. Darker cells indicate higher attention weights. ``J'aime'' attends mainly to ``I'' and ``love''; ``apprendre'' attends to ``learning''; ``profond'' attends strongly to ``deep''. This soft alignment is learned, not hand-coded}{figure.8.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.4}Bahdanau Attention}{465}{section.8.4}\protected@file@percent }
\newlabel{sec:bahdanau}{{8.4}{465}{Bahdanau Attention}{section.8.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4.1}From Fixed Context to Dynamic Context}{465}{subsection.8.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4.2}Interpreting Attention as Soft Alignment}{469}{subsection.8.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8.7}{\ignorespaces Attention weight visualisation for English-German translation. Each row shows the attention distribution when generating one German word. The roughly diagonal pattern reflects similar word order, with ``der'' (the, dative) attending to both ``the'' and ``mat'' since German articles depend on the noun they modify.}}{470}{figure.8.7}\protected@file@percent }
\newlabel{fig:bahdanau-alignment}{{8.7}{470}{Attention weight visualisation for English-German translation. Each row shows the attention distribution when generating one German word. The roughly diagonal pattern reflects similar word order, with ``der'' (the, dative) attending to both ``the'' and ``mat'' since German articles depend on the noun they modify}{figure.8.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4.3}Bahdanau Attention in the Query-Key-Value Framework}{470}{subsection.8.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4.4}Implementation Considerations}{471}{subsection.8.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8.5}Multi-Head Attention}{471}{section.8.5}\protected@file@percent }
\newlabel{sec:multihead}{{8.5}{471}{Multi-Head Attention}{section.8.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5.1}Motivation: Diverse Attention Patterns}{471}{subsection.8.5.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8.8}{\ignorespaces Multi-head attention architecture. Queries, keys, and values are each projected through $h$ different learned projections. Scaled dot-product attention is applied in each subspace independently. The results are concatenated and projected to produce the final output.}}{473}{figure.8.8}\protected@file@percent }
\newlabel{fig:multihead-attention}{{8.8}{473}{Multi-head attention architecture. Queries, keys, and values are each projected through $h$ different learned projections. Scaled dot-product attention is applied in each subspace independently. The results are concatenated and projected to produce the final output}{figure.8.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5.2}Dimension Management}{473}{subsection.8.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5.3}What Do Different Heads Learn?}{474}{subsection.8.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8.6}Self-Attention}{474}{section.8.6}\protected@file@percent }
\newlabel{sec:self-attention}{{8.6}{474}{Self-Attention}{section.8.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.6.1}Definition and Intuition}{475}{subsection.8.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.6.2}Properties of Self-Attention}{477}{subsection.8.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.6.3}Masked Self-Attention for Autoregressive Generation}{479}{subsection.8.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8.7}Positional Encoding}{480}{section.8.7}\protected@file@percent }
\newlabel{sec:positional-encoding}{{8.7}{480}{Positional Encoding}{section.8.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.7.1}The Problem: Order Blindness}{481}{subsection.8.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.7.2}Sinusoidal Positional Encoding}{481}{subsection.8.7.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8.9}{\ignorespaces Sinusoidal positional encoding at different dimensions. Lower dimensions vary slowly with position (low frequency); higher dimensions vary rapidly (high frequency). The combination of frequencies at all dimensions provides a unique ``signature'' for each position.}}{483}{figure.8.9}\protected@file@percent }
\newlabel{fig:sinusoidal-encoding}{{8.9}{483}{Sinusoidal positional encoding at different dimensions. Lower dimensions vary slowly with position (low frequency); higher dimensions vary rapidly (high frequency). The combination of frequencies at all dimensions provides a unique ``signature'' for each position}{figure.8.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.7.3}Alternative Positional Encoding Methods}{484}{subsection.8.7.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8.8}The Transformer Architecture}{484}{section.8.8}\protected@file@percent }
\newlabel{sec:transformer}{{8.8}{484}{The Transformer Architecture}{section.8.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.8.1}The Transformer Encoder}{485}{subsection.8.8.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8.10}{\ignorespaces Transformer encoder layer. Multi-head self-attention allows each position to attend to all positions. The position-wise FFN adds nonlinearity and capacity. Residual connections and layer normalisation facilitate training of deep stacks. The encoder consists of $N$ identical layers.}}{486}{figure.8.10}\protected@file@percent }
\newlabel{fig:transformer-encoder}{{8.10}{486}{Transformer encoder layer. Multi-head self-attention allows each position to attend to all positions. The position-wise FFN adds nonlinearity and capacity. Residual connections and layer normalisation facilitate training of deep stacks. The encoder consists of $N$ identical layers}{figure.8.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.8.2}The Transformer Decoder}{487}{subsection.8.8.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8.11}{\ignorespaces Transformer decoder layer. Masked self-attention prevents looking at future positions. Cross-attention allows attending to the encoder output (keys and values from encoder, queries from decoder). The final linear layer and softmax produce token probabilities.}}{488}{figure.8.11}\protected@file@percent }
\newlabel{fig:transformer-decoder}{{8.11}{488}{Transformer decoder layer. Masked self-attention prevents looking at future positions. Cross-attention allows attending to the encoder output (keys and values from encoder, queries from decoder). The final linear layer and softmax produce token probabilities}{figure.8.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.8.3}Transformer Architectural Variants}{488}{subsection.8.8.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8.9}BERT: Bidirectional Encoder Representations}{489}{section.8.9}\protected@file@percent }
\newlabel{sec:bert}{{8.9}{489}{BERT: Bidirectional Encoder Representations}{section.8.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.9.1}Architecture and Pretraining}{490}{subsection.8.9.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.9.2}Fine-tuning BERT for Downstream Tasks}{492}{subsection.8.9.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.9.3}BERT Variants and Legacy}{493}{subsection.8.9.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8.10}Vision Transformer (ViT)}{494}{section.8.10}\protected@file@percent }
\newlabel{sec:vit}{{8.10}{494}{Vision Transformer (ViT)}{section.8.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.10.1}Image as Sequence of Patches}{494}{subsection.8.10.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8.12}{\ignorespaces Vision Transformer (ViT) architecture. An image is divided into fixed-size patches (e.g., $16 \times 16$ pixels), which are flattened and projected to embeddings. A learnable \texttt  {[CLS]} token is prepended, positional embeddings are added, and the sequence is processed by a standard Transformer encoder. The \texttt  {[CLS]} token output is used for classification.}}{495}{figure.8.12}\protected@file@percent }
\newlabel{fig:vit-architecture}{{8.12}{495}{Vision Transformer (ViT) architecture. An image is divided into fixed-size patches (e.g., $16 \times 16$ pixels), which are flattened and projected to embeddings. A learnable \texttt {[CLS]} token is prepended, positional embeddings are added, and the sequence is processed by a standard Transformer encoder. The \texttt {[CLS]} token output is used for classification}{figure.8.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.10.2}ViT Model Variants}{495}{subsection.8.10.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.10.3}Data Requirements and Inductive Bias}{496}{subsection.8.10.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.10.4}ViT Variants and Improvements}{497}{subsection.8.10.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8.11}Computational Considerations}{497}{section.8.11}\protected@file@percent }
\newlabel{sec:computational}{{8.11}{497}{Computational Considerations}{section.8.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.12}Summary and Key Takeaways}{499}{section.8.12}\protected@file@percent }
\newlabel{sec:summary}{{8.12}{499}{Summary and Key Takeaways}{section.8.12}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.13}Connections to Other Topics}{502}{section.8.13}\protected@file@percent }
\newlabel{sec:connections}{{8.13}{502}{Connections to Other Topics}{section.8.13}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {9}Week 9: Large Language Models in Practice}{505}{chapter.9}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:week9}{{9}{505}{Week 9: Large Language Models in Practice}{chapter.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9.1}AI Alignment: The Challenge of Helpful, Harmless, and Honest Systems}{506}{section.9.1}\protected@file@percent }
\newlabel{sec:alignment}{{9.1}{506}{AI Alignment: The Challenge of Helpful, Harmless, and Honest Systems}{section.9.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1.1}Hallucinations: Confident Fabrication}{507}{subsection.9.1.1}\protected@file@percent }
\newlabel{subsec:hallucinations}{{9.1.1}{507}{Hallucinations: Confident Fabrication}{subsection.9.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1.2}Data-Based Bias: Learning Society's Prejudices}{508}{subsection.9.1.2}\protected@file@percent }
\newlabel{subsec:bias}{{9.1.2}{508}{Data-Based Bias: Learning Society's Prejudices}{subsection.9.1.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1.3}Offensive and Illegal Content}{510}{subsection.9.1.3}\protected@file@percent }
\newlabel{subsec:offensive}{{9.1.3}{510}{Offensive and Illegal Content}{subsection.9.1.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1.4}LLMs vs Chatbots: The Alignment Gap}{510}{subsection.9.1.4}\protected@file@percent }
\newlabel{subsec:llm-vs-chatbot}{{9.1.4}{510}{LLMs vs Chatbots: The Alignment Gap}{subsection.9.1.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9.2}Post-Training: Aligning LLMs}{512}{section.9.2}\protected@file@percent }
\newlabel{sec:post-training}{{9.2}{512}{Post-Training: Aligning LLMs}{section.9.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2.1}The LLM Training Pipeline}{512}{subsection.9.2.1}\protected@file@percent }
\newlabel{subsec:training-pipeline}{{9.2.1}{512}{The LLM Training Pipeline}{subsection.9.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.1}{\ignorespaces The LLM training pipeline. Pre-training on massive text corpora produces a base model that can continue text fluently but does not follow instructions. Post-training via SFT and RLHF transforms this into a useful assistant. The compute allocation is highly asymmetric: pre-training dominates, but post-training determines user experience.}}{513}{figure.9.1}\protected@file@percent }
\newlabel{fig:training-pipeline}{{9.1}{513}{The LLM training pipeline. Pre-training on massive text corpora produces a base model that can continue text fluently but does not follow instructions. Post-training via SFT and RLHF transforms this into a useful assistant. The compute allocation is highly asymmetric: pre-training dominates, but post-training determines user experience}{figure.9.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2.2}LLM Inference: Behind the Scenes}{513}{subsection.9.2.2}\protected@file@percent }
\newlabel{subsec:inference-structure}{{9.2.2}{513}{LLM Inference: Behind the Scenes}{subsection.9.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2.3}Supervised Fine-Tuning (SFT)}{514}{subsection.9.2.3}\protected@file@percent }
\newlabel{subsec:sft}{{9.2.3}{514}{Supervised Fine-Tuning (SFT)}{subsection.9.2.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9.3}Reinforcement Learning from Human Feedback (RLHF)}{516}{section.9.3}\protected@file@percent }
\newlabel{sec:rlhf}{{9.3}{516}{Reinforcement Learning from Human Feedback (RLHF)}{section.9.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.1}The Three-Step RLHF Process}{516}{subsection.9.3.1}\protected@file@percent }
\newlabel{subsec:rlhf-process}{{9.3.1}{516}{The Three-Step RLHF Process}{subsection.9.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.2}{\ignorespaces The three-step RLHF process. Step 1 (SFT) creates an initial policy from human demonstrations. Step 2 trains a reward model on human preference data (using responses from $\pi _{\text  {SFT}}$). Step 3 (PPO) optimises the policy to maximise reward while staying close to $\pi _{\text  {SFT}}$ via a KL penalty.}}{518}{figure.9.2}\protected@file@percent }
\newlabel{fig:rlhf-pipeline}{{9.2}{518}{The three-step RLHF process. Step 1 (SFT) creates an initial policy from human demonstrations. Step 2 trains a reward model on human preference data (using responses from $\pi _{\text {SFT}}$). Step 3 (PPO) optimises the policy to maximise reward while staying close to $\pi _{\text {SFT}}$ via a KL penalty}{figure.9.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.2}Why Preferences Over Demonstrations?}{518}{subsection.9.3.2}\protected@file@percent }
\newlabel{subsec:why-preferences}{{9.3.2}{518}{Why Preferences Over Demonstrations?}{subsection.9.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.3}Proximal Policy Optimisation (PPO)}{519}{subsection.9.3.3}\protected@file@percent }
\newlabel{subsec:ppo}{{9.3.3}{519}{Proximal Policy Optimisation (PPO)}{subsection.9.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.4}Ethical Concerns in RLHF}{520}{subsection.9.3.4}\protected@file@percent }
\newlabel{subsec:rlhf-ethics}{{9.3.4}{520}{Ethical Concerns in RLHF}{subsection.9.3.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.5}Alternatives and Extensions to RLHF}{520}{subsection.9.3.5}\protected@file@percent }
\newlabel{subsec:rlhf-alternatives}{{9.3.5}{520}{Alternatives and Extensions to RLHF}{subsection.9.3.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9.4}The Bitter Lesson}{521}{section.9.4}\protected@file@percent }
\newlabel{sec:bitter-lesson}{{9.4}{521}{The Bitter Lesson}{section.9.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4.1}Historical Evidence}{522}{subsection.9.4.1}\protected@file@percent }
\newlabel{subsec:bitter-evidence}{{9.4.1}{522}{Historical Evidence}{subsection.9.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4.2}Implications for LLM Development}{524}{subsection.9.4.2}\protected@file@percent }
\newlabel{subsec:bitter-implications}{{9.4.2}{524}{Implications for LLM Development}{subsection.9.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4.3}Counterarguments and Nuance}{524}{subsection.9.4.3}\protected@file@percent }
\newlabel{subsec:bitter-counter}{{9.4.3}{524}{Counterarguments and Nuance}{subsection.9.4.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9.5}Reasoning Models}{525}{section.9.5}\protected@file@percent }
\newlabel{sec:reasoning}{{9.5}{525}{Reasoning Models}{section.9.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.5.1}What Are Reasoning Models?}{526}{subsection.9.5.1}\protected@file@percent }
\newlabel{subsec:reasoning-definition}{{9.5.1}{526}{What Are Reasoning Models?}{subsection.9.5.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.5.2}Performance Characteristics of Reasoning Models}{527}{subsection.9.5.2}\protected@file@percent }
\newlabel{subsec:reasoning-performance}{{9.5.2}{527}{Performance Characteristics of Reasoning Models}{subsection.9.5.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.5.3}How Reasoning Models Are Trained}{530}{subsection.9.5.3}\protected@file@percent }
\newlabel{subsec:reasoning-training}{{9.5.3}{530}{How Reasoning Models Are Trained}{subsection.9.5.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.5.4}The Future of Reasoning Models}{530}{subsection.9.5.4}\protected@file@percent }
\newlabel{subsec:reasoning-future}{{9.5.4}{530}{The Future of Reasoning Models}{subsection.9.5.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9.6}Retrieval-Augmented Generation (RAG)}{531}{section.9.6}\protected@file@percent }
\newlabel{sec:rag}{{9.6}{531}{Retrieval-Augmented Generation (RAG)}{section.9.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.6.1}Motivation: The Knowledge Currency Problem}{531}{subsection.9.6.1}\protected@file@percent }
\newlabel{subsec:rag-motivation}{{9.6.1}{531}{Motivation: The Knowledge Currency Problem}{subsection.9.6.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.6.2}RAG Architecture}{532}{subsection.9.6.2}\protected@file@percent }
\newlabel{subsec:rag-architecture}{{9.6.2}{532}{RAG Architecture}{subsection.9.6.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.3}{\ignorespaces RAG architecture. Documents are embedded and indexed offline (dashed arrow). At query time, the user query is embedded, similar documents are retrieved via vector search, and the augmented prompt (query + retrieved context) is passed to the LLM for generation. The response is ``grounded'' in the retrieved documents rather than relying solely on parametric knowledge.}}{534}{figure.9.3}\protected@file@percent }
\newlabel{fig:rag-architecture}{{9.3}{534}{RAG architecture. Documents are embedded and indexed offline (dashed arrow). At query time, the user query is embedded, similar documents are retrieved via vector search, and the augmented prompt (query + retrieved context) is passed to the LLM for generation. The response is ``grounded'' in the retrieved documents rather than relying solely on parametric knowledge}{figure.9.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.6.3}Document Retrieval Methods}{534}{subsection.9.6.3}\protected@file@percent }
\newlabel{subsec:rag-retrieval}{{9.6.3}{534}{Document Retrieval Methods}{subsection.9.6.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.6.4}RAG Benefits and Limitations}{537}{subsection.9.6.4}\protected@file@percent }
\newlabel{subsec:rag-tradeoffs}{{9.6.4}{537}{RAG Benefits and Limitations}{subsection.9.6.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9.7}Fine-Tuning LLMs}{538}{section.9.7}\protected@file@percent }
\newlabel{sec:finetuning}{{9.7}{538}{Fine-Tuning LLMs}{section.9.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.7.1}The Landscape of LLM Availability}{538}{subsection.9.7.1}\protected@file@percent }
\newlabel{subsec:llm-openness}{{9.7.1}{538}{The Landscape of LLM Availability}{subsection.9.7.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.7.2}Challenges in Fine-Tuning}{540}{subsection.9.7.2}\protected@file@percent }
\newlabel{subsec:finetuning-challenges}{{9.7.2}{540}{Challenges in Fine-Tuning}{subsection.9.7.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.7.3}Parameter-Efficient Fine-Tuning (PEFT)}{540}{subsection.9.7.3}\protected@file@percent }
\newlabel{subsec:peft}{{9.7.3}{540}{Parameter-Efficient Fine-Tuning (PEFT)}{subsection.9.7.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.7.4}LoRA: Low-Rank Adaptation}{541}{subsection.9.7.4}\protected@file@percent }
\newlabel{subsec:lora}{{9.7.4}{541}{LoRA: Low-Rank Adaptation}{subsection.9.7.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.4}{\ignorespaces LoRA architecture. The pre-trained weight matrix $W_0$ is frozen. Two small matrices $A$ (down-projection, $r \times k$) and $B$ (up-projection, $d \times r$) are trained. The output is the sum of the original path ($W_0 x$) and the LoRA path ($BAx$). Since $r \ll \min (d, k)$, the trainable parameters are a small fraction of the original matrix.}}{543}{figure.9.4}\protected@file@percent }
\newlabel{fig:lora-architecture}{{9.4}{543}{LoRA architecture. The pre-trained weight matrix $W_0$ is frozen. Two small matrices $A$ (down-projection, $r \times k$) and $B$ (up-projection, $d \times r$) are trained. The output is the sum of the original path ($W_0 x$) and the LoRA path ($BAx$). Since $r \ll \min (d, k)$, the trainable parameters are a small fraction of the original matrix}{figure.9.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.7.5}Fine-Tuning Proprietary Models}{545}{subsection.9.7.5}\protected@file@percent }
\newlabel{subsec:finetuning-proprietary}{{9.7.5}{545}{Fine-Tuning Proprietary Models}{subsection.9.7.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9.8}Few-Shot Learning}{546}{section.9.8}\protected@file@percent }
\newlabel{sec:fewshot}{{9.8}{546}{Few-Shot Learning}{section.9.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9.9}Structured Outputs}{550}{section.9.9}\protected@file@percent }
\newlabel{sec:structured}{{9.9}{550}{Structured Outputs}{section.9.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.9.1}JSON Schema and Format Constraints}{551}{subsection.9.9.1}\protected@file@percent }
\newlabel{subsec:json-schema}{{9.9.1}{551}{JSON Schema and Format Constraints}{subsection.9.9.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.9.2}Chain-of-Thought with Structured Output}{552}{subsection.9.9.2}\protected@file@percent }
\newlabel{subsec:structured-cot}{{9.9.2}{552}{Chain-of-Thought with Structured Output}{subsection.9.9.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9.10}Tool Calling}{553}{section.9.10}\protected@file@percent }
\newlabel{sec:tools}{{9.10}{553}{Tool Calling}{section.9.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.10.1}What Is Tool Calling?}{554}{subsection.9.10.1}\protected@file@percent }
\newlabel{subsec:tool-definition}{{9.10.1}{554}{What Is Tool Calling?}{subsection.9.10.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.10.2}The Five-Step Tool Calling Flow}{555}{subsection.9.10.2}\protected@file@percent }
\newlabel{subsec:tool-flow}{{9.10.2}{555}{The Five-Step Tool Calling Flow}{subsection.9.10.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.5}{\ignorespaces The five-step tool calling flow. (1) Developer provides tool definitions and user message. (2) Model decides to call a tool, outputting a structured request. (3) Developer's code executes the actual function. (4) Results are returned to the model as a new message. (5) Model generates final response incorporating tool results. The model \textit  {orchestrates} but does not \textit  {execute}-execution remains under developer control.}}{555}{figure.9.5}\protected@file@percent }
\newlabel{fig:tool-calling-flow}{{9.5}{555}{The five-step tool calling flow. (1) Developer provides tool definitions and user message. (2) Model decides to call a tool, outputting a structured request. (3) Developer's code executes the actual function. (4) Results are returned to the model as a new message. (5) Model generates final response incorporating tool results. The model \textit {orchestrates} but does not \textit {execute}-execution remains under developer control}{figure.9.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9.11}AI Agents}{558}{section.9.11}\protected@file@percent }
\newlabel{sec:agents}{{9.11}{558}{AI Agents}{section.9.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.11.1}Defining AI Agents}{559}{subsection.9.11.1}\protected@file@percent }
\newlabel{subsec:agent-definition}{{9.11.1}{559}{Defining AI Agents}{subsection.9.11.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.11.2}Examples of AI Agents}{560}{subsection.9.11.2}\protected@file@percent }
\newlabel{subsec:agent-examples}{{9.11.2}{560}{Examples of AI Agents}{subsection.9.11.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.11.3}Agent Categorisation and Governance}{562}{subsection.9.11.3}\protected@file@percent }
\newlabel{subsec:agent-governance}{{9.11.3}{562}{Agent Categorisation and Governance}{subsection.9.11.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.11.4}Future Implications of AI Agents}{564}{subsection.9.11.4}\protected@file@percent }
\newlabel{subsec:agent-future}{{9.11.4}{564}{Future Implications of AI Agents}{subsection.9.11.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9.12}Summary and Connections}{566}{section.9.12}\protected@file@percent }
\newlabel{sec:summary}{{9.12}{566}{Summary and Connections}{section.9.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.12.1}Connections to Other Topics}{568}{subsection.9.12.1}\protected@file@percent }
\newlabel{subsec:connections}{{9.12.1}{568}{Connections to Other Topics}{subsection.9.12.1}{}}
\gdef \@abspage@last{569}
