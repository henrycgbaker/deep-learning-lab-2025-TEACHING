\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Deep Learning in Public Policy}{7}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:week1}{{1}{7}{Deep Learning in Public Policy}{chapter.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Enter Caption}}{7}{figure.1.1}\protected@file@percent }
\newlabel{fig:dl-venn}{{1.1}{7}{Enter Caption}{figure.1.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}ML vs DL}{8}{section.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Universal Approximation Theorem}{8}{section.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.3}What is DL}{8}{section.1.3}\protected@file@percent }
\newlabel{fig:rep-learning-1}{{1.3}{9}{What is DL}{section.1.3}{}}
\newlabel{fig:rep-learning-2}{{1.3}{9}{What is DL}{section.1.3}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Deep Neural Networks I}{11}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:week2}{{2}{11}{Deep Neural Networks I}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Overview}{11}{section.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.2}High-level overview: How a neural network learns}{11}{section.2.2}\protected@file@percent }
\newlabel{fig:w2-1}{{2.2}{11}{High-level overview: How a neural network learns}{section.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Artificial Neurons (Hidden units)}{12}{subsection.2.2.1}\protected@file@percent }
\newlabel{fig:w2-2}{{2.2.1}{12}{Artificial Neurons (Hidden units)}{subsection.2.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces hidden unit's connection to input activation}}{15}{figure.2.1}\protected@file@percent }
\newlabel{fig:w2-3}{{2.1}{15}{hidden unit's connection to input activation}{figure.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Output layer (in a single-layered NN)}{15}{subsection.2.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Single-layer NNs}{16}{subsection.2.2.3}\protected@file@percent }
\newlabel{fig:w2-4}{{2.2.3}{16}{Single-layer NNs}{subsection.2.2.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces NB: $W^{[1]}$ is a matrix; $w^{(2)}$ is a vector!}}{16}{figure.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Feed-forward NN}{20}{section*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.3}NNs basic components}{24}{section.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Parameters \& hyperparameteters}{24}{subsection.2.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Back propagation}{24}{subsection.2.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Non linear activation functions:}{25}{subsection.2.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Comparing sigmoid vs ReLU}}{25}{figure.2.3}\protected@file@percent }
\newlabel{fig:w2-5}{{2.3}{25}{Comparing sigmoid vs ReLU}{figure.2.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Tanh}}{26}{figure.2.4}\protected@file@percent }
\newlabel{fig:w2-6}{{2.4}{26}{Tanh}{figure.2.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Heavyside}}{26}{figure.2.5}\protected@file@percent }
\newlabel{fig:w2-7}{{2.5}{26}{Heavyside}{figure.2.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.4}Output layer activation function}{27}{subsection.2.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Single output}{27}{section*.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Multiclass classification}{27}{section*.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces for $k$ output nodes; $W^{[2}]$ now has $k \times i$ dim; biases = vector of $k$}}{27}{figure.2.6}\protected@file@percent }
\newlabel{fig:w2-8}{{2.6}{27}{for $k$ output nodes; $W^{[2}]$ now has $k \times i$ dim; biases = vector of $k$}{figure.2.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces The process thus far}}{28}{figure.2.7}\protected@file@percent }
\newlabel{fig:w2-9}{{2.7}{28}{The process thus far}{figure.2.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.5}Loss function}{28}{subsection.2.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Loss function for regression}{28}{section*.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Loss function for K-class classification}{29}{section*.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces $x$ approaches 0 -> $y$ negative; $x$ approaches 1 -> $y$ 0 }}{31}{figure.2.8}\protected@file@percent }
\newlabel{fig:placeholder}{{2.8}{31}{$x$ approaches 0 -> $y$ negative; $x$ approaches 1 -> $y$ 0}{figure.2.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Capacity of a NN (`expressiveness')}{31}{section.2.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces Top: linearly separable; bottom, not linearly-separable}}{31}{figure.2.9}\protected@file@percent }
\newlabel{fig:w2-10}{{2.9}{31}{Top: linearly separable; bottom, not linearly-separable}{figure.2.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces Linearly separable problems}}{32}{figure.2.10}\protected@file@percent }
\newlabel{fig:w2-11}{{2.10}{32}{Linearly separable problems}{figure.2.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces Transformation of input features}}{32}{figure.2.11}\protected@file@percent }
\newlabel{fig:w2-12}{{2.11}{32}{Transformation of input features}{figure.2.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.12}{\ignorespaces Illustration of a how a single-layer NN can represent a non-linearfunction with activations.}}{33}{figure.2.12}\protected@file@percent }
\newlabel{fig:w2-13}{{2.12}{33}{Illustration of a how a single-layer NN can represent a non-linearfunction with activations}{figure.2.12}{}}
\@writefile{toc}{\contentsline {subsubsection}{Universal Approximation Theorem}{33}{section*.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Gradient Descent}{34}{section.2.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.13}{\ignorespaces The gradient is just a \textbf  {vector of the partial derivatives}. We need to take the negative of this in order to approach the min.}}{34}{figure.2.13}\protected@file@percent }
\newlabel{fig:w2-14}{{2.13}{34}{The gradient is just a \textbf {vector of the partial derivatives}. We need to take the negative of this in order to approach the min}{figure.2.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.14}{\ignorespaces \textit  {NB, even for fixed $\eta $, the steps becomes smaller; as we get closer to 0 the functional values that we plug in will becomes smaller, the gradient evaluated at each successive point becomes smaller. The steps become smaller as the gradients' curve approaches 0, (algebraically the gradient component of the formula gets smaller)}}}{35}{figure.2.14}\protected@file@percent }
\newlabel{fig:w2-15}{{2.14}{35}{\textit {NB, even for fixed $\eta $, the steps becomes smaller; as we get closer to 0 the functional values that we plug in will becomes smaller, the gradient evaluated at each successive point becomes smaller. The steps become smaller as the gradients' curve approaches 0, (algebraically the gradient component of the formula gets smaller)}}{figure.2.14}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Calculating Loss \& Backpropagation}{36}{section.2.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.1}Backpropagation process: }{36}{subsection.2.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.2}Computing the Gradient in Backpropagation (step \#3)}{36}{subsection.2.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Multivariate chain rule}{38}{section*.8}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.15}{\ignorespaces $f(u,v)$, but also $f(a,b)$ and $f(w,x,y,z)$}}{38}{figure.2.15}\protected@file@percent }
\newlabel{fig:w2-16}{{2.15}{38}{$f(u,v)$, but also $f(a,b)$ and $f(w,x,y,z)$}{figure.2.15}{}}
\@writefile{toc}{\contentsline {subsubsection}{Gradient update}{42}{section*.9}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.7}Bigger picture}{42}{section.2.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.16}{\ignorespaces Enter Caption}}{42}{figure.2.16}\protected@file@percent }
\newlabel{fig:w2-17}{{2.16}{42}{Enter Caption}{figure.2.16}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Deep Neural Networks II}{43}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:week3}{{3}{43}{Deep Neural Networks II}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Overview}{43}{section.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Backpropagation (continued)}{43}{section.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Reminder: single-layered NN}{43}{subsection.3.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Architecture}{43}{section*.10}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces single-layered NN}}{43}{figure.3.1}\protected@file@percent }
\newlabel{fig:w3-1}{{3.1}{43}{single-layered NN}{figure.3.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{Formal Expression}{43}{section*.11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Partial derivative:}{44}{section*.12}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Gradient update:}{46}{section*.13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Multivariate chain rule}{46}{subsection.3.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Multivariate chain rule, where notes=variables, edges=functional dependence}}{46}{figure.3.2}\protected@file@percent }
\newlabel{fig:w3-2}{{3.2}{46}{Multivariate chain rule, where notes=variables, edges=functional dependence}{figure.3.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Multiple Output Nodes ($k>1$)}{47}{section.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces 2 output nodes ($k=2$)}}{47}{figure.3.3}\protected@file@percent }
\newlabel{fig:w3-3}{{3.3}{47}{2 output nodes ($k=2$)}{figure.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Cross-Entropy Loss:}{48}{subsection.3.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Gradient Calculation:}{50}{subsection.3.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Multivariate Chain Rule}{50}{section*.14}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Cross-Entropy's Loss Gradient (\textcolor {red}{simplifies!})}{50}{section*.15}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}Two hidden layer NNs (Multilayer Perceptron)}{53}{subsection.3.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Mutlilayer Perceptron (2 hidden layers)}}{53}{figure.3.4}\protected@file@percent }
\newlabel{fig:w3-4}{{3.4}{53}{Mutlilayer Perceptron (2 hidden layers)}{figure.3.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Vectorization}{54}{section.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Basic concept}{55}{subsection.3.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Scalar Multiplications}{55}{section*.16}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Vector Multiplications}{55}{section*.17}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Advantages of Vectorization (for computation)}{55}{section*.18}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Vectorizing the Neural Network}{55}{subsection.3.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Single-Layer Neural Network:}{55}{section*.19}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Vectorized Form:}{56}{section*.20}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces with associated dimensions subscripted}}{56}{figure.3.5}\protected@file@percent }
\newlabel{fig:w3-5}{{3.5}{56}{with associated dimensions subscripted}{figure.3.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{More Compact Representation:}{56}{section*.21}\protected@file@percent }
\newlabel{fig:w3-6}{{3.4.2}{57}{More Compact Representation:}{section*.21}{}}
\@writefile{toc}{\contentsline {subsubsection}{Matrix Representation of the Weight Matrices:}{57}{section*.22}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Forward Pass: 2-Hidden-Layer Neural Network}{57}{section*.23}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Forward Pass: L-Hidden-Layer Neural Network}{57}{section*.24}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.3}Backpropagation Vectorized}{58}{subsection.3.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Last Layer}{58}{section*.25}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Other Layers}{60}{section*.26}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Dimensions of the Gradient}{61}{section*.27}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Dimensions of the gradient}}{61}{figure.3.6}\protected@file@percent }
\newlabel{fig:w3-7}{{3.6}{61}{Dimensions of the gradient}{figure.3.6}{}}
\@writefile{toc}{\contentsline {paragraph}{Gradient with Respect to Weights:}{61}{section*.28}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Error Signal:}{62}{section*.29}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Minibatch Stochastic Gradient Descent}{63}{section.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.1}Stochastic Gradient Descent}{63}{subsection.3.5.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces batch gradient descent - forward pass dimensions}}{64}{figure.3.7}\protected@file@percent }
\newlabel{fig:w3-8}{{3.7}{64}{batch gradient descent - forward pass dimensions}{figure.3.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.2}Batch Gradient Descent}{64}{subsection.3.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Key Characteristics of Batch Gradient Descent:}{64}{section*.30}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Dimensions:}{64}{figure.3.7}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Cautions:}{65}{section*.33}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.3}Introducing Mini-batches}{65}{subsection.3.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Formation of Mini-batches}{65}{section*.34}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Example:}{65}{section*.35}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Mini-batch Gradient Descent Algorithm}{65}{section*.36}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Advantages of Using Mini-batches}{66}{section*.37}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces Mini-batch gradient descent generally allows for quicker updates compared to batch gradient descent, which processes the entire dataset before making an update. This can lead to faster learning initially. Batch gradient descent typically provides a smoother loss curve with less variance in the updates, making it more stable but potentially slower as the entire dataset must be evaluated before updating the weights. The fluctuations in mini-batch gradient descent can be beneficial as they introduce noise that may help escape local minima, providing a form of implicit regularisation.}}{66}{figure.3.8}\protected@file@percent }
\newlabel{fig:w3-9}{{3.8}{66}{Mini-batch gradient descent generally allows for quicker updates compared to batch gradient descent, which processes the entire dataset before making an update. This can lead to faster learning initially. Batch gradient descent typically provides a smoother loss curve with less variance in the updates, making it more stable but potentially slower as the entire dataset must be evaluated before updating the weights. The fluctuations in mini-batch gradient descent can be beneficial as they introduce noise that may help escape local minima, providing a form of implicit regularisation}{figure.3.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces Enter Caption}}{66}{figure.3.9}\protected@file@percent }
\newlabel{fig:w3-10}{{3.9}{66}{Enter Caption}{figure.3.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Training Process}{67}{section.3.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.1}Generalization in Supervised Learning}{67}{subsection.3.6.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.10}{\ignorespaces Enter Caption}}{67}{figure.3.10}\protected@file@percent }
\newlabel{fig:w3-11}{{3.10}{67}{Enter Caption}{figure.3.10}{}}
\@writefile{toc}{\contentsline {paragraph}{Cross-Validation:}{68}{section*.42}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.11}{\ignorespaces Enter Caption}}{68}{figure.3.11}\protected@file@percent }
\newlabel{fig:w3-12}{{3.11}{68}{Enter Caption}{figure.3.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.12}{\ignorespaces Enter Caption}}{69}{figure.3.12}\protected@file@percent }
\newlabel{fig:w3-13}{{3.12}{69}{Enter Caption}{figure.3.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.2}Performance Metrics Common in Deep Learning}{69}{subsection.3.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Binary Classification Metrics}{69}{section*.44}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.13}{\ignorespaces }}{70}{figure.3.13}\protected@file@percent }
\newlabel{fig:w3-14}{{3.13}{70}{}{figure.3.13}{}}
\@writefile{toc}{\contentsline {subsubsection}{ROC AUC (Area Under the ROC Curve)}{71}{section*.45}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.14}{\ignorespaces blue line: trade off across different decision thresholds}}{71}{figure.3.14}\protected@file@percent }
\newlabel{fig:w3-15}{{3.14}{71}{blue line: trade off across different decision thresholds}{figure.3.14}{}}
\@writefile{toc}{\contentsline {subsubsection}{Multi-Class Classification Metrics}{72}{section*.46}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.3}Training Tips}{72}{subsection.3.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Underfitting}{72}{section*.47}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Overfitting}{73}{section*.48}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Visualizing Features (parameters)}{73}{section*.49}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.15}{\ignorespaces Good training will show sparse hidden units across samples, indicating effective feature extraction.}}{73}{figure.3.15}\protected@file@percent }
\newlabel{fig:w3-16}{{3.15}{73}{Good training will show sparse hidden units across samples, indicating effective feature extraction}{figure.3.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.16}{\ignorespaces Poor training may result in hidden units exhibiting strong correlations and ignoring input, showing less structured patterns.}}{73}{figure.3.16}\protected@file@percent }
\newlabel{fig:w3-17}{{3.16}{73}{Poor training may result in hidden units exhibiting strong correlations and ignoring input, showing less structured patterns}{figure.3.16}{}}
\@writefile{toc}{\contentsline {subsubsection}{Training Tips: Common Issues}{73}{section*.50}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.17}{\ignorespaces Enter Caption}}{74}{figure.3.17}\protected@file@percent }
\newlabel{fig:w3-18}{{3.17}{74}{Enter Caption}{figure.3.17}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.7}Vanishing Gradient Problem}{74}{section.3.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7.1}Saturation}{74}{subsection.3.7.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.18}{\ignorespaces at the extremes, the gradient becomes flat - i.e. close to 0, NB: ReLu dos not suffer this.}}{75}{figure.3.18}\protected@file@percent }
\newlabel{fig:w3-19}{{3.18}{75}{at the extremes, the gradient becomes flat - i.e. close to 0, NB: ReLu dos not suffer this}{figure.3.18}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7.2}Overcoming the Vanishing Gradient Problem}{76}{subsection.3.7.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.19}{\ignorespaces ReLU}}{76}{figure.3.19}\protected@file@percent }
\newlabel{fig:w3-20}{{3.19}{76}{ReLU}{figure.3.19}{}}
\newlabel{fig:w3-21}{{3.7.2}{77}{Solution 3. Residual Networks: Overcoming the Vanishing Gradient Problem with Skip Connections}{section*.53}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.20}{\ignorespaces Skipping 1 layer}}{77}{figure.3.20}\protected@file@percent }
\newlabel{fig:w3-22}{{3.20}{77}{Skipping 1 layer}{figure.3.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.21}{\ignorespaces Architectural differences between a standard convolutional network (like VGG-19), a plain 34-layer network, and a 34-layer residual network.}}{79}{figure.3.21}\protected@file@percent }
\newlabel{fig:w3-23}{{3.21}{79}{Architectural differences between a standard convolutional network (like VGG-19), a plain 34-layer network, and a 34-layer residual network}{figure.3.21}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Convolutional Neural Networks I}{81}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:week4}{{4}{81}{Convolutional Neural Networks I}{chapter.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Computer Vision}}{81}{figure.4.1}\protected@file@percent }
\newlabel{fig:w4-1}{{4.1}{81}{Computer Vision}{figure.4.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Challenges Solved by Convolutional Layers}{82}{section.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Context: Fully Connected Layers}{82}{subsection.4.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Fully Connected network}}{82}{figure.4.2}\protected@file@percent }
\newlabel{fig:w4-2}{{4.2}{82}{Fully Connected network}{figure.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Convolutional Layers: Images and input features}{83}{subsection.4.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Properties of CNNs}{84}{section.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Versatility}{85}{subsection.4.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Discrete Convolution Operations}{86}{section.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Definition of Discrete Convolution}{86}{subsection.4.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Example of Discrete Convolution}{86}{subsection.4.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.3}Purpose}{87}{subsection.4.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Discrete Cross Correlation Operation}{87}{section.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}Using Convolutional Kernels as Weights}{87}{subsection.4.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Effect of Applying a Convolution}{88}{section.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.1}Feature Detection}{88}{subsection.4.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.2}Non-linear Activation}{88}{subsection.4.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.3}Example: Change from dark to light}{89}{subsection.4.5.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces NB: this is a correlation operation}}{89}{figure.4.3}\protected@file@percent }
\newlabel{fig:w4-3}{{4.3}{89}{NB: this is a correlation operation}{figure.4.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.6}Padding}{89}{section.4.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Different padding strategies: zero-padding, mirroring, and continuous extension}}{90}{figure.4.4}\protected@file@percent }
\newlabel{fig:w4-4}{{4.4}{90}{Different padding strategies: zero-padding, mirroring, and continuous extension}{figure.4.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.7}Pooling Layers}{90}{section.4.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7.1}Motivation: From Local to Global}{90}{subsection.4.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7.2}Max Pooling Operation}{91}{subsection.4.7.2}\protected@file@percent }
\newlabel{fig:w4-5}{{4.7.2}{91}{Max Pooling Operation}{subsection.4.7.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7.3}Effect of Pooling: Reducing Dimensionality \& Local Translation Invariance}{91}{subsection.4.7.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces translation invariance}}{92}{figure.4.5}\protected@file@percent }
\newlabel{fig:w4-6}{{4.5}{92}{translation invariance}{figure.4.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces In the figure, two slightly different inputs are shown, where the key features (values of 255) are shifted slightly. Despite these shifts, the result of the pooling operation is the same for both inputs. The output feature map remains invariant to these small translations.}}{92}{figure.4.6}\protected@file@percent }
\newlabel{fig:w4-7}{{4.6}{92}{In the figure, two slightly different inputs are shown, where the key features (values of 255) are shifted slightly. Despite these shifts, the result of the pooling operation is the same for both inputs. The output feature map remains invariant to these small translations}{figure.4.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7.4}Mathematical Formulation}{92}{subsection.4.7.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Max Pooling}{93}{section*.67}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Average Pooling}{93}{section*.68}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7.5}Pooling and Convolutions}{93}{subsection.4.7.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.8}Convolutional Neural Networks}{93}{section.4.8}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces Color image represented as 3D tensor with RGB channels}}{94}{figure.4.7}\protected@file@percent }
\newlabel{fig:multiple-channels}{{4.7}{94}{Color image represented as 3D tensor with RGB channels}{figure.4.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces $(1 \cdot 1 + 2 \cdot 2 + 4 \cdot 3 + 5 \cdot 4) + (0 \cdot 0 + 1 \cdot 1 + 3 \cdot 2 + 4 \cdot 3) = 56$}}{94}{figure.4.8}\protected@file@percent }
\newlabel{fig:w4-8}{{4.8}{94}{$(1 \cdot 1 + 2 \cdot 2 + 4 \cdot 3 + 5 \cdot 4) + (0 \cdot 0 + 1 \cdot 1 + 3 \cdot 2 + 4 \cdot 3) = 56$}{figure.4.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.9}{\ignorespaces \textbf  {3 input 2 output channels} - NB: 1 filter across multiple input channels to a single feature map... then 2 parallel feature maps as output (given 2 filters)}}{95}{figure.4.9}\protected@file@percent }
\newlabel{fig:w4-9}{{4.9}{95}{\textbf {3 input 2 output channels} - NB: 1 filter across multiple input channels to a single feature map... then 2 parallel feature maps as output (given 2 filters)}{figure.4.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.10}{\ignorespaces 2 Output Channels. Here we run a convolution layer that looks for eyes, then the pooling layer reduces this to 'yes there's an eye' as one data point that is locally invariant and passed onto the network.}}{96}{figure.4.10}\protected@file@percent }
\newlabel{fig:w4-10}{{4.10}{96}{2 Output Channels. Here we run a convolution layer that looks for eyes, then the pooling layer reduces this to 'yes there's an eye' as one data point that is locally invariant and passed onto the network}{figure.4.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.11}{\ignorespaces LeNet}}{97}{figure.4.11}\protected@file@percent }
\newlabel{fig:w4-11}{{4.11}{97}{LeNet}{figure.4.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.8.1}Training a CNN}{97}{subsection.4.8.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.12}{\ignorespaces Tiling of Neurons in GoogleLeNet}}{98}{figure.4.12}\protected@file@percent }
\newlabel{fig:w4-12}{{4.12}{98}{Tiling of Neurons in GoogleLeNet}{figure.4.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.13}{\ignorespaces https://distill.pub/2017/feature-visualization/appendix/}}{98}{figure.4.13}\protected@file@percent }
\newlabel{fig:w4-13}{{4.13}{98}{https://distill.pub/2017/feature-visualization/appendix/}{figure.4.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.14}{\ignorespaces https://distill.pub/2017/feature-visualization/appendix/}}{99}{figure.4.14}\protected@file@percent }
\newlabel{fig:w4-14}{{4.14}{99}{https://distill.pub/2017/feature-visualization/appendix/}{figure.4.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.15}{\ignorespaces From textures to objects}}{99}{figure.4.15}\protected@file@percent }
\newlabel{fig:w4-15}{{4.15}{99}{From textures to objects}{figure.4.15}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Convolutional Neural Networks II}{101}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:week5}{{5}{101}{Convolutional Neural Networks II}{chapter.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Labeled Data \& Augmentation}{101}{section.5.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces }}{102}{figure.5.1}\protected@file@percent }
\newlabel{fig:w5-1}{{5.1}{102}{}{figure.5.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces CIFAR-10}}{103}{figure.5.2}\protected@file@percent }
\newlabel{fig:w5-2}{{5.2}{103}{CIFAR-10}{figure.5.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Enter Caption}}{104}{figure.5.3}\protected@file@percent }
\newlabel{fig:w5-3}{{5.3}{104}{Enter Caption}{figure.5.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces Enter Caption}}{105}{figure.5.4}\protected@file@percent }
\newlabel{fig:w5-4}{{5.4}{105}{Enter Caption}{figure.5.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces What is a truck, what is a van?}}{106}{figure.5.5}\protected@file@percent }
\newlabel{fig:w5-5}{{5.5}{106}{What is a truck, what is a van?}{figure.5.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces Process Flow}}{107}{figure.5.6}\protected@file@percent }
\newlabel{fig:w5-6}{{5.6}{107}{Process Flow}{figure.5.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}Data Augmentation: generating existing examples}{108}{subsection.5.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces data augmentation}}{108}{figure.5.7}\protected@file@percent }
\newlabel{fig:w5-7}{{5.7}{108}{data augmentation}{figure.5.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.8}{\ignorespaces Adjusting Brightness, Contrast, Saturation, and Hue: This form of augmentation is particularly valuable in real-world applications where lighting is inconsistent or objects might appear under diverse conditions.}}{109}{figure.5.8}\protected@file@percent }
\newlabel{fig:w5-8}{{5.8}{109}{Adjusting Brightness, Contrast, Saturation, and Hue: This form of augmentation is particularly valuable in real-world applications where lighting is inconsistent or objects might appear under diverse conditions}{figure.5.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.9}{\ignorespaces colour augmentation: teaches the model to focus on structural details rather than relying on color information}}{109}{figure.5.9}\protected@file@percent }
\newlabel{fig:w5-9}{{5.9}{109}{colour augmentation: teaches the model to focus on structural details rather than relying on color information}{figure.5.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.10}{\ignorespaces random distortion}}{110}{figure.5.10}\protected@file@percent }
\newlabel{fig:w5-10}{{5.10}{110}{random distortion}{figure.5.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.11}{\ignorespaces Smoothed random distortion}}{110}{figure.5.11}\protected@file@percent }
\newlabel{fig:w5-11}{{5.11}{110}{Smoothed random distortion}{figure.5.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Modern CNN models}{111}{section.5.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.12}{\ignorespaces VGG Block}}{112}{figure.5.12}\protected@file@percent }
\newlabel{fig:w5-12}{{5.12}{112}{VGG Block}{figure.5.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.13}{\ignorespaces VGG}}{112}{figure.5.13}\protected@file@percent }
\newlabel{fig:w5-13}{{5.13}{112}{VGG}{figure.5.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.14}{\ignorespaces GoogLeNet: Inception Blocks}}{113}{figure.5.14}\protected@file@percent }
\newlabel{fig:w5-14}{{5.14}{113}{GoogLeNet: Inception Blocks}{figure.5.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.15}{\ignorespaces }}{114}{figure.5.15}\protected@file@percent }
\newlabel{fig:w5-15}{{5.15}{114}{}{figure.5.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.16}{\ignorespaces the residual block (portion of the block in dotted lines) learns the \textbf  {residual mapping} $g(x) = f(x)-x$.}}{118}{figure.5.16}\protected@file@percent }
\newlabel{fig:w5-16}{{5.16}{118}{the residual block (portion of the block in dotted lines) learns the \textbf {residual mapping} $g(x) = f(x)-x$}{figure.5.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.17}{\ignorespaces Standard ResNet Block}}{119}{figure.5.17}\protected@file@percent }
\newlabel{fig:w5-17}{{5.17}{119}{Standard ResNet Block}{figure.5.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.18}{\ignorespaces ResNet18}}{123}{figure.5.18}\protected@file@percent }
\newlabel{fig:w5-18}{{5.18}{123}{ResNet18}{figure.5.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.19}{\ignorespaces Fine-tuning}}{126}{figure.5.19}\protected@file@percent }
\newlabel{fig:w5-19}{{5.19}{126}{Fine-tuning}{figure.5.19}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Object Detection}{128}{section.5.3}\protected@file@percent }
\newlabel{fig:w5-20}{{5.3}{128}{Object Detection}{section.5.3}{}}
\newlabel{fig:w5-21}{{5.3}{128}{Object Detection}{section.5.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.20}{\ignorespaces multiple objects}}{129}{figure.5.20}\protected@file@percent }
\newlabel{fig:w5-22}{{5.20}{129}{multiple objects}{figure.5.20}{}}
\newlabel{fig:w5-23}{{5.3}{130}{Basic Workflow}{section*.107}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.21}{\ignorespaces predicted bounding boxes}}{131}{figure.5.21}\protected@file@percent }
\newlabel{fig:w5-24}{{5.21}{131}{predicted bounding boxes}{figure.5.21}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.22}{\ignorespaces class prediction}}{131}{figure.5.22}\protected@file@percent }
\newlabel{fig:w5-25}{{5.22}{131}{class prediction}{figure.5.22}{}}
\newlabel{fig:w5-27}{{5.3}{134}{Anchor Boxes}{section*.116}{}}
\newlabel{fig:w5-28}{{5.3}{134}{Multiscale Anchor Boxes}{section*.117}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.23}{\ignorespaces WWe have different feature maps at different scales; Anchor boxes are uniformly distributed over each feature map; This allows to create bounding boxes at different scales}}{135}{figure.5.23}\protected@file@percent }
\newlabel{fig:w5-29}{{5.23}{135}{WWe have different feature maps at different scales; Anchor boxes are uniformly distributed over each feature map; This allows to create bounding boxes at different scales}{figure.5.23}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.24}{\ignorespaces SSD Architecture}}{135}{figure.5.24}\protected@file@percent }
\newlabel{fig:w5-30}{{5.24}{135}{SSD Architecture}{figure.5.24}{}}
\newlabel{fig:w5-31}{{5.3}{137}{SSD Training and Loss Function}{section*.120}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.25}{\ignorespaces Smooth L1 loss, also known as Huber loss in some contexts}}{137}{figure.5.25}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.26}{\ignorespaces Semantic segmentation}}{138}{figure.5.26}\protected@file@percent }
\newlabel{fig:w5-32}{{5.26}{138}{Semantic segmentation}{figure.5.26}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.27}{\ignorespaces Image segmentation}}{139}{figure.5.27}\protected@file@percent }
\newlabel{fig:w5-33}{{5.27}{139}{Image segmentation}{figure.5.27}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.28}{\ignorespaces Instance segmentation}}{139}{figure.5.28}\protected@file@percent }
\newlabel{fig:w5-34}{{5.28}{139}{Instance segmentation}{figure.5.28}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.29}{\ignorespaces RHS> many parallel feature maps - each is a different channel into the next layer - each one is learning slightly different things}}{140}{figure.5.29}\protected@file@percent }
\newlabel{fig:w5-35}{{5.29}{140}{RHS> many parallel feature maps - each is a different channel into the next layer - each one is learning slightly different things}{figure.5.29}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.30}{\ignorespaces U-Net architecture}}{141}{figure.5.30}\protected@file@percent }
\newlabel{fig:w5-36}{{5.30}{141}{U-Net architecture}{figure.5.30}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.31}{\ignorespaces Enter Caption}}{142}{figure.5.31}\protected@file@percent }
\newlabel{fig:w5-37}{{5.31}{142}{Enter Caption}{figure.5.31}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Recurrent Neural Networks and Sequence Modeling}{143}{chapter.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:week6}{{6}{143}{Recurrent Neural Networks and Sequence Modeling}{chapter.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Intro to Sequence Modeling}{143}{section.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.1}Characteristics of Sequential Data}{143}{subsection.6.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces log files as time series data}}{144}{figure.6.1}\protected@file@percent }
\newlabel{fig:w6-1}{{6.1}{144}{log files as time series data}{figure.6.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.2}Challenges in Modeling Sequential Data}{145}{subsection.6.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.3}Sequences in Public Policy – Time Series}{146}{subsection.6.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces Time Series}}{146}{figure.6.2}\protected@file@percent }
\newlabel{fig:w6-2}{{6.2}{146}{Time Series}{figure.6.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Examples of Sequence Modeling Tasks}{146}{section.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.1}Forecasting and Predicting Next Steps}{146}{subsection.6.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces Electricity load forecasting}}{147}{figure.6.3}\protected@file@percent }
\newlabel{fig:w6-3}{{6.3}{147}{Electricity load forecasting}{figure.6.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces Search query completion}}{147}{figure.6.4}\protected@file@percent }
\newlabel{fig:w6-4}{{6.4}{147}{Search query completion}{figure.6.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.2}Classification}{147}{subsection.6.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.5}{\ignorespaces non-intrusive load monitoring}}{147}{figure.6.5}\protected@file@percent }
\newlabel{fig:w6-5}{{6.5}{147}{non-intrusive load monitoring}{figure.6.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.6}{\ignorespaces sound classification}}{148}{figure.6.6}\protected@file@percent }
\newlabel{fig:w6-6}{{6.6}{148}{sound classification}{figure.6.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.3}Clustering}{148}{subsection.6.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.7}{\ignorespaces Clusters of load profiles of industrial customers determined by k-Means clustering}}{148}{figure.6.7}\protected@file@percent }
\newlabel{fig:w6-7}{{6.7}{148}{Clusters of load profiles of industrial customers determined by k-Means clustering}{figure.6.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.4}Pattern Matching}{148}{subsection.6.2.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.8}{\ignorespaces pattern matching a heartbeart}}{149}{figure.6.8}\protected@file@percent }
\newlabel{fig:w6-8}{{6.8}{149}{pattern matching a heartbeart}{figure.6.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.5}Anomaly Detection}{149}{subsection.6.2.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.9}{\ignorespaces Anomaly Detection}}{149}{figure.6.9}\protected@file@percent }
\newlabel{fig:w6-9}{{6.9}{149}{Anomaly Detection}{figure.6.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.6}Motif Detection}{149}{subsection.6.2.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.10}{\ignorespaces motif detection}}{149}{figure.6.10}\protected@file@percent }
\newlabel{fig:w6-10}{{6.10}{149}{motif detection}{figure.6.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Sequence Modeling Techniques}{150}{section.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.1}Recurrent Neural Networks (RNN)}{150}{subsection.6.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.2}Long Short-Term Memory (LSTM) Networks}{150}{subsection.6.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.3}Gated Recurrent Units (GRU)}{151}{subsection.6.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.4}Convolutional Neural Networks (CNNs) for Sequences}{151}{subsection.6.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.5}Transformers and Self-Attention Mechanism}{151}{subsection.6.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.6}Choosing a Model for Sequential Data}{151}{subsection.6.3.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.11}{\ignorespaces General approaches to sequence modeling tasks}}{152}{figure.6.11}\protected@file@percent }
\newlabel{fig:w6-11}{{6.11}{152}{General approaches to sequence modeling tasks}{figure.6.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.12}{\ignorespaces BoW}}{153}{figure.6.12}\protected@file@percent }
\newlabel{fig:w6-12}{{6.12}{153}{BoW}{figure.6.12}{}}
\newlabel{fig:w6-13}{{6.3.6}{154}{Challenges in Raw Sequence Modeling}{section*.136}{}}
\newlabel{fig:w6-14}{{6.3.6}{155}{Challenges in Raw Sequence Modeling}{section*.136}{}}
\newlabel{fig:w6-15}{{6.3.6}{155}{Challenges in Raw Sequence Modeling}{section*.136}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.4}Sequence Models}{156}{section.6.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.13}{\ignorespaces Fully Connected Network}}{156}{figure.6.13}\protected@file@percent }
\newlabel{fig:w6-16}{{6.13}{156}{Fully Connected Network}{figure.6.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.14}{\ignorespaces The recurrence relationship}}{157}{figure.6.14}\protected@file@percent }
\newlabel{fig:w6-17}{{6.14}{157}{The recurrence relationship}{figure.6.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.15}{\ignorespaces Illustration of an unrolled RNN, where $h_t$ represents the hidden state at each time step and $A$ denotes the RNN unit.}}{158}{figure.6.15}\protected@file@percent }
\newlabel{fig:w6-18}{{6.15}{158}{Illustration of an unrolled RNN, where $h_t$ represents the hidden state at each time step and $A$ denotes the RNN unit}{figure.6.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.16}{\ignorespaces Unrolled RNN}}{159}{figure.6.16}\protected@file@percent }
\newlabel{fig:w6-19}{{6.16}{159}{Unrolled RNN}{figure.6.16}{}}
\newlabel{fig:w6-20}{{6.4}{159}{"Vanilla" Recurrent NNs}{figure.6.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.17}{\ignorespaces Illustration of an unrolled RNN, where $h_t$ represents the hidden state at each time step and $A$ denotes the RNN unit.}}{160}{figure.6.17}\protected@file@percent }
\newlabel{fig:w6-21}{{6.17}{160}{Illustration of an unrolled RNN, where $h_t$ represents the hidden state at each time step and $A$ denotes the RNN unit}{figure.6.17}{}}
\@writefile{toc}{\contentsline {paragraph}{Concatenation of \( h_{t-1} \) and \( x_t \)}{164}{section*.147}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Visual Representation of Dimensions}{164}{section*.148}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.18}{\ignorespaces Short term context: “the clouds are in the \textit  {sky}”}}{165}{figure.6.18}\protected@file@percent }
\newlabel{fig:w6-22}{{6.18}{165}{Short term context: “the clouds are in the \textit {sky}”}{figure.6.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.19}{\ignorespaces Long term context: “I grew up in France… I speak fluent \textit  {French}.”}}{165}{figure.6.19}\protected@file@percent }
\newlabel{fig:w6-23}{{6.19}{165}{Long term context: “I grew up in France… I speak fluent \textit {French}.”}{figure.6.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.20}{\ignorespaces RNN with a hidden state}}{166}{figure.6.20}\protected@file@percent }
\newlabel{fig:w6-24}{{6.20}{166}{RNN with a hidden state}{figure.6.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.21}{\ignorespaces Computational graph showing dependencies for an RNN model with three time steps. Boxes represent variables (not shaded) or parameters (shaded) and circles represent operators}}{168}{figure.6.21}\protected@file@percent }
\newlabel{fig:w6-25}{{6.21}{168}{Computational graph showing dependencies for an RNN model with three time steps. Boxes represent variables (not shaded) or parameters (shaded) and circles represent operators}{figure.6.21}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.22}{\ignorespaces Vanilla RNN}}{170}{figure.6.22}\protected@file@percent }
\newlabel{fig:w6-26}{{6.22}{170}{Vanilla RNN}{figure.6.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.23}{\ignorespaces LTSM}}{170}{figure.6.23}\protected@file@percent }
\newlabel{fig:w6-27}{{6.23}{170}{LTSM}{figure.6.23}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.24}{\ignorespaces Cell state}}{170}{figure.6.24}\protected@file@percent }
\newlabel{fig:w6-28}{{6.24}{170}{Cell state}{figure.6.24}{}}
\newlabel{fig:w6-29}{{6.4}{171}{1. Cell State ($c_t$) and Hidden State ($h_t$)}{figure.6.24}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.25}{\ignorespaces Cell state}}{171}{figure.6.25}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.26}{\ignorespaces Forget gate}}{171}{figure.6.26}\protected@file@percent }
\newlabel{fig:w6-30}{{6.26}{171}{Forget gate}{figure.6.26}{}}
\newlabel{fig:w6-31}{{6.4}{172}{2. Forget Gate}{figure.6.26}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.27}{\ignorespaces Input gate}}{173}{figure.6.27}\protected@file@percent }
\newlabel{fig:w6-32}{{6.27}{173}{Input gate}{figure.6.27}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.28}{\ignorespaces Input gate}}{174}{figure.6.28}\protected@file@percent }
\newlabel{fig:w6-33}{{6.28}{174}{Input gate}{figure.6.28}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.29}{\ignorespaces Output gate}}{176}{figure.6.29}\protected@file@percent }
\newlabel{fig:w6-34}{{6.29}{176}{Output gate}{figure.6.29}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.30}{\ignorespaces Output gate}}{176}{figure.6.30}\protected@file@percent }
\newlabel{fig:w6-35}{{6.30}{176}{Output gate}{figure.6.30}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.31}{\ignorespaces GRU unit}}{178}{figure.6.31}\protected@file@percent }
\newlabel{fig:w6-36}{{6.31}{178}{GRU unit}{figure.6.31}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.32}{\ignorespaces Vanilla RNN unit.png}}{179}{figure.6.32}\protected@file@percent }
\newlabel{fig:w6-37}{{6.32}{179}{Vanilla RNN unit.png}{figure.6.32}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.33}{\ignorespaces LSTM unit}}{179}{figure.6.33}\protected@file@percent }
\newlabel{fig:w6-38}{{6.33}{179}{LSTM unit}{figure.6.33}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.34}{\ignorespaces GRU unit}}{179}{figure.6.34}\protected@file@percent }
\newlabel{fig:w6-39}{{6.34}{179}{GRU unit}{figure.6.34}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.35}{\ignorespaces CNN can be used for sequential input by feeding “windows” of data.}}{182}{figure.6.35}\protected@file@percent }
\newlabel{fig:w6-40}{{6.35}{182}{CNN can be used for sequential input by feeding “windows” of data}{figure.6.35}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.36}{\ignorespaces Enter Caption}}{183}{figure.6.36}\protected@file@percent }
\newlabel{fig:w6-41}{{6.36}{183}{Enter Caption}{figure.6.36}{}}
\newlabel{fig:w6-42}{{6.4}{184}{Causal Convolutions}{section*.167}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.37}{\ignorespaces Causal Convolutions}}{184}{figure.6.37}\protected@file@percent }
\newlabel{fig:w6-43}{{6.37}{184}{Causal Convolutions}{figure.6.37}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.38}{\ignorespaces Dilated Convolutions}}{185}{figure.6.38}\protected@file@percent }
\newlabel{fig:w6-44}{{6.38}{185}{Dilated Convolutions}{figure.6.38}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.39}{\ignorespaces Transformer Architecture}}{186}{figure.6.39}\protected@file@percent }
\newlabel{fig:w6-45}{{6.39}{186}{Transformer Architecture}{figure.6.39}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.5}Example Task: Time Series Forecasting}{187}{section.6.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.40}{\ignorespaces WaveNet}}{187}{figure.6.40}\protected@file@percent }
\newlabel{fig:w6-46}{{6.40}{187}{WaveNet}{figure.6.40}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.41}{\ignorespaces TCN}}{188}{figure.6.41}\protected@file@percent }
\newlabel{fig:w6-47}{{6.41}{188}{TCN}{figure.6.41}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.42}{\ignorespaces Tree Ensembles for Time Series Prediction }}{190}{figure.6.42}\protected@file@percent }
\newlabel{fig:w6-48}{{6.42}{190}{Tree Ensembles for Time Series Prediction}{figure.6.42}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Natural Language Processing I}{191}{chapter.7}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:week7}{{7}{191}{Natural Language Processing I}{chapter.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Introduction}{191}{section.7.1}\protected@file@percent }
\newlabel{fig:w7-1}{{7.1}{192}{Example: Analyzing Party Manifestos and Climate Risk Disclosure}{section*.175}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Document Embedding}{193}{section.7.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.1}{\ignorespaces BoW}}{193}{figure.7.1}\protected@file@percent }
\newlabel{fig:w7-2}{{7.1}{193}{BoW}{figure.7.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.2}{\ignorespaces Word Occurrence Matrix / Count vectorisation}}{193}{figure.7.2}\protected@file@percent }
\newlabel{fig:w7-3}{{7.2}{193}{Word Occurrence Matrix / Count vectorisation}{figure.7.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.3}{\ignorespaces \textbf  {Document embeddings}: where documents are mapped to a space where similar topics are clustered (e.g., \textit  {government borrowings}, \textit  {energy markets}).}}{195}{figure.7.3}\protected@file@percent }
\newlabel{fig:w7-4}{{7.3}{195}{\textbf {Document embeddings}: where documents are mapped to a space where similar topics are clustered (e.g., \textit {government borrowings}, \textit {energy markets})}{figure.7.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.4}{\ignorespaces \textbf  {Word embeddings}: as learned by neural networks. Words like "development" and "transactions" are closer due to their contextual similarity, indicating that embeddings capture semantic meanings.}}{196}{figure.7.4}\protected@file@percent }
\newlabel{fig:w7-5}{{7.4}{196}{\textbf {Word embeddings}: as learned by neural networks. Words like "development" and "transactions" are closer due to their contextual similarity, indicating that embeddings capture semantic meanings}{figure.7.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.5}{\ignorespaces Zipf's law: a common characteristic of word frequency distributions, where a small number of tokens occur very frequently, while a large number of tokens occur infrequently; the frequency of tokens decreases as their complexity (unigram, bigram, trigram) increases.}}{197}{figure.7.5}\protected@file@percent }
\newlabel{fig:w7-6}{{7.5}{197}{Zipf's law: a common characteristic of word frequency distributions, where a small number of tokens occur very frequently, while a large number of tokens occur infrequently; the frequency of tokens decreases as their complexity (unigram, bigram, trigram) increases}{figure.7.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.6}{\ignorespaces NB Bert-Attention are integrated}}{199}{figure.7.6}\protected@file@percent }
\newlabel{fig:w7-7}{{7.6}{199}{NB Bert-Attention are integrated}{figure.7.6}{}}
\newlabel{fig:w7-8}{{7.2}{200}{Embeddings I - One-Hot Encoding}{section*.186}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.7}{\ignorespaces Skip-gram model: $P$(“the”, “man”, “his”, “son”|”loves”)}}{202}{figure.7.7}\protected@file@percent }
\newlabel{fig:w7-9}{{7.7}{202}{Skip-gram model: $P$(“the”, “man”, “his”, “son”|”loves”)}{figure.7.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.8}{\ignorespaces CBOW: $P$ (``loves'' $|$ ``the'', ``man'', ``his'', ``son'')}}{202}{figure.7.8}\protected@file@percent }
\newlabel{fig:w7-10}{{7.8}{202}{CBOW: $P$ (``loves'' $|$ ``the'', ``man'', ``his'', ``son'')}{figure.7.8}{}}
\@writefile{toc}{\contentsline {subsubsection}{Probability and Vector Representation}{202}{section*.190}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.9}{\ignorespaces Prediction task: predict the context words $w^{(t+j)}$ based on the center word $w^{(t)}$}}{205}{figure.7.9}\protected@file@percent }
\newlabel{fig:w7-11}{{7.9}{205}{Prediction task: predict the context words $w^{(t+j)}$ based on the center word $w^{(t)}$}{figure.7.9}{}}
\newlabel{fig:w7-12}{{1}{206}{Training}{Item.97}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.10}{\ignorespaces Skip-Gram model architecture. Input = centre word $v_i$; output = context word $u_j$; matrix vector of interest = $v_c$ which represents centre word (we could also use the context word's representation in matrix vector $u_o$, but that's more customary for CBOW)}}{207}{figure.7.10}\protected@file@percent }
\newlabel{fig:w7-13}{{7.10}{207}{Skip-Gram model architecture. Input = centre word $v_i$; output = context word $u_j$; matrix vector of interest = $v_c$ which represents centre word (we could also use the context word's representation in matrix vector $u_o$, but that's more customary for CBOW)}{figure.7.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.11}{\ignorespaces In the CBOW architecture, the embeddings for the context words are indeed represented by the weight matrix $W'$}}{215}{figure.7.11}\protected@file@percent }
\newlabel{fig:w7-14}{{7.11}{215}{In the CBOW architecture, the embeddings for the context words are indeed represented by the weight matrix $W'$}{figure.7.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.12}{\ignorespaces Sentiment analysis approach: combines a pretraining approach with DL architecture to perform the classification task.}}{218}{figure.7.12}\protected@file@percent }
\newlabel{fig:w7-15}{{7.12}{218}{Sentiment analysis approach: combines a pretraining approach with DL architecture to perform the classification task}{figure.7.12}{}}
\newlabel{fig:w7-16}{{7.2}{218}{Basic RNNs for Sentiment Analysis}{section*.201}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.13}{\ignorespaces Character sequence modeling: [m,ac,h,i,n.e]}}{219}{figure.7.13}\protected@file@percent }
\newlabel{fig:w7-17}{{7.13}{219}{Character sequence modeling: [m,ac,h,i,n.e]}{figure.7.13}{}}
\newlabel{fig:w7-18}{{7.2}{220}{Bidirectional RNN Architecture}{section*.205}{}}
\@writefile{lot}{\contentsline {table}{\numberline {7.1}{\ignorespaces Intuition - what comes later downstream is informative of the previous word.}}{221}{table.7.1}\protected@file@percent }
\newlabel{tab:fill_in_the_blanks}{{7.1}{221}{Intuition - what comes later downstream is informative of the previous word}{table.7.1}{}}
\newlabel{fig:w7-19}{{7.2}{223}{3. Dropout}{section*.213}{}}
\newlabel{fig:w7-20}{{7.2}{223}{3. Dropout}{section*.213}{}}
\gdef \@abspage@last{224}
