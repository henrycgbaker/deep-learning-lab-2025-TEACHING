% =============================================================================
% Week 3: Joint Random Variables
% =============================================================================

\chapter{Week 3: Joint Random Variables}
\label{ch:week3}

\begin{bluebox}[Learning Objectives]
After completing this chapter, you should be able to:
\begin{itemize}
    \item Define joint random variables and understand how functions of multiple random variables work
    \item Distinguish between joint, marginal, and conditional probability mass functions
    \item Apply marginalisation to extract single-variable distributions from joint distributions
    \item Verify independence of random variables using the factorisation criterion
    \item Explain the distinction between independence and conditional independence
    \item Compute expectations and variances for common distributions (Binomial, Multinomial, Poisson)
    \item Apply the linearity of expectation and understand when variance is additive
    \item Introduce covariance as a measure of how random variables vary together
\end{itemize}
\end{bluebox}

\textbf{Prerequisites:} This chapter assumes familiarity with basic probability theory (\cref{ch:week1}) and conditional probability (\cref{ch:week2}), including Bayes' theorem and the law of total probability. You should be comfortable with the definitions of random variables, probability mass functions, and cumulative distribution functions.

% =============================================================================
\section{Joint Random Variables and Their Distributions}
\label{sec:joint-rv}
% =============================================================================

When modelling real-world phenomena, we often need to consider multiple random quantities simultaneously. For instance, we might want to study both the height and weight of individuals in a population, or the number of sunny days and average temperature in a month. Joint random variables provide the framework for analysing such relationships.

\subsection{Functions of Multiple Random Variables}

\begin{definition}[Joint Random Variable]
\label{def:joint-rv}
Given an experiment with sample space $S$, suppose $X$ and $Y$ are random variables that map each outcome $s \in S$ to values $X(s)$ and $Y(s)$ respectively. For any function $g: \mathbb{R}^2 \to \mathbb{R}$, the composition $g(X, Y)$ defines a new random variable that maps each outcome $s$ to $g(X(s), Y(s))$.
\end{definition}

This definition tells us that when we have two random variables $X$ and $Y$ defined on the same sample space, any function of these variables is itself a random variable. Common examples include:
\begin{itemize}
    \item The sum $X + Y$
    \item The product $XY$
    \item The maximum $\max(X, Y)$
    \item The indicator $\mathbf{1}_{X > Y}$ (equals 1 if $X > Y$, 0 otherwise)
\end{itemize}

\begin{example}[Sum of Dice Rolls]
\label{ex:dice-sum}
Let $X$ be the outcome of rolling a fair six-sided die, and let $Y$ be the outcome of rolling a second independent fair die. The sum $S = X + Y$ is a new random variable taking values in $\{2, 3, \ldots, 12\}$. The probability that $S = 7$ is determined by counting the pairs $(x, y)$ with $x + y = 7$:
\[
P(S = 7) = P(\{(1,6), (2,5), (3,4), (4,3), (5,2), (6,1)\}) = \frac{6}{36} = \frac{1}{6}.
\]
\end{example}

\subsection{The Random Walk (Motivating Example)}

A \emph{random walk} is a mathematical model that describes a path consisting of successive random steps. It serves as an excellent illustration of how joint random variables arise naturally.

\begin{example}[Simple Random Walk]
\label{ex:random-walk}
Consider a particle starting at position 0 on the integer line. At each time step, it moves either one step to the right (with probability $p$) or one step to the left (with probability $1-p$). Let $X_i$ denote the $i$-th step:
\[
X_i = \begin{cases}
+1 & \text{with probability } p \\
-1 & \text{with probability } 1-p
\end{cases}
\]

After $n$ steps, the particle's position is $S_n = X_1 + X_2 + \cdots + X_n$. The joint distribution of $(X_1, X_2, \ldots, X_n)$ determines the distribution of $S_n$.

For a symmetric random walk ($p = 1/2$), the position after $n$ steps has:
\begin{itemize}
    \item Expected position: $\E[S_n] = \sum_{i=1}^{n} \E[X_i] = n \cdot 0 = 0$
    \item Variance: $\Var(S_n) = n\Var(X_1) = n \cdot 1 = n$ (since the $X_i$ are independent)
\end{itemize}

The number of right steps $R$ follows a Binomial$(n, p)$ distribution, and the final position is $S_n = R - (n - R) = 2R - n$. Thus:
\[
P(S_n = k) = P(R = \tfrac{n+k}{2}) = \binom{n}{\frac{n+k}{2}} p^{\frac{n+k}{2}} (1-p)^{\frac{n-k}{2}}
\]
for $k \in \{-n, -n+2, \ldots, n-2, n\}$ (i.e., $k$ has the same parity as $n$).
\end{example}

% =============================================================================
\section{Joint Distributions: How Two Random Variables Interact}
\label{sec:joint-distributions}
% =============================================================================

The joint distribution captures all probabilistic information about how two (or more) random variables behave together. We now formalise the key concepts.

\subsection{Joint Probability Mass Function}

\begin{definition}[Joint PMF]
\label{def:joint-pmf}
For discrete random variables $X$ and $Y$, the \textbf{joint probability mass function} is defined as:
\[
p_{X,Y}(x, y) = P(X = x, Y = y)
\]
for all possible values $x$ and $y$.
\end{definition}

\begin{bluebox}[Properties of Joint PMF]
The joint PMF must satisfy two conditions:
\begin{enumerate}
    \item \textbf{Non-negativity:} $p_{X,Y}(x, y) \geq 0$ for all $x, y$
    \item \textbf{Normalisation:} $\displaystyle\sum_{x}\sum_{y} p_{X,Y}(x, y) = 1$
\end{enumerate}
\end{bluebox}

The joint PMF can be visualised as a three-dimensional bar chart, where the height of each bar at position $(x, y)$ represents the probability $P(X = x, Y = y)$.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{images/week_3/Screenshot 2023-10-21 140956.png}
    \caption{Visualisation of a joint PMF as a 3D bar chart. Each bar's height represents the probability of the corresponding $(x, y)$ pair.}
    \label{fig:joint-pmf}
\end{figure}

\subsection{Joint Cumulative Distribution Function}

\begin{definition}[Joint CDF]
\label{def:joint-cdf}
For random variables $X$ and $Y$ (discrete or continuous), the \textbf{joint cumulative distribution function} is:
\[
F_{X,Y}(x, y) = P(X \leq x, Y \leq y)
\]
\end{definition}

The joint CDF gives the probability that $X$ is at most $x$ \emph{and} $Y$ is at most $y$ simultaneously. For discrete random variables, it can be computed from the joint PMF:
\[
F_{X,Y}(x, y) = \sum_{x' \leq x} \sum_{y' \leq y} p_{X,Y}(x', y')
\]

% =============================================================================
\section{Marginal Distributions}
\label{sec:marginal}
% =============================================================================

Given the joint distribution of $(X, Y)$, we can recover the distribution of $X$ alone (or $Y$ alone) by \emph{marginalising} over the other variable.

\subsection{Marginal PMF}

\begin{definition}[Marginal PMF]
\label{def:marginal-pmf}
The \textbf{marginal PMF of $X$} is obtained by summing the joint PMF over all values of $Y$:
\[
p_X(x) = P(X = x) = \sum_{y} P(X = x, Y = y) = \sum_{y} p_{X,Y}(x, y)
\]
Similarly, the \textbf{marginal PMF of $Y$} is:
\[
p_Y(y) = P(Y = y) = \sum_{x} P(X = x, Y = y) = \sum_{x} p_{X,Y}(x, y)
\]
\end{definition}

\begin{intuition}[Geometric Interpretation of Marginalisation]
Marginalisation ``flattens'' the joint distribution along one axis. If the joint PMF is visualised as a 3D bar chart, the marginal PMF of $X$ is obtained by projecting all bars onto the $X$-axis, summing the heights of bars with the same $x$-coordinate.
\end{intuition}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{images/week_3/Screenshot 2023-10-21 141320.png}
    \caption{Marginal PMF of $X$ obtained by summing over all values of $Y$. The 2D distribution along the $X$-axis represents $p_X(x)$.}
    \label{fig:marginal-pmf}
\end{figure}

The term ``marginal'' comes from the practice of writing row and column totals in the margins of contingency tables, as we shall see in the examples below.

\begin{theorem}[Law of Total Probability for Random Variables]
\label{thm:marginal-validity}
The marginal PMF $p_X(x)$ is a valid probability mass function. That is:
\begin{enumerate}
    \item $p_X(x) \geq 0$ for all $x$
    \item $\sum_x p_X(x) = 1$
\end{enumerate}
\end{theorem}

\begin{proof}
Non-negativity follows from the non-negativity of the joint PMF. For normalisation:
\[
\sum_x p_X(x) = \sum_x \sum_y p_{X,Y}(x, y) = \sum_x \sum_y P(X = x, Y = y) = 1
\]
where the last equality uses the normalisation property of the joint PMF.
\end{proof}

% =============================================================================
\section{Conditional Distributions}
\label{sec:conditional}
% =============================================================================

Conditional distributions describe the behaviour of one random variable given knowledge about another.

\subsection{Conditional PMF}

\begin{definition}[Conditional PMF]
\label{def:conditional-pmf}
The \textbf{conditional PMF of $Y$ given $X = x$} is:
\[
p_{Y|X}(y|x) = P(Y = y \mid X = x) = \frac{P(X = x, Y = y)}{P(X = x)} = \frac{p_{X,Y}(x, y)}{p_X(x)}
\]
provided that $P(X = x) > 0$.
\end{definition}

\begin{bluebox}[{Key Relationship: Joint, Marginal, and Conditional}]
The three types of distributions are related by:
\[
\underbrace{p_{X,Y}(x, y)}_{\text{Joint}} = \underbrace{p_{Y|X}(y|x)}_{\text{Conditional}} \cdot \underbrace{p_X(x)}_{\text{Marginal}}
\]
This is the \emph{chain rule of probability} for random variables.
\end{bluebox}

\begin{intuition}[Geometric Interpretation]
If the joint PMF is a 3D bar chart, the conditional PMF $p_{Y|X}(y|x)$ is obtained by taking a ``slice'' at a fixed value of $X = x$, then renormalising so that the slice sums to 1. You are conditioning on knowing the value of $X$, and asking: given this information, what is the distribution of $Y$?
\end{intuition}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{images/week_3/Screenshot 2023-10-21 141759.png}
    \caption{Conditional PMF of $Y$ given $X = x$: a cross-section of the joint distribution, renormalised.}
    \label{fig:conditional-pmf}
\end{figure}

% =============================================================================
\section{Independence of Random Variables}
\label{sec:independence}
% =============================================================================

Independence is a fundamental concept that captures when knowledge of one variable provides no information about another.

\begin{definition}[Independence of Random Variables]
\label{def:independence}
Random variables $X$ and $Y$ are \textbf{independent} if and only if their joint distribution factorises as the product of their marginals:
\begin{itemize}
    \item \textbf{Discrete case:} $P(X = x, Y = y) = P(X = x) \cdot P(Y = y)$ for all $x, y$
    \item \textbf{Continuous case:} $P(X \leq x, Y \leq y) = P(X \leq x) \cdot P(Y \leq y)$ for all $x, y$
\end{itemize}
\end{definition}

\begin{bluebox}[Testing for Independence]
To verify that $X$ and $Y$ are independent, you must check that $P(X = x, Y = y) = P(X = x) \cdot P(Y = y)$ holds for \textbf{all} pairs $(x, y)$.

To show that $X$ and $Y$ are \textbf{not} independent, it suffices to find \textbf{one} pair $(x, y)$ where the equality fails.
\end{bluebox}

\subsection{Conditional Independence}

Independence can also be defined relative to a third variable.

\begin{definition}[Conditional Independence]
\label{def:conditional-independence}
Random variables $X$ and $Y$ are \textbf{conditionally independent given $Z$} if:
\[
P(X \leq x, Y \leq y \mid Z = z) = P(X \leq x \mid Z = z) \cdot P(Y \leq y \mid Z = z)
\]
for all $x$, $y$, and $z$ where $P(Z = z) > 0$.
\end{definition}

\begin{warning}[Independence vs Conditional Independence]
\textbf{Independence does not imply conditional independence}, and \textbf{conditional independence does not imply independence}. These are distinct concepts.
\end{warning}

\begin{example}[Independence Does Not Imply Conditional Independence]
Let $X$ indicate whether your friend Bob calls you next Friday, and $Y$ indicate whether your friend Alice calls. Suppose $X$ and $Y$ are independent (Bob's and Alice's decisions are unrelated).

Let $Z$ indicate that \emph{exactly one} friend calls. Conditional on $Z = 1$, if $X = 1$ (Bob calls), then necessarily $Y = 0$ (Alice does not call), and vice versa. Thus $X$ and $Y$ are perfectly (negatively) dependent given $Z$, even though they are marginally independent.
\end{example}

\begin{example}[Conditional Independence Does Not Imply Independence]
Consider a medical scenario where $X$ is an indicator for smoking, $Y$ is an indicator for lung cancer, and $Z$ is an indicator for a genetic marker that predisposes to both smoking behaviour and lung cancer.

It may be that $X$ and $Y$ are conditionally independent given $Z$ (once we control for genetics, smoking and cancer are unrelated in this hypothetical), yet marginally dependent (smokers have higher cancer rates in the population).
\end{example}

\begin{remark}
The distinction between independence and conditional independence is central to causal inference. Much of applied causal analysis involves finding conditions under which variables become conditionally independent, enabling identification of causal effects. This idea underlies techniques such as regression adjustment and instrumental variables.
\end{remark}

% =============================================================================
\section{Worked Example: Gene and Disease}
\label{sec:gene-disease-example}
% =============================================================================

Let us work through a complete example involving joint, marginal, and conditional distributions.

\begin{example}[Gene and Disease Association]
\label{ex:gene-disease}
A random sample from a population yields the following data:
\begin{itemize}
    \item $X$: indicator for the presence of a certain gene (1 = present, 0 = absent)
    \item $Y$: indicator for developing a certain disease (1 = disease, 0 = no disease)
\end{itemize}

The joint distribution is given by the contingency table:

\begin{table}[h!]
\centering
\begin{tabular}{c|cc|c}
\toprule
 & $Y = 1$ & $Y = 0$ & Marginal of $X$ \\
\midrule
$X = 1$ & $5/100$ & $20/100$ & $25/100$ \\
$X = 0$ & $3/100$ & $72/100$ & $75/100$ \\
\midrule
Marginal of $Y$ & $8/100$ & $92/100$ & $1$ \\
\bottomrule
\end{tabular}
\caption{Contingency table for gene ($X$) and disease ($Y$) with marginal distributions shown in the margins.}
\label{tab:gene-disease}
\end{table}

\textbf{Joint PMF:} The entries in the interior of the table give the joint PMF directly. For instance, $P(X = 1, Y = 1) = 5/100 = 0.05$.

\textbf{Marginal PMFs:}
\begin{itemize}
    \item $P(X = 1) = 5/100 + 20/100 = 25/100 = 0.25$
    \item $P(X = 0) = 3/100 + 72/100 = 75/100 = 0.75$
    \item $P(Y = 1) = 5/100 + 3/100 = 8/100 = 0.08$
    \item $P(Y = 0) = 20/100 + 72/100 = 92/100 = 0.92$
\end{itemize}

\textbf{Conditional Distribution of $Y$ given $X = 1$:}
\begin{align*}
P(Y = 1 \mid X = 1) &= \frac{P(X = 1, Y = 1)}{P(X = 1)} = \frac{5/100}{25/100} = \frac{5}{25} = 0.2 \\
P(Y = 0 \mid X = 1) &= \frac{P(X = 1, Y = 0)}{P(X = 1)} = \frac{20/100}{25/100} = \frac{20}{25} = 0.8
\end{align*}

Thus, the conditional distribution of $Y$ given $X = 1$ is $\text{Bernoulli}(0.2)$: among those with the gene, 20\% develop the disease.

\textbf{Testing for Independence:}
\begin{align*}
P(X = 1) \cdot P(Y = 1) &= 0.25 \times 0.08 = 0.02 \\
P(X = 1, Y = 1) &= 0.05
\end{align*}

Since $0.05 \neq 0.02$, we have found a pair where the factorisation fails, so $X$ and $Y$ are \textbf{not independent}. The gene and disease are associated-having the gene increases the probability of disease from the baseline rate of $8\%$ to $20\%$.
\end{example}

% =============================================================================
\section{Extended Worked Example: Joint to Marginal to Conditional}
\label{sec:extended-example}
% =============================================================================

The following example demonstrates the systematic computation of marginal and conditional distributions from a joint PMF.

\begin{example}[Complete Analysis of a Joint Distribution]
\label{ex:complete-joint-analysis}
Consider two discrete random variables $X$ and $Y$ with the following joint distribution:

\[
\begin{array}{c|ccc}
p_{X,Y}(x,y) & Y = 1 & Y = 2 & Y = 3 \\
\hline
X = 1 & 0.10 & 0.20 & 0.10 \\
X = 2 & 0.05 & 0.25 & 0.10 \\
X = 3 & 0.05 & 0.10 & 0.05 \\
\end{array}
\]

\textbf{Step 1: Verify Normalisation}
\[
\sum_x \sum_y p_{X,Y}(x,y) = 0.10 + 0.20 + 0.10 + 0.05 + 0.25 + 0.10 + 0.05 + 0.10 + 0.05 = 1.00 \checkmark
\]

\textbf{Step 2: Compute Marginal PMF of $X$}
\begin{align*}
p_X(1) &= p_{X,Y}(1,1) + p_{X,Y}(1,2) + p_{X,Y}(1,3) = 0.10 + 0.20 + 0.10 = 0.40 \\
p_X(2) &= p_{X,Y}(2,1) + p_{X,Y}(2,2) + p_{X,Y}(2,3) = 0.05 + 0.25 + 0.10 = 0.40 \\
p_X(3) &= p_{X,Y}(3,1) + p_{X,Y}(3,2) + p_{X,Y}(3,3) = 0.05 + 0.10 + 0.05 = 0.20
\end{align*}

\textbf{Step 3: Compute Marginal PMF of $Y$}
\begin{align*}
p_Y(1) &= p_{X,Y}(1,1) + p_{X,Y}(2,1) + p_{X,Y}(3,1) = 0.10 + 0.05 + 0.05 = 0.20 \\
p_Y(2) &= p_{X,Y}(1,2) + p_{X,Y}(2,2) + p_{X,Y}(3,2) = 0.20 + 0.25 + 0.10 = 0.55 \\
p_Y(3) &= p_{X,Y}(1,3) + p_{X,Y}(2,3) + p_{X,Y}(3,3) = 0.10 + 0.10 + 0.05 = 0.25
\end{align*}

\textbf{Step 4: Compute Conditional PMF of $X$ given $Y$}

For $Y = 1$ (where $p_Y(1) = 0.20$):
\begin{align*}
p_{X|Y}(1|1) &= \frac{0.10}{0.20} = 0.50, \quad
p_{X|Y}(2|1) = \frac{0.05}{0.20} = 0.25, \quad
p_{X|Y}(3|1) = \frac{0.05}{0.20} = 0.25
\end{align*}

For $Y = 2$ (where $p_Y(2) = 0.55$):
\begin{align*}
p_{X|Y}(1|2) &= \frac{0.20}{0.55} \approx 0.364, \quad
p_{X|Y}(2|2) = \frac{0.25}{0.55} \approx 0.455, \quad
p_{X|Y}(3|2) = \frac{0.10}{0.55} \approx 0.182
\end{align*}

For $Y = 3$ (where $p_Y(3) = 0.25$):
\begin{align*}
p_{X|Y}(1|3) &= \frac{0.10}{0.25} = 0.40, \quad
p_{X|Y}(2|3) = \frac{0.10}{0.25} = 0.40, \quad
p_{X|Y}(3|3) = \frac{0.05}{0.25} = 0.20
\end{align*}

\textbf{Step 5: Compute Conditional PMF of $Y$ given $X$}

For $X = 1$ (where $p_X(1) = 0.40$):
\begin{align*}
p_{Y|X}(1|1) &= \frac{0.10}{0.40} = 0.25, \quad
p_{Y|X}(2|1) = \frac{0.20}{0.40} = 0.50, \quad
p_{Y|X}(3|1) = \frac{0.10}{0.40} = 0.25
\end{align*}

For $X = 2$ (where $p_X(2) = 0.40$):
\begin{align*}
p_{Y|X}(1|2) &= \frac{0.05}{0.40} = 0.125, \quad
p_{Y|X}(2|2) = \frac{0.25}{0.40} = 0.625, \quad
p_{Y|X}(3|2) = \frac{0.10}{0.40} = 0.25
\end{align*}

For $X = 3$ (where $p_X(3) = 0.20$):
\begin{align*}
p_{Y|X}(1|3) &= \frac{0.05}{0.20} = 0.25, \quad
p_{Y|X}(2|3) = \frac{0.10}{0.20} = 0.50, \quad
p_{Y|X}(3|3) = \frac{0.05}{0.20} = 0.25
\end{align*}
\end{example}

% =============================================================================
\section{Expectation}
\label{sec:expectation}
% =============================================================================

The expectation (or expected value, or mean) of a random variable is a measure of its ``centre''-a weighted average of the possible values, where the weights are the probabilities.

\begin{definition}[Expected Value]
\label{def:expectation}
For a discrete random variable $X$, the \textbf{expected value} (or \textbf{mean}) is:
\[
\E[X] = \sum_{x} x \cdot P(X = x) = \sum_{x} x \cdot p_X(x)
\]
\end{definition}

\begin{bluebox}[Interpretation of Expectation]
The expected value $\E[X]$ represents the long-run average of $X$ if the experiment were repeated infinitely many times. It need not be a value that $X$ can actually take.
\end{bluebox}

\begin{example}[Fair Die]
\label{ex:die-expectation}
Let $X$ be the result of rolling a fair six-sided die. Each outcome has probability $1/6$, so:
\[
\E[X] = 1 \cdot \frac{1}{6} + 2 \cdot \frac{1}{6} + 3 \cdot \frac{1}{6} + 4 \cdot \frac{1}{6} + 5 \cdot \frac{1}{6} + 6 \cdot \frac{1}{6} = \frac{21}{6} = 3.5
\]
Note that $X$ never actually equals 3.5-the expected value is a theoretical average, not a possible outcome.
\end{example}

\begin{example}[Coin Flips]
\label{ex:coin-expectation}
Let $X$ be the number of heads when flipping a fair coin twice. The PMF is:
\[
P(X = 0) = \frac{1}{4}, \quad P(X = 1) = \frac{1}{2}, \quad P(X = 2) = \frac{1}{4}
\]
Thus:
\[
\E[X] = 0 \cdot \frac{1}{4} + 1 \cdot \frac{1}{2} + 2 \cdot \frac{1}{4} = 0 + \frac{1}{2} + \frac{1}{2} = 1
\]
\end{example}

\begin{example}[Bernoulli Distribution]
\label{ex:bernoulli-expectation}
For $X \sim \text{Bernoulli}(p)$:
\[
\E[X] = 0 \cdot (1-p) + 1 \cdot p = p
\]
The expected value of a Bernoulli random variable is simply its success probability.
\end{example}

\subsection{Linearity of Expectation}

One of the most powerful properties of expectation is its linearity.

\begin{theorem}[Linearity of Expectation]
\label{thm:linearity-expectation}
For any random variables $X$ and $Y$ (not necessarily independent) and any constants $a, b, c$:
\begin{enumerate}
    \item $\E[aX + b] = a\E[X] + b$
    \item $\E[X + Y] = \E[X] + \E[Y]$
    \item More generally: $\E\left[\sum_{i=1}^{n} a_i X_i + c\right] = \sum_{i=1}^{n} a_i \E[X_i] + c$
\end{enumerate}
\end{theorem}

\begin{warning}[Linearity Only Works for Linear Functions]
Linearity of expectation does \textbf{not} extend to non-linear functions. In general:
\[
\E[g(X)] \neq g(\E[X])
\]
For example, $\E[X^2] \neq (\E[X])^2$ in general. The difference $\E[X^2] - (\E[X])^2$ is precisely the variance, as we shall see.
\end{warning}

\begin{example}[Linearity in Action]
\label{ex:linearity}
Let $X$ be the number of heads in two coin flips ($\E[X] = 1$), and let $Z$ be the result of rolling a fair die ($\E[Z] = 3.5$). For $W = X + Z$:
\[
\E[W] = \E[X + Z] = \E[X] + \E[Z] = 1 + 3.5 = 4.5
\]
This holds regardless of whether $X$ and $Z$ are independent.
\end{example}

% =============================================================================
\section{Variance}
\label{sec:variance}
% =============================================================================

While expectation measures the centre of a distribution, variance measures its spread-how far values typically deviate from the mean.

\begin{definition}[Variance]
\label{def:variance}
The \textbf{variance} of a random variable $X$ with mean $\mu = \E[X]$ is:
\[
\Var(X) = \E[(X - \mu)^2] = \E[(X - \E[X])^2]
\]
The \textbf{standard deviation} is $\sigma = \sqrt{\Var(X)}$.
\end{definition}

\begin{bluebox}[Computational Formula for Variance]
Expanding the definition using linearity of expectation yields a computationally convenient formula:
\[
\Var(X) = \E[X^2] - (\E[X])^2
\]
This says: ``the variance is the expected value of the square minus the square of the expected value.''
\end{bluebox}

\begin{proof}[Derivation of computational formula]
\begin{align*}
\Var(X) &= \E[(X - \E[X])^2] \\
&= \E[X^2 - 2X\E[X] + (\E[X])^2] \\
&= \E[X^2] - 2\E[X]\E[X] + (\E[X])^2 \quad \text{(linearity, and $\E[X]$ is a constant)} \\
&= \E[X^2] - 2(\E[X])^2 + (\E[X])^2 \\
&= \E[X^2] - (\E[X])^2
\end{align*}
\end{proof}

\begin{example}[Variance of a Fair Die]
\label{ex:die-variance}
Let $X$ be the result of rolling a fair six-sided die. We computed $\E[X] = 3.5$.

For $\E[X^2]$:
\[
\E[X^2] = \frac{1}{6}(1^2 + 2^2 + 3^2 + 4^2 + 5^2 + 6^2) = \frac{1 + 4 + 9 + 16 + 25 + 36}{6} = \frac{91}{6} \approx 15.17
\]

Thus:
\[
\Var(X) = \E[X^2] - (\E[X])^2 = \frac{91}{6} - (3.5)^2 = \frac{91}{6} - \frac{49}{4} = \frac{182 - 147}{12} = \frac{35}{12} \approx 2.92
\]
\end{example}

\subsection{Properties of Variance}

\begin{theorem}[Variance Properties]
\label{thm:variance-properties}
For random variables $X$ and $Y$ and constants $a, c$:
\begin{enumerate}
    \item $\Var(c) = 0$ for any constant $c$
    \item $\Var(X + c) = \Var(X)$ (shifting by a constant does not change spread)
    \item $\Var(aX) = a^2 \Var(X)$ (scaling by $a$ scales variance by $a^2$)
    \item If $X$ and $Y$ are \textbf{independent}: $\Var(X + Y) = \Var(X) + \Var(Y)$
\end{enumerate}
\end{theorem}

\begin{warning}[Variance is NOT Linear]
Unlike expectation, variance does not obey simple linearity:
\begin{itemize}
    \item $\Var(aX) = a^2 \Var(X) \neq a \Var(X)$ (unless $a \in \{0, 1\}$)
    \item $\Var(X + Y) \neq \Var(X) + \Var(Y)$ in general (only holds for independent $X, Y$)
\end{itemize}
The general formula is $\Var(X + Y) = \Var(X) + \Var(Y) + 2\Cov(X, Y)$, where covariance measures the dependence between $X$ and $Y$. See \cref{sec:covariance-intro} and \cref{ch:week7} for details.
\end{warning}

% =============================================================================
\section{Mean and Variance of Common Distributions}
\label{sec:common-distributions}
% =============================================================================

We now derive the mean and variance for several important discrete distributions. For comprehensive reference material on these and other distributions, see \cref{ch:distributions}.

\subsection{Bernoulli Distribution}

\begin{definition}[Bernoulli Distribution]
A random variable $X$ follows a \textbf{Bernoulli distribution} with parameter $p \in [0,1]$, written $X \sim \text{Bernoulli}(p)$, if:
\[
P(X = 1) = p, \quad P(X = 0) = 1 - p
\]
\end{definition}

\begin{theorem}[Bernoulli Mean and Variance]
\label{thm:bernoulli-moments}
For $X \sim \text{Bernoulli}(p)$:
\[
\E[X] = p, \qquad \Var(X) = p(1-p)
\]
\end{theorem}

\begin{proof}
\textbf{Mean:}
\[
\E[X] = 0 \cdot (1-p) + 1 \cdot p = p
\]

\textbf{Variance:} First compute $\E[X^2]$. Since $X \in \{0, 1\}$, we have $X^2 = X$, so $\E[X^2] = \E[X] = p$. Thus:
\[
\Var(X) = \E[X^2] - (\E[X])^2 = p - p^2 = p(1-p)
\]
\end{proof}

\begin{remark}
The variance $p(1-p)$ is maximised when $p = 1/2$, giving $\Var(X) = 1/4$. This makes intuitive sense: a fair coin has maximum uncertainty, while a coin that always lands heads ($p = 1$) or always tails ($p = 0$) has zero variance.
\end{remark}

\subsection{Binomial Distribution}

\begin{definition}[Binomial Distribution]
A random variable $X$ follows a \textbf{Binomial distribution} with parameters $n \in \mathbb{N}$ and $p \in [0,1]$, written $X \sim \text{Binomial}(n, p)$, if it represents the number of successes in $n$ independent Bernoulli$(p)$ trials:
\[
P(X = k) = \binom{n}{k} p^k (1-p)^{n-k}, \quad k = 0, 1, \ldots, n
\]
\end{definition}

\begin{theorem}[Binomial Mean and Variance]
\label{thm:binomial-moments}
For $X \sim \text{Binomial}(n, p)$:
\[
\E[X] = np, \qquad \Var(X) = np(1-p)
\]
\end{theorem}

\begin{proof}
The key insight is that a Binomial$(n, p)$ random variable can be written as a sum of independent Bernoulli$(p)$ random variables. Let $X_1, X_2, \ldots, X_n$ be independent with $X_i \sim \text{Bernoulli}(p)$. Then $X = X_1 + X_2 + \cdots + X_n$.

\textbf{Mean:} By linearity of expectation:
\[
\E[X] = \E\left[\sum_{i=1}^{n} X_i\right] = \sum_{i=1}^{n} \E[X_i] = \sum_{i=1}^{n} p = np
\]

\textbf{Variance:} Since the $X_i$ are independent:
\[
\Var(X) = \Var\left(\sum_{i=1}^{n} X_i\right) = \sum_{i=1}^{n} \Var(X_i) = \sum_{i=1}^{n} p(1-p) = np(1-p)
\]
\end{proof}

\begin{intuition}
The mean $np$ is exactly what you would expect: if each trial succeeds with probability $p$, then on average $np$ out of $n$ trials will succeed. The variance $np(1-p)$ grows linearly with $n$, but the standard deviation $\sqrt{np(1-p)}$ grows only as $\sqrt{n}$, so the \emph{relative} spread (coefficient of variation) decreases as $n$ increases.
\end{intuition}

\begin{example}[Coin Flipping]
\label{ex:binomial-coins}
Flip a fair coin 100 times. Let $X$ be the number of heads. Then $X \sim \text{Binomial}(100, 0.5)$, so:
\[
\E[X] = 100 \times 0.5 = 50, \qquad \Var(X) = 100 \times 0.5 \times 0.5 = 25
\]
The standard deviation is $\sigma = 5$, so we expect roughly $50 \pm 10$ heads (within two standard deviations) about 95\% of the time.
\end{example}

\subsection{Multinomial Distribution}

The multinomial distribution generalises the binomial to experiments with more than two possible outcomes.

\begin{definition}[Multinomial Distribution]
Consider $n$ independent trials, each resulting in one of $k$ categories with probabilities $p_1, p_2, \ldots, p_k$ (where $\sum_{j=1}^{k} p_j = 1$). Let $X_j$ be the count of outcomes in category $j$. The vector $(X_1, \ldots, X_k)$ follows a \textbf{Multinomial distribution}:
\[
P(X_1 = n_1, \ldots, X_k = n_k) = \frac{n!}{n_1! n_2! \cdots n_k!} p_1^{n_1} p_2^{n_2} \cdots p_k^{n_k}
\]
where $n_1 + n_2 + \cdots + n_k = n$.
\end{definition}

\begin{theorem}[Multinomial Marginal Moments]
\label{thm:multinomial-moments}
For a Multinomial$(n; p_1, \ldots, p_k)$ distribution, each marginal $X_j$ satisfies:
\[
\E[X_j] = np_j, \qquad \Var(X_j) = np_j(1 - p_j)
\]
\end{theorem}

\begin{proof}
Each category $j$ can be viewed as a ``success'' with probability $p_j$, while all other outcomes are ``failures.'' Thus $X_j$, the count in category $j$, follows a Binomial$(n, p_j)$ distribution marginally. The result follows from \cref{thm:binomial-moments}.
\end{proof}

\begin{remark}
The multinomial distribution also has a covariance structure. For $i \neq j$:
\[
\Cov(X_i, X_j) = -np_i p_j
\]
The negative covariance reflects the constraint $\sum_j X_j = n$: if more outcomes fall in category $i$, fewer can fall in category $j$.
\end{remark}

\subsection{Poisson Distribution}

The Poisson distribution models the number of events occurring in a fixed interval when events happen at a constant average rate.

\begin{definition}[Poisson Distribution]
A random variable $X$ follows a \textbf{Poisson distribution} with parameter $\lambda > 0$, written $X \sim \text{Poisson}(\lambda)$, if:
\[
P(X = k) = \frac{\lambda^k e^{-\lambda}}{k!}, \quad k = 0, 1, 2, \ldots
\]
\end{definition}

\begin{theorem}[Poisson Mean and Variance]
\label{thm:poisson-moments}
For $X \sim \text{Poisson}(\lambda)$:
\[
\E[X] = \lambda, \qquad \Var(X) = \lambda
\]
\end{theorem}

\begin{proof}
\textbf{Mean:}
\begin{align*}
\E[X] &= \sum_{k=0}^{\infty} k \cdot \frac{\lambda^k e^{-\lambda}}{k!}
= \sum_{k=1}^{\infty} k \cdot \frac{\lambda^k e^{-\lambda}}{k!} \quad \text{(the $k=0$ term vanishes)} \\
&= \sum_{k=1}^{\infty} \frac{\lambda^k e^{-\lambda}}{(k-1)!}
= \lambda e^{-\lambda} \sum_{k=1}^{\infty} \frac{\lambda^{k-1}}{(k-1)!} \\
&= \lambda e^{-\lambda} \sum_{m=0}^{\infty} \frac{\lambda^{m}}{m!} \quad \text{(substituting $m = k-1$)} \\
&= \lambda e^{-\lambda} \cdot e^{\lambda} = \lambda
\end{align*}

\textbf{Variance:} We compute $\E[X(X-1)]$ first (this is a useful trick):
\begin{align*}
\E[X(X-1)] &= \sum_{k=0}^{\infty} k(k-1) \cdot \frac{\lambda^k e^{-\lambda}}{k!}
= \sum_{k=2}^{\infty} \frac{\lambda^k e^{-\lambda}}{(k-2)!} \\
&= \lambda^2 e^{-\lambda} \sum_{k=2}^{\infty} \frac{\lambda^{k-2}}{(k-2)!}
= \lambda^2 e^{-\lambda} \cdot e^{\lambda} = \lambda^2
\end{align*}

Now:
\[
\E[X^2] = \E[X(X-1)] + \E[X] = \lambda^2 + \lambda
\]

Therefore:
\[
\Var(X) = \E[X^2] - (\E[X])^2 = \lambda^2 + \lambda - \lambda^2 = \lambda
\]
\end{proof}

\begin{bluebox}[Poisson: Mean Equals Variance]
A distinctive property of the Poisson distribution is that $\E[X] = \Var(X) = \lambda$. This property is sometimes used as a diagnostic: if empirical data show the sample mean approximately equal to the sample variance, a Poisson model may be appropriate.
\end{bluebox}

\begin{example}[Poisson Approximation to Binomial]
\label{ex:poisson-binomial}
When $n$ is large and $p$ is small such that $\lambda = np$ is moderate, the Binomial$(n, p)$ distribution is well-approximated by Poisson$(\lambda)$. This can be seen by comparing moments:
\begin{itemize}
    \item Binomial mean: $np = \lambda$ \checkmark
    \item Binomial variance: $np(1-p) \approx np = \lambda$ when $p$ is small \checkmark
\end{itemize}
\end{example}

% =============================================================================
\section{Introduction to Covariance}
\label{sec:covariance-intro}
% =============================================================================

We have seen that $\Var(X + Y) = \Var(X) + \Var(Y)$ only holds when $X$ and $Y$ are independent. To understand what happens in the general case, we need the concept of covariance.

\begin{definition}[Covariance]
\label{def:covariance}
The \textbf{covariance} of random variables $X$ and $Y$ is:
\[
\Cov(X, Y) = \E[(X - \E[X])(Y - \E[Y])]
\]
\end{definition}

\begin{bluebox}[Covariance Computational Formula]
Expanding the definition gives:
\[
\Cov(X, Y) = \E[XY] - \E[X]\E[Y]
\]
Covariance is ``the expectation of the product minus the product of the expectations.''
\end{bluebox}

\begin{intuition}
Covariance measures how $X$ and $Y$ ``vary together'':
\begin{itemize}
    \item If $X$ tends to be above its mean when $Y$ is above its mean (and below when below), then $\Cov(X, Y) > 0$ (positive covariance)
    \item If $X$ tends to be above its mean when $Y$ is below its mean, then $\Cov(X, Y) < 0$ (negative covariance)
    \item If there is no systematic relationship, $\Cov(X, Y) \approx 0$
\end{itemize}
\end{intuition}

\begin{theorem}[Variance of a Sum]
\label{thm:variance-sum}
For any random variables $X$ and $Y$:
\[
\Var(X + Y) = \Var(X) + \Var(Y) + 2\Cov(X, Y)
\]
\end{theorem}

\begin{corollary}
If $X$ and $Y$ are independent, then $\Cov(X, Y) = 0$, so $\Var(X + Y) = \Var(X) + \Var(Y)$.
\end{corollary}

\begin{warning}[Zero Covariance Does Not Imply Independence]
If $\Cov(X, Y) = 0$, we say $X$ and $Y$ are \textbf{uncorrelated}. However, uncorrelated does not imply independent. Covariance only captures \emph{linear} relationships; $X$ and $Y$ can be perfectly dependent (e.g., $Y = X^2$) yet have zero covariance.

A detailed treatment of covariance, correlation, and the distinction between independence and uncorrelatedness appears in \cref{ch:week7}.
\end{warning}

% =============================================================================
\section{Summary}
\label{sec:week3-summary}
% =============================================================================

\begin{bluebox}[Chapter Summary]
\textbf{Joint Distributions:}
\begin{itemize}
    \item Joint PMF: $p_{X,Y}(x, y) = P(X = x, Y = y)$
    \item Joint CDF: $F_{X,Y}(x, y) = P(X \leq x, Y \leq y)$
\end{itemize}

\textbf{Marginal and Conditional:}
\begin{itemize}
    \item Marginal: $p_X(x) = \sum_y p_{X,Y}(x, y)$
    \item Conditional: $p_{Y|X}(y|x) = \dfrac{p_{X,Y}(x, y)}{p_X(x)}$
    \item Chain rule: $p_{X,Y}(x, y) = p_{Y|X}(y|x) \cdot p_X(x)$
\end{itemize}

\textbf{Independence:}
\begin{itemize}
    \item $X \perp Y$ iff $P(X = x, Y = y) = P(X = x) P(Y = y)$ for all $x, y$
    \item Independence $\not\Leftrightarrow$ Conditional independence
\end{itemize}

\textbf{Expectation and Variance:}
\begin{itemize}
    \item $\E[X] = \sum_x x \cdot P(X = x)$
    \item Linearity: $\E[aX + bY + c] = a\E[X] + b\E[Y] + c$ (always)
    \item $\Var(X) = \E[X^2] - (\E[X])^2$
    \item $\Var(aX) = a^2 \Var(X)$
    \item $\Var(X + Y) = \Var(X) + \Var(Y)$ only if $X \perp Y$
\end{itemize}

\textbf{Key Distribution Moments:}
\begin{center}
\begin{tabular}{lcc}
\toprule
Distribution & Mean & Variance \\
\midrule
Bernoulli$(p)$ & $p$ & $p(1-p)$ \\
Binomial$(n, p)$ & $np$ & $np(1-p)$ \\
Poisson$(\lambda)$ & $\lambda$ & $\lambda$ \\
\bottomrule
\end{tabular}
\end{center}
\end{bluebox}
