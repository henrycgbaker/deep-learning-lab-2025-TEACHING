% =============================================================================
% Week 9: Linear Algebra II
% =============================================================================

\chapter{Linear Algebra II}
\label{ch:week9}

\thispagestyle{empty}
{\large \textbf{Mid Terms Fall 2023  --  Henry Baker}}
\par\noindent\rule{\textwidth}{0.4pt} 

\vspace*{0.3cm} 
\begin{center} 
	{\Large \bf M4DS Mid Terms Revision: Session 9 \\ Linear Algebra II}
	\vspace{2mm}
	
\end{center}  
\vspace{0.4cm}

\section{Linear Dependence}

Linear dependence means redundant information: at least one of the vectors can be expressed as a linear combination of the others. \\

A set of vectors is linearly independent if and only if there is \textbf{no} redundancy in the information they provide.\\

If even one row (or column) of a matrix is linearly dependent on others, then the entire set of rows (or columns) is considered linearly dependent. For a matrix to be completely linearly independent, every single row (or column) must be linearly independent of the others.\\

For square matrices, another way to think about this is in terms of the determinant. A square matrix is linearly independent (and hence invertible) if and only if its determinant is non-zero. If even one row or column is linearly dependent, the determinant of the matrix will be zero, indicating linear dependence.\\

\begin{tcolorbox}
\textbf{TL;DR}: Linear Dependence: there exists a set of scalar coefficients, not all zero, such that a weighted sum of the columns (or rows) equals a zero vector 
\[c_1v_1 + c_2v_2 + \cdots + c_nv_n = 0\]
\end{tcolorbox}

\begin{itemize}
    \item Let the sets $S = {v_1, v_2, \cdots, v_n} $ be a set of vectors.
    \item Members of the set $s$ are \textbf{linearly dependent if at least one of the members of the set can be written as a linear combination of the other members.}
    \item i.e. if no scalar multiple of one vector can be added to a scalar multiple of the other to produce the zero vector, unless both scalars are zero. 
    \item In simpler terms, one vector cannot be expressed as a multiple of the other.
    \item formally, Linear Dependence: if and only if
    \[c_1v_1 + c_2v_2 + \cdots + c_nv_n = 0\]
    \textit{where at least one of the $c_i$s is non-zero (non-trivial solution)}
    \item intuitively: \begin{itemize}
        \item for Linear Dependence, although it looks like we are constraining oursleves by setting the condition ``= 0'', we are in fact not. We can just throw in some given/arbitrary scalar values for $c$ that make this true, i.e. we can simply just rescale some of the vectors such that they cancel out other (and thus reach 0). This is because they are ultimately acting on the same dimensional plane; you can rescale them to express the movement of the other(s). \textbf{Thus, by saying that some rescaled combination of vectors come to 0, we are saying that they are acting on / moving across / spanning the same dimensions; this is thus redundant information, and thus the columns are linearly dependent.}
        \item Whereas, for Linear Independence, we are in fact \textbf{more constrained: there is NO way to just scale vectors such that the above equation is true, in that they don't operate on the same dimensions, so can't just be rescaled to counter the movements/effects of another(s)} (and thus reach 0). Thus by NOT coming to 0, we are specifying that each column here must provide new information re: movements through dimensional space. Each has to be unique, in that you cannot rescale another vector to express that same information (if you could, you could rescale that vector in the opposite direction to the initial vector to reach 0.) Thus by saying it cannot = 0, we are saying it has to be unique; thus linear independence.
        
        \item Eg --- two Linearly Independent vectors:
        \begin{bmatrix}
            4 \\
            0
        \end{bmatrix}
        and
        \begin{bmatrix}
            0 \\
            3
        \end{bmatrix}
        no way to express a function that gets us from 4, 0 to 0, 3; \textbf{the only unique solution to get them to equal each other is by setting both $C_1$ and $C_2$ to 0} --- which is the trivial example which doesn't satisfy linear dependence.
        \item Linearly Independent vectors represent arrows pointing in different dimensions; they are different pieces of information.
        
    \end{itemize}
\end{itemize}\\
The set $\left\{\begin{bmatrix}2 \\ 3\end{bmatrix}, \begin{bmatrix}4 \\ 6\end{bmatrix}\right\}$ is linearly dependent. \\

The set $\left\{\begin{bmatrix}0 \\ 3\end{bmatrix}, \begin{bmatrix}4 \\ 0\end{bmatrix}\right\}$ is linearly independent.\\

The set $\left\{\begin{bmatrix}0 \\ 3\end{bmatrix}, \begin{bmatrix}4 \\ 0\end{bmatrix}, \begin{bmatrix}2 \\ 2\end{bmatrix}\right\}$ is linearly dependent.\\

\subsection{Examples of proving linear (in)dependence}

\begin{tcolorbox}
    1) set up the equation $c_1\mathbf{v}_1 + c_2\mathbf{v}_2 \cdot + c_n\mathbf{v}_n = \mathbf{0}$\\
    2) solve the system of equations: take one $c_nv_n$ over to the other side so you can write one of the $c_nv_n$ as a function of the others, then plug back in, etc.  
\end{tcolorbox}

\subsubsection{E.g.1 Proving Linear Dependence: finding non-trivial solution}
\begin{tcolorbox}
\begin{align*}
\textbf{Vectors:} \quad 
&\mathbf{v}_1 = \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix}, \quad 
\mathbf{v}_2 = \begin{bmatrix} 2 \\ 4 \\ 6 \end{bmatrix}, \quad 
\mathbf{v}_3 = \begin{bmatrix} -1 \\ -2 \\ -3 \end{bmatrix} 
\end{align*}

\textbf{Step 1}: Set up the equation \[c_1\mathbf{v}_1 + c_2\mathbf{v}_2 + c_3\mathbf{v}_3 = \mathbf{0}\]
This leads to the system: 
\begin{align*}
1c_1 + 2c_2 --- 1c_3 &= 0 \\
2c_1 + 4c_2 --- 2c_3 &= 0 \\
3c_1 + 6c_2 --- 3c_3 &= 0 \\
\end{align*}
\end{tcolorbox}

\textbf{Step 2}: Solve the system:
\begin{itemize}
    \item \textbf{Finding a Non-Trivial Solution:}
    \begin{itemize}
        \item e.g: let's assign $c_2 = 1$ and $c_3 = 0$...
        \item \ldots{}then from the first equation: $c_1 + 2(-1) + -1(0) = 0$...
        \item \ldots{}gives $c_1 = 2$.
        \item Thus $c_1 = 2; c_2 = -1; c_3 = 0$ is a valid solution
        \item when dependent, a non-trivial solution always exists (ie other than $c_1 = c_2 = c_3 = 0$)
    \end{itemize}
    \item \textbf{Observation:}
    \begin{itemize}
        \item each row is a multiple of the first row: not independent.
        \item example of a homogeneous system, where all the equations are multiples of each other
    \end{itemize}
    \end{itemize}
We were able to find the non-zero coefficients $c_1, c_2, c_3$ that satisfy the linear combination $c_1\mathbf{v}_1 + c_2\mathbf{v}_2 + c_3\mathbf{v}_3 = \mathbf{0}$.\\
We found a non-trivial solution where not all $c_i$ are 0: so they are linearly dependent. 
\subsubsection{E.g.2 Proving Linear Independence: only a trivial solution}
\begin{tcolorbox}
\begin{align*}
\textbf{Vectors:} \quad 
&\mathbf{v}_1 = \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}, \quad 
\mathbf{v}_2 = \begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix}, \quad 
\mathbf{v}_3 = \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix} 
\end{align*}

\textbf{Step 1}: Set up the equation \[c_1\mathbf{v}_1 + c_2\mathbf{v}_2 + c_3\mathbf{v}_3 = \mathbf{0}\]
This leads to the system: 
\begin{align*}
1c_1 + 0c_2 + 0c_3 &= 0 \\
0c_1 + 1c_2 + 0c_3 &= 0 \\
0c_1 + 0c_2 + 1c_3 &= 0 \\
\end{align*}

Simplify:
\begin{align*}
c_1 &= 0 \\
c_2 &= 0 \\
c_3 &= 0 \\
\end{align*}
\end{tcolorbox}

\textbf{Step 2}: Solve the system:\\
Since the only solution to this system is the trivial solution $c_1 = c_2 = c_3 = 0$, the vectors are linearly independent.

\textbf{Observation}: \\
Can see that there is no way to multiply one of the vectors to map it onto another = independent.

\subsubsection{E.g.3 Proving Linear Dependence (Mixed Case)}
\begin{tcolorbox}
\begin{align*}
\textbf{Vectors:} \quad 
&\mathbf{v}_1 = \begin{bmatrix} 1 \\ 3 \\ 4 \end{bmatrix}, \quad 
\mathbf{v}_2 = \begin{bmatrix} 2 \\ 6 \\ 8 \end{bmatrix}, \quad 
\mathbf{v}_3 = \begin{bmatrix} 0 \\ 1 \\ 2 \end{bmatrix} 
\end{align*}

\textbf{Step 1}: Set up the equation \[c_1\mathbf{v}_1 + c_2\mathbf{v}_2 + c_3\mathbf{v}_3 = \mathbf{0}\]
This leads to the system: 
\begin{align*}
c_1 + 2c_2 &= 0 \\
3c_1 + 6c_2 + c_3 &= 0 \\
\end{align*}
\end{tcolorbox}

\textbf{Step 2}: Solve the system:\\
\textbf{Finding a Non-Trivial Solution:}
\begin{itemize}
    \item Solving the first equation for $c_1$ gives $c_1 = -2c_2$.
    \item Substitute $c_1$ into the second equation: $3(-2c_2) + 6c_2 + c_3 = 0$ simplifies to $c_3 = 0$.
    \item Substitute $c_1$ and $c_3$ into the third equation: it holds true for any value of $c_2$.
\end{itemize}

We found a non-trivial solution where not all $c_i$ are 0: so the vectors are linearly dependent.

\textbf{Observation}: \\
Can see that $v_1$ and $v_2$ are linearly dependent.

\subsubsection{Determining Linear Dependence or Independence}
\begin{tcolorbox}
\textbf{Given Vectors:} \\
\mathbf{v}_1 = \begin{bmatrix} 0 \\ 3 \end{bmatrix}, \quad
\mathbf{v}_2 = \begin{bmatrix} 4 \\ 0 \end{bmatrix}, \quad
\mathbf{v}_3 = \begin{bmatrix} 2 \\ 2 \end{bmatrix}

\textbf{Step 1}: Set up the equation for linear combination: \\
\[ c_1\mathbf{v}_1 + c_2\mathbf{v}_2 + c_3\mathbf{v}_3 = \mathbf{0} \]
This leads to the system of linear equations:
\begin{align*}
0c_1 + 4c_2 + 2c_3 &= 0 \\
3c_1 + 0c_2 + 2c_3 &= 0
\end{align*}
\end{tcolorbox}


\textbf{Step 2}: Solve the system: 
\begin{itemize}
    \item From the first equation, we get: \( 2c_2 + c_3 = 0 \rightarrow c_3 = -2c_2 \). 
    \item Substituting \( c_3 = -2c_2 \) into the second equation gives \( 3c_1 --- 4c_2 = 0 \rightarrow 3c_1 = 4c_2 \).
\end{itemize}


\textbf{Conclusion}: \\
The system does not provide a clear non-trivial solution for \( c_1, c_2, c_3 \), suggesting that the vectors are linearly independent under typical circumstances.



\section{The Span}
\subsection{in terms of Vector Space / Basis vectors}

We define the span of a the set of vectors $S$ as \textbf{all linear combination of the vectors in the set}. 

\begin{itemize}
    \item the span of $v_1$ and $v_2$ incl all vectors that can be formed by taking $a \cdot v_1 + b \cdot v_2$, where both vectors are linearly independent (independence not a hard requirement, but otherwise, we are just including  redundant info if you think about it geometrically)
    \item the span of 2 linearly independent vectors $v_1 \in \mathbb{R}^2$ and $v_2 \in \mathbb{R}^2$ is $\mathbb{R}^2$ : this means that any vector in the 2D space can be expressed as a linear combination of $v_1$ or $v_2$
    \item this implies that $v_1$ and $v_2$ form a basis for $R^2$ since they can generate the entire 2D space through their linear combos (i.e. they provide a complete representation of the 2D space)
\end{itemize}

\subsection{in terms of the Spanning Set}
Going the other way, if $V$ is a vector space and $S$ a set of vectors in $V$, then we say that \textbf{$S$ is a spanning set for $V$ if all the dimensions of $V$ can be represented by linear combinations of $S$}.

\begin{itemize}
    \item a Spanning Set $S$ of a vector space $V$ is a set of vectors, such that every vector in $V$ can be expressed as a linear combo of the vectors in $S$.
    \item if $S$ spans $V$, it means that $S$ contains enough linearly independent vectors to cover the entire vector space $V$
    \item generally a spanning set $S$ must contain as least as many elements as the linearly independent vectors from $V$\ldots{} but a Spanning set not a minimal thing: as long as it contains $n$ linearly independent vectors then it can also include additional vectors, but they will be redundant info.
\end{itemize}



\textbf{There are exactly $n$ linearly independent vectors in $\mathbb{R}^n$} (no more, no less!)
\begin{itemize}
    \item \ldots{} 2 in $\mathbb{R}^2$
    \item \ldots{} 3 in $\mathbb{R}^3$
    \item .\ldots{} $n$ in $\mathbb{R}^n$
    \item these vectors = the basis for the space
    \item Geometrically: visualise $\mathbb{R}^2$ vector space: every movement can be made up of linear combination is movements along $y$ and along $x$ --- these are the fundamental building blocks which are \textbf{orthogonal (linearly independent)} --- everything else is linearly dependent.
    \item  linearly dependent vectors present redundant information, in that they are \textbf{colinear} (geometric interpretation in R2) or \textbf{co-planar} (geometric interpretation of R2+) to the basis of the space, 
    \item i.e. one of the vectors of the set can be represented as a combination of others \textbf{(it doesn't give us any new dimensions)}
    \item A set of vectors is said to be linearly independent if no vector in the set can be written as a linear combination of the others: none of the vectors in the set is redundant in terms of the information it provides about the vector space.
\end{itemize}

 The \textbf{dimension of a vector space $V$ is defined as the maximum number of linearly independent vectors in $V$}. It represents the number of coordinates needed to specify any vector in the space uniquely.
\begin{itemize}
    \item For a set $S$ to span a vector space $V$, it must contain at least as many vectors as the dimension of $V$.
    \item However, having exactly as many vectors in $S$ as the dimension of $V$ does not automatically guarantee that $S$ spans $V$. Those vectors also need to be linearly independent.
\end{itemize}

\begin{tcolorbox}
    A Spanning Set $S$ must contain at last as many elements as the linearly independent vectors from $V$.\\
   \\ 
   There are exactly $n$ linearly independent vectors in $R^n$\\
   \\
    Dimension of a vector space $V$ is defined as the maximum number of linearly independent vectors in $V$. It represents the number of coordinates needed to specify any vector in the space uniquely
\end{tcolorbox}

\section{The Determinant}

The determinant is a scalar value that can be computed from the elements of a square matrix.\\
Often denoted as $det(\textbf{A})$ or $|\textbf{A}|$\\
= a way to multiply entries of the matrix to produce a single number.

\subsection{Uses}
\begin{itemize}
    \item \textbf{Linear Transformations} --- the absolute value of the determinant of a matrix representing a linear transformation reflects how the transformation changes the area (in 2D) or volume (in 3D) of shapes. A negative determinant indicates that the transformation also involves a reflection.
    \item \textbf{Eigenvalues and Characteristic Polynomial}: The characteristic polynomial of a matrix, used to find its eigenvalues, is derived from its determinant. (I think this is related to the above, as a determinant tells you nature of the transformation, and eigenvalue-eigenvector pairings do the same??)
    \item \textbf{to determine if a matrix has an inverse} (is non-singular). A square matrix is singular (non-invertible) if and only if its determinant is zero. (see Savov p. 169-171)
    \item \textbf{to check for linear independence} --- related to checking if has inverse: as a matrix has an inverse if and only if its columns are linearly independent (see Savov p. 167)
    \item \textbf{to compute areas and volumes} --- the absolute value of the determinant of a matrix formed by vectors representing the sides of a parallelogram (in 2D) or a parallelepiped (in 3D) gives the area (in 2D) or volume (in 3D) of that geometric shape. (see Savov p. 161)
    \item \textbf{solving systems of equations} (see Cramerâ€™s Rule, Savov p. 166)   
\end{itemize}

\subsection{Calculation}
\begin{tcolorbox}
    For a 1x1 Matrix (scalar):
\[ A = \begin{bmatrix} a \end{bmatrix} \] 
the determinant is simply the value of that single element:
\[ \det(A) = a \]
\end{tcolorbox}

\begin{tcolorbox}
For a 2x2 Matrix:
\[ A = \begin{bmatrix} a & b \\ c & d \end{bmatrix} \]
\[ \det(A) = ad --- bc \]
\end{tcolorbox}

\begin{tcolorbox}
For a 3x3 Matrix:
\[ A = \begin{bmatrix} a & b & c \\ d & e & f \\ g & h & i \end{bmatrix} \]
\[ \det(A) = a(ei --- fh) --- b(di --- fg) + c(dh --- eg) \]
\end{tcolorbox}


The matrices bigger than 3x3 ,the determinant calculation becomes complex and typically calculated using methods such as expansion by minors or Laplace expansion.\\

go through https://www.khanacademy.org/math/linear-algebra/vectors-and-spaces/linear-independence/v/linear-algebra-introduction-to-linear-independence from 7 mins onwards

\begin{itemize}
    \item general intutition product of top left x bottom right, minus product of top right x bottom left...
    \item \ldots{} but this only works properly for 2x2 matrix, after that it gets complicated...
    \item \ldots{} for 3x3: take each element in term as a constant, multiply the remaining minor, alternately subtract then add each element.
\end{itemize}

\begin{tcolorbox}
General formula: \[{det}(A) = \sum_{j=1}^{n} (-1)^{1+j}a_{1j}M_{1j}\] 
\textit{where $M_ij$ is called the minor associated with $a_{i}$: the determinant of the submatrix generated by removing row i and column j from the matrix A.}
\end{tcolorbox}

In the determinant formula, the $(-1)^{1+j}$ just reverses the sign starting with +, then -, then +, etc. This means we go up to the power of $+j$ where $j$ is the number of columns in the matrix, so 1+[odd value column] is to raise it to an even power.

2x2 Matrix:
\begin{align*}
    \begin{vmatrix}
    a_{11} & a_{12} \\
    a_{21} & a_{22} \\
    \end{vmatrix}
    = a_{11}a_{22} --- a_{12}a_{21}
\end{align*}

3x3 Matrix
\begin{align*}
    \text{det}(A) = a \cdot \text{det}\left|
    \begin{array}{cc}
    e & h \\
    f & i \\
    \end{array}
    \right| --- b \cdot \text{det}\left|
    \begin{array}{cc}
    d & g \\
    f & i \\
    \end{array}
    \right| + c \cdot \text{det}\left|
    \begin{array}{cc}
    d & g \\
    e & h \\
    \end{array}
    \right|    
\end{align*}
\[ \rightarrow \det(A) = a(ei --- fh) --- b(di --- fg) + c(dh --- eg) \]

Resources for determinants: Savov, p. 158-161

\section{Matrix Inverse}
\[A^{-1} A = I\]
Not all matrices invertible: only exists if and only if: 
\[det(A) \neq 0\]
Inverse defined: 
\[A^{-1} = \frac{1}{\text{det}(A)} \text{adj}(A)\]
Where adj(A) is the adjugate matrix\\


The inverse of a 2x2 matrix \( A = \begin{pmatrix} a & b \\ c & d \end{pmatrix} \) is given by
\[
A^{-1} = \frac{1}{\det(A)} \begin{pmatrix} d & -b \\ -c & a \end{pmatrix}
\]
where \( \det(A) = ad --- bc \) is the determinant of \( A \). The matrix \( A \) is invertible if and only if \( \det(A) \neq 0 \).

Given the matrix \( A = \begin{pmatrix} 3 & 0 \\ 0 & 3 \end{pmatrix} \).

The determinant of \( A \) is calculated as
\[
\det(A) = 3 \times 3 --- 0 \times 0 = 9
\]

The formula for the inverse of a 2x2 matrix is
\[
A^{-1} = \frac{1}{\det(A)} \begin{pmatrix} d & -b \\ -c & a \end{pmatrix}
\]

Applying this to matrix \( A \):
\[
A^{-1} = \frac{1}{9} \begin{pmatrix} 3 & 0 \\ 0 & 3 \end{pmatrix} = \begin{pmatrix} \frac{1}{3} & 0 \\ 0 & \frac{1}{3} \end{pmatrix}
\]

Thus, the inverse of matrix \( A \) is \( A^{-1} = \begin{pmatrix} \frac{1}{3} & 0 \\ 0 & \frac{1}{3} \end{pmatrix} \).\\

OR AN EASIER WAY is just to eyeball it for easy matrices:
\[
\begin{pmatrix} 3 & 0 \\ 0 & 3 \end{pmatrix} \begin{pmatrix} \frac{1}{3} & 0 \\ 0 & \frac{1}{3} \end{pmatrix} = \begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix}
\]
So, what do you have to multiply the values, to get to 1, 0, 0, 1.


\subsection{Adjugate Matrix}
= the tranpose of the matrix of cofactors:
\[adj(A) = C^T\]
... see Savov p169-171 for matrix of cofactors

\section{To know}
\begin{tcolorbox}
\textbf{
    \begin{enumerate}
    \item a matrix has an inverse, if and only if it has a non-zero determiant
    \item a matrix has an inverse, if and only if its columns are linearly independent
    \item thus, a matrix has a non-zero determinant if and only if its columns are linearly independent.
\end{enumerate}}

\textit{This is actually intuitive: if you think of linear dependence as about vectors/columns providing information about the vector space, and the determinant tells you about the nature of the linear transformation.\ldots{} if you have redundant (dependent) columns then this means the matrix is not a clean representation of a linear transformation, qhich is they it has implications for its determinant.}\\

So the direction of causality is:\\
\textbf{Linear Independence $\rightarrow$ Non-zero Determinant $\rightarrow$ has Inverse}

\end{tcolorbox}


\subsection{Conditions under which a matrix is invertible:}

\textbf{\begin{enumerate}
    \item matrix is square
    \item columns are linearly independent (full rank)
    \item non-zero determinant
\end{enumerate}}

Given the matrix \( A = \begin{pmatrix} 1 & 0 \\ 2 & 0 \end{pmatrix} \), the determinant of \( A \) is calculated as
\[
\det(A) = 1 \times 0 --- 0 \times 2 = 0
\]
Since the determinant of \( A \) is zero, the columns are not linearly independent, the matrix \( A \) is not invertible.\\

\subsubsection{Proof by contradiction:}

Suppose a linearly dependent Matrix \textit{is invertible}\ldots{}\\

Let $A$ be a square matrix with dependent columns: $a_1, a_2 \cdots a_n$ 
\begin{align*}
    A = \begin{bmatrix}
    a_1 & a_2 & \cdots & a_n
    \end{bmatrix}_{i}
\end{align*}


By definition of Linear Independence:
\[c_1a_1 + c_2a_2 + \ldots + c_na_n = 0\]
and $c_i \neq 0$ for some $i$ \\

We can write this equivalently as:
\[\quad A\mathbf{c} = \mathbf{0}, \text{where } \mathbf{c} = \begin{bmatrix}
c_1 \\
c_2 \\
\vdots \\
c_n
\end{bmatrix}
\]
This equation means that $c$ is a vector that, when multiplied by $A$, results in the zero vector. This is the definition of a vector being in the null space of $A$.\\

If $A$ was invertible, then we could multiply both sides by its inverse:
\begin{align*}
    A^{-1} A\mathbf{c} = A^{-1} \mathbf{0} \\
    I\mathbf{c} = \mathbf{0} \\
    \mathbf{c} = \mathbf{0}
\end{align*}

BUT, by the definition of linear dependence, $c$ cannot be the zero vector. \\
So we have a contradiction. \\
So $A$ must be Linearly independent.

\section{Eigenvalues and Eigenvectors}

\begin{tcolorbox}
    Eigenvalue-vector pairing effectively reduces matrix to a single scalar ($\lambda$), along a particular path (provided by the vector).\\

    Eigenvectors represent directions in the data space, and eigenvalues indicate the magnitude of variance along these directions.\\


    The matrix describes a transformation / a journey / a movement, and that lambda value is the most direct route to replicate that journey (but it is specific / associated with that vector)
\end{tcolorbox}

Definition of an eigenvector-eigenvalue pairing:
\[Av = \lambda v\]
Where $v$ is an eigenvector, and $\lambda$ is an eigenvalue (a scalar)\\

I.e.

\begin{itemize}
    \item if you multiply the matrix by a specific vector (eigenvector) you get the same result as multiplying that same vector by a single number (eigenvalue).
    \item = a specific vector-scalar pairing that reduces the matrix to a single number (for a given vector)
    \item Think of the eigenvector as a projected movement in a specific direction, and the eigenvalue as the extent / magnitude / degree of that movement.
    \item we are reducing the matrix to a single scalar value (its essence), which it can be reduced to when transforming a specific vector.
    \item Alternatively, think of $A$ as some transformation of $v$: it projects $v$ from one place to another. \textbf{If $A$ is the journey, then the eigenvalue $\lambda$ is the most direct route; the essence of the transformation} (the eigen = the self)
\end{itemize}

For example:
\begin{align*}
    \underbrace{\begin{bmatrix}
        1 & 2 \\
        4 & 3
    \end{bmatrix}}_{A}
    \underbrace{\begin{bmatrix}
        1 \\
        2
    \end{bmatrix}}_{v}
    &= \underbrace{5}_{\lambda}
    \underbrace{\begin{bmatrix}
        1 \\
        2
    \end{bmatrix}}_{v}
\end{align*}

In this case, $v = \begin{bmatrix} 1 \\ 2 \end{bmatrix} \text{ is an eigenvector and } \lambda = 5 \text{ is an eigenvalue for } A = \begin{bmatrix} 1 & 2 \\ 4 & 3 \end{bmatrix}$\\

\textit{NB Chat GPT gave the below automatically when I fed it the above, which is interesting, it's presenting the data matrix A, a given vector, the eigen value, and the eigen vectors}

For example:
\begin{align*}
    \underbrace{\begin{bmatrix}
        1 & 2 \\
        4 & 3
    \end{bmatrix}}_{A}
    \underbrace{\begin{bmatrix}
        v_1 \\
        v_2
    \end{bmatrix}}_{v}
    &= 5
    \underbrace{\begin{bmatrix}
        \lambda_1 \\
        \lambda_2
    \end{bmatrix}}_{\lambda}
\end{align*}

\subsection{Conditions for existence of (non-trivial) Eigenvector/values}

Not every matrix can be decomposed into eigenvalue-vector pairs.The conditions for the existence of non-trivial eigenvectors and eigenvalues for a square matrix \( A \) are:

\begin{enumerate}
    \item The matrix \( A \) must be a \textbf{square} matrix. The existence of eigenvalues and eigenvectors is a fundamental property of square matrices. Everything comes from there??
    \item There must exist a \textbf{scalar \( \lambda \) such that   \(\det \mathbf{(\lambda I --- A)} = 0 \)}, where \( I \) is the identity matrix of the same dimension as \( A \). This is derived from the \textbf{characteristic equation \(\det(\lambda I --- A) = 0 \)}.
    \item For each eigenvalue \( \lambda \), there must exist a non-zero solution \( \mathbf{v} \) to the equation \( (A --- \lambda I)\mathbf{v} = 0 \).
    \item Eigenvectors must be non-zero. The zero vector is not considered an eigenvector.
    \item The algebraic multiplicity of an eigenvalue must be at least as great as its geometric multiplicity, which is the number of linearly independent eigenvectors corresponding to that eigenvalue.
\end{enumerate}



\subsubsection{Proof for finding eigenvalues}

Start with how we defined the eigenvector and eigenvalue:
\[Av = \lambda v \]
Simply rearrange: 
\[0 = \lambda v --- Av\]
Now, let's replace $\lambda$ with $\lambda I_n $ (this is the same): 
\[0 = \lambda I_n v --- Av \]
Which allows us to write: 
\begin{align*}
0 &= (\lambda I_n --- A)v\\
0 &= \underbrace{(\lambda_n I --- A)}_{\text{call this B} \mathbf{v}} \\
0 &= \mathbf{Bv}
\end{align*}

This is trivially satisfied by $v = 0$. \\
We're looking for some non-trivial solution; i.e. some non-zero $v$\\

But more importantly, this is \textit{precisely} the definition of \textbf{linear dependence for the columns of B}, where our current vector $v$ is the vector of constants $c_1, c_2, \dots, c_n$. \\

\textbf{If a matrix has linearly dependent columns, then its determinant is zero; if we want a nontrivial solution to the above (IS THIS THE DEFINITION / EXISTENCE OF EIGEN VECTORS?), we require:}

\[det(\lambda I_n --- A) = 0\]

\begin{tcolorbox}
    \[det(\lambda I_n --- A) = 0\]
    Same as
    \[det(A --- \lambda I_n) = 0\]

This is the eigenvalue equation. \\
It is a generic form of the characteristic equation of a matrix (IS IT?). \\
Eigenvalues are the values of \lambda that satisfy the equation. \\

\textit{where:
\begin{itemize}
    \item $A$ = square $n \times n$ matrix; whose eigenvalues we are interested in finding.
    \item $\lambda$ = (scalar) eigenvalue of matrix $A$; goal is to finv values of $\lambda$ that satisfy this equation.
    \item $I_n$ = identify matrix of same size as $A$ 
    \item \textbf{$\rightarrow \lambda I_n --- A)$ = matrix formed by subtracting the matrix $A$ from the scalar $\lambda$ multiplied by the identity matrix $I_n$}. The result is a matrix where the diagonal elements are $\lambda --- a_{ij}$ where $a_{ij}$ are diagonal elements of $A$. WHAT ABOUT NON-DIAGONAL ELEMENTS
    \item the determinant of that matrix
    \item set that determinant to 0 $\rightarrow$ becomes the characteristic equation. Solve to find eigenvalues of $A$.   
\end{itemize}}

\textbf{Essentially: looking for values of $\lambda$ that make the determinant of $( \lambda I_n --- A)$ equal to zero.}

\end{tcolorbox}

\subsubsection{Explanation}
The determinant of a square matrix is a scalar value that provides important information about the matrix, including whether its columns (or rows) are linearly independent. When a matrix has linearly dependent columns, its determinant is zero. \\

\begin{itemize}
    \item \textbf{Linear Dependence:} If the columns (or rows) of a matrix are linearly dependent, it means that at least one column (or row) can be expressed as a linear combination of the others. In other words, there exists a set of scalar coefficients, not all zero, such that a weighted sum of the columns (or rows) equals a zero vector. \[c_1v_1 + c_2v_2 + \cdots + c_nv_n = 0\]
    \item \textbf{Determinant and Linear Transformations:} The determinant can be interpreted as a scaling factor of the linear transformation represented by the matrix. If a matrix has linearly dependent columns, it means that the space it maps to is ``flattened'' in at least one dimension (since one column is a combination of others). This ``flattening'' means the matrix compresses the space into a lower dimension, resulting in a volume of zero in the original space.
    \item \textbf{Geometric Interpretation:} Geometrically, the determinant of a matrix can be thought of as the volume of the parallelepiped spanned by its columns (in the case of a 3x3 matrix) or rows. If columns are linearly dependent, this parallelepiped collapses into a lower-dimensional space (like a line or a plane), which has zero volume in the context of the original space.
    \item \textbf{Properties of the Determinant Function:} The determinant function has a property that if two columns (or rows) of a matrix are identical, or one column is a scalar multiple of another, the determinant is zero. This is a specific case of linear dependence. In a more general sense, any linear combination leading to a redundant or dependent column (or row) results in a zero determinant.
\end{itemize}

\subsection{Finding Eigenvalues/vectors}
\begin{tcolorbox}    
Start with condition: 
\[det(\lambda I_n --- A) = 0\]
\end{tcolorbox}

Applying this to our matrix:
\begin{align*}
\det \left( \lambda \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} --- \begin{bmatrix} 1 & 2 \\ 4 & 3 \end{bmatrix} \right) = 0
\end{align*}
Computing this yields the \textbf{characteristic polynomial}:
\[\lambda^2 --- 4\lambda --- 5 = 0\]
Giving us $\lambda = 5, \lambda = -1$.\\

These are the eigenvalues.\\

\begin{tcolorbox}
To find eigenvectors from these values:
\begin{itemize}
    \item For each eigenvalue \lambda you have found, substitute it back into the eigenvalue equation \[(A --- \lambda I)v = 0\] 
    \item where $A$ is the original matrix.
    \item $v$ is the associated eigenvector we are trying to solve for.
    \item Solve the linear system of equations for $v$
\end{itemize}
\end{tcolorbox}

\subsection{Eigenspace}
From Eigenvalues \rightarrow Eigenvectors. \\

There will be more than one eigenvector corresponding to a given eigenvalue $\lambda$. \\
\begin{tcolorbox}
The \textbf{eigenspace of $\lambda$, $E_\lambda$, is all vectors $v$ that satsify the condition $Av = \lambda v$}\\
\end{tcolorbox}

So, we find all vectors $v$ such that 

\begin{align*}
    \begin{bmatrix}
        1 & 2 \\
        4 & 3
    \end{bmatrix}
    \begin{bmatrix}
        v_1 \\
        v_2
    \end{bmatrix}
    &= 5
    \begin{bmatrix}
        v_1 \\
        v_2
    \end{bmatrix}
\end{align*}

Which is the span of 
\[ \begin{bmatrix}
    1 \\
    2
\end{bmatrix}\]
See below for guidance:
\subsubsection*{Step 1: Matrix Multiplication}
First, perform the matrix multiplication on the left side of the equation.
\begin{align*}
    \begin{bmatrix}
        1 & 2 \\
        4 & 3
    \end{bmatrix}
    \begin{bmatrix}
        v_1 \\
        v_2
    \end{bmatrix}
    &= \begin{bmatrix}
        1 \cdot v_1 + 2 \cdot v_2 \\
        4 \cdot v_1 + 3 \cdot v_2
    \end{bmatrix} \\
    &= \begin{bmatrix}
        v_1 + 2v_2 \\
        4v_1 + 3v_2
    \end{bmatrix}
\end{align*}

\subsubsection*{Step 2: Scalar Multiplication}
Next, perform the scalar multiplication on the right side of the equation.
\begin{align*}
    5
    \begin{bmatrix}
        v_1 \\
        v_2
    \end{bmatrix}
    &= \begin{bmatrix}
        5v_1 \\
        5v_2
    \end{bmatrix}
\end{align*}

\subsubsection*{Step 3: Setting Up the Equation}
Now, equate the results of the left and right side multiplications.
\begin{align*}
    \begin{bmatrix}
        v_1 + 2v_2 \\
        4v_1 + 3v_2
    \end{bmatrix}
    &= \begin{bmatrix}
        5v_1 \\
        5v_2
    \end{bmatrix}
\end{align*}

\subsubsection*{Step 4: Solving the System of Equations}
This results in a system of linear equations:
\begin{align*}
    v_1 + 2v_2 &= 5v_1 \\
    4v_1 + 3v_2 &= 5v_2
\end{align*}
These equations can be solved to find the values of \( v_1 \) and \( v_2 \).

\subsection{Putting it all together}

\[
\underbrace{\begin{bmatrix}
    1 & 2 \\
    4 & 3
\end{bmatrix}}_{A}
\underbrace{\begin{bmatrix}
    v_1 \\
    v_2
\end{bmatrix}}_{\mathbf{v}}
= \underbrace{5}_{\lambda} 
\underbrace{\begin{bmatrix}
    v_1 \\
    v_2
\end{bmatrix}}_{\mathbf{v}}
\]

We have found one eigenvector-eigenvalue combination for the matrix \( A \) (there was another for \( \lambda = -1 \)).\\

see Khan academi links p23

\section{Eigen decomposition}

\subsection{Definition}

We can use this eigenvalue-vector pairings to find the \textbf{eigendecomposition} of $A$

\[A = Q\Lambda Q^{-1}\]
Where:
\begin{itemize}
    \item \( A \) is a square matrix.
    \item $\Lambda$ is a diagonal matrix with eigenvalues of $A$ on the main diagonal (and 0 everywhere else). \textit{(An $n \times n$ square will have $n$ eigenvalues, so $\Lambda$ will also be $n \times n$)}
    \item $Q$ is a matrix where the columns are eigenvectors of $A$, correspeonding to the eigenvalues in $\Lambda$ 
\end{itemize}


\subsection{The conditions for eigendecomposition}

\begin{enumerate}
\item The matrix \( A \) must be \textbf{square ($n \times n$}.
\item The matrix must \textbf{have \( n \) linearly independent eigenvectors}, where \( n \) is the size of the matrix.
\item The matrix \( A \) is diagonalizable if it is similar to a diagonal matrix \( \Lambda \), i.e., there exists an invertible matrix \( Q \) such that \( Q^{-1}AQ = \Lambda \). This is possible if and only if \( A \) has \( n \) independent eigenvectors. (this comes from having $n$ independent eigenvectors condition above.)
\end{enumerate}

\subsection{Calculation}
Taking our matrix \( A \) from before, we write:
\begin{align*}
A &= 
\underbrace{\begin{bmatrix}
    1 & -1 \\
    2 & 1 
\end{bmatrix}}_{Q}
\underbrace{\begin{bmatrix}
    5 & 0 \\
    0 & -1 
\end{bmatrix}}_{\Lambda}
\underbrace{\begin{bmatrix}
    1 & -1 \\
    2 & 1 
\end{bmatrix}^{-1}}_{Q^{-1}} \\
\end{align*}

\begin{itemize}
    \item recalling our solutions \( \lambda_1 = 5 \) and \( \lambda_2 = -1 \), 
    \item and the corresponding eigenvectors \( v_1 = 
        \begin{bmatrix}
            1 \\
            2 
        \end{bmatrix} \) (which we found above) 
    \item and \( v_2 = 
        \begin{bmatrix}
            -1 \\
            1 
        \end{bmatrix} \) 
\end{itemize}

\subsection{Why?}
Reducing a matrix to its essence is efficient for computing further transformations 

\section{Singular Value Decomposition of a Matrix}
\subsection{SVD Definition}

Eigendecomposition only works for square matrices. \\
SVD allows us to get a matrix of any dimension to its essence.

\begin{tcolorbox}
\[A = U \Sigma V^T\]
where \(A \in \mathbb{R}^{m \times n}\)
\begin{itemize}
    \item $U$ is a matrix containing left singular vectors of A (contains the $m$ eigenvectors of the square matrix $AA^T$) ($n \times n$)
    \item $V^T$ is a matrix containing the right singular vectors of A (contains the $n$ eigenvectors of the square matrix $A^T A$ ($m \times n$)
    \item $\Sigma$ is a matrix where the diagonal entries are the singular values of $A$ (square roots of the eigenvalues of $A^TA$ (or $AA^T$) on the diagonal. ($n \times n$)
\end{itemize}
\end{tcolorbox}

\begin{tcolorbox}
\textbf{Dimensions:}\\

Matrix $A$: Suppose $A$ is an $m \times n$ matrix.

Matrix $U$: $U$ is an $m \times m$ orthogonal matrix. The columns of $U$ are the left singular vectors of $A$.

Matrix $\Sigma$: $\Sigma$ is an $m \times n$ diagonal matrix. The non-zero elements of $\Sigma$ (which are on the diagonal) are the singular values of $A$. If $m > n$, the additional rows will be filled with zeros. If $n > m$, there will be additional zero columns.

Matrix $V^T$: $V^T$ is the transpose of an $n \times n$ orthogonal matrix $V$. The columns of $V$ (or the rows of $V^T$) are the right singular vectors of $A$.

So, in the SVD $A = U\Sigma V^T$:
\begin{itemize}
    \item $U$ aligns with the row dimension of $A$.
    \item $\Sigma$ matches the overall dimensions of $A$, but is diagonal.
    \item $V^T$ aligns with the column dimension of $A$.
\end{itemize}
\end{tcolorbox}

Think of $AA^T$ or $A^TA$ as ``squaring'' the matrix in two ways: 1) it is like $A^2$ in some ways, 2) it crucially transforms a non-square matrix into a square, so it can be decomposed.\\

This is why we use singular values of $A$ in $Sigma$'s diagonal: singular values are square roots of the eigenvalues of the ``squared $A$'' matrix ($AA^T$ or $A^TA$)

\subsection{SVD Is Profoundly Informative About the Structure and Dimensionality of Your Data}
\begin{itemize}
    \item SVD like an an x-ray of a matrix:
    \begin{itemize}
        \item \textbf{$U$ and $V^T$ represent how much it rotates an object} when multiplied by it, as they contain the singular vectors (like eigenvectors, but for non-square matrices)
        \item \textbf{$\Sigma$ represents associated scaling}, as it contains the associated singular values across its diagonal. (NB remember, these are the square roots of the A's eigenvalues)
    \end{itemize}
    \item Large singular values correspond to large ``strength'' of the transformation that $A$ makes along the subspace created by the associated left and right singular vectors. 
    \begin{itemize}
        \item NB: a 0 = redundant
        \item the \textbf{number of non-zero singular values gives the rank of the matrix, or the number of linearly independent columns}
        \item if \textbf{many singular values are zero (or very small), it suggests redundancy} in the data: data can be represented in a lower dimensional space without much loss of info
    \end{itemize}
\end{itemize}

% --- Lab Section ---
\section{Lab: PCA as Eigendecomposition}

\thispagestyle{empty}
{\large \textbf{Mid Terms Fall 2023  --  Henry Baker}}
\par\noindent\rule{\textwidth}{0.4pt} 

\vspace*{0.3cm} 
\begin{center} 
	{\Large \bf M4DS Finals Revision: Session 9 \\ Linear Algebra II \\ Lab: Principle Component Analysis}
	\vspace{2mm}
	
\end{center}  
\vspace{0.4cm}

\section{Set Up: PCA applied to image compression}
\begin{itemize}
    \item PCA = dimension reduction technique. Efficiently summarizes and describes large datasets.
    \item PCA has large variety of applications in genomics, facial recognition, computervision, finance  and econ, climate science, social science, etc.
    \item PCA foundations: \begin{itemize}
        \item maximisation variance 
        \item eigendecomposition
    \end{itemize}
\end{itemize}

\section{PCA as a Variance-Maximisation Problem}

\subsection{As variance in a matrix}

\begin{itemize}
    \item Take a data matrix $X$ ($n \times p$) --- centered around the mean (i.e. demeaned).
    \item Challenge: \textbf{how to convey as much info about $X$ as possible in only 1 column} of data (one $n$ lengthed vector)? What vector to choose?
    \begin{itemize}
        \item suppose $X$ contains one column that was pretty similar across observations (e.g. everyone's a Hertie student)
        \item and another that was q different (e.g. country of origin). 
        \item \textbf{we would want to keep the column with \textit{higher variance}}.
    \end{itemize}
    \item Now, rather than keeping one column from $X$, \textbf{we can take a column z$_1$ that is a \textit{linear combination} of all the columns of $x$.} 
    \begin{itemize}
        \item A linear combination is an expression constructed from a set of terms, by multiplying each term by a constant and adding the results. \textbf{It's a way of combining a set of vectors by scaling and adding them together}.
        \item Each column/vector is multipled by a corresponding coefficient $\phi$, after which the modified columns are added together, to form a new column/vector.
    \end{itemize}
    Here, this means that:
    $$\text{z}_1 = \phi_{11}x_1 + \phi_{21}x_2 + \ldots + \phi_{p1}x_p$$
    where:
    \begin{itemize} \begin{itemize}

        \item \( x_1, x_2, \ldots, x_p \) are the columns of the matrix \( X \). \textbf{They are vectors}
        \item \( \phi_{11}, \phi_{21}, \ldots, \phi_{p1} \) are the coefficients (the scales by which we multiply each column to be added). \textbf{They are scalar values}
        \item The product \( \phi_{11}x_1, \phi_{21}x_2, \ldots \) scales each column of \( X \) by its respective coefficient. \textbf{They are vectors}
        \item The sum of these products, \( \phi_{11}x_1 + \phi_{21}x_2 + \ldots + \phi_{p1}x_p \), creates the new vector \( z_1 \).  \textbf{It is a new vector \textit{- adding vectors together is performed element-wise}}  
    \end{itemize} \\

    
        \item \textbf{z$_1$ is formed by 1) multiplying each column in $X$ by its corresponding coefficient $\phi$, then 2) each scaled column is added together}.
        \item Another way of visualising this is :
                \begin{align*}
                \text{z}_{11} &= \phi_{11}x_{11} + \phi_{21}x_{12} + \ldots + \phi_{p1}x_{1p} \\
                \text{z}_{21} &= \phi_{11}x_{21} + \phi_{21}x_{22} + \ldots + \phi_{p1}x_{2p} \\
                &\vdots \\
                \text{z}_{n1} &= \phi_{11}x_{n1} + \phi_{n1}x_{n2} + \ldots + \phi_{n1}x_{np} \\
                \end{align*}
                Where \begin{itemize}
                    \item each $\phi$ is a constant.
                    \item z$_i$ is just a vector containing all the $z_{ik}$ values. In this sense \textbf{it stores data from all of the individual $x_k$ elements of the original matrix $X$}
                \end{itemize}
                \item This generates a new vector z$_1$ because each $x_i$ is a vector, and when you multiply a vector by a scalar $\phi$ the result is a vector.
                
                $$\textbf{z}_1 = \begin{bmatrix}
                    z_{11} \\
                    z_{21} \\
                    \vdots \\
                    z_{n1}
                \end{bmatrix}$$
                Here $z{_1}$ is a column vector \textbf{where each element is a linear combination of the columns of the matrix $X$ based on the PCA loadings ($\phi$)}.
                \item Where each element of z$_1$: $z_{i1}$  (where $i = 1, 2 \cdots n$ corresponds to the rows of the data), as: 
                $$z_{i1} = \phi_{11}x_{i1} + \phi_{21}x_{i2} + \ldots + \phi_{p1}x_{ip}$$
                \end{itemize}

        \end{itemize}

    \begin{figure}[H]
        \centering
        \includegraphics[width=1\linewidth]{images/week_9/Screenshot 2024-01-26 181222.png}
        \caption{Enter Caption}
        \label{fig:enter-label}
    \end{figure}

\subsection{As a maximisation problem}
    \begin{itemize}
    \item Which linear combination would you choose? The \textbf{linear combination (ie the values of $\phi_{11}, \phi_{21}, \cdots, \phi_{p1}$) that maximises the variance of your z$_1$ vector}. 
    \begin{itemize}
        \item by choosing different coefficients of $\phi$, you can emphasize certain features over others
        \item so for PCA: we are trying to find/choose coefficients $\phi$ that results in greatest variance across z$_1$ vector. 
        \item it in effect takes a bit of each column [develop this explanation].
        \item or, if you purpose was feature engineering, you can uncover hidden structures in the data, such that z$_1$ reveals patterns or relationships that aren't obvious in the original dataset.
    \end{itemize}
\end{itemize}

\begin{tcolorbox}
    
That is, you \textbf{solve the maximisation problem:}
\begin{align*}
    \max_{\phi_{11}, \phi_{21}, \ldots, \phi_{p1}} \text{Var}(\text{z}_1) &= \max_{\phi_{11}, \phi_{21}, \ldots, \phi_{p1}} \frac{1}{n} \sum_{i=1}^{n} (z_{i1} --- \bar{\text{z}}_1)^2 \\
    &= \max_{\phi_{11}, \phi_{21}, \ldots, \phi_{p1}} \frac{1}{n} \sum_{i=1}^{n} z_{i1}^2
\end{align*}
\end{tcolorbox}

Where: \begin{itemize}
    \item first line is the definition of variance: $\bar{\text{z}}_1$ is the column mean of z$_1$.
    \item second line follows from the fact that we demeaned our columns such that $\bar{\text{z}}_1 = 0$
\end{itemize}
\begin{tcolorbox}
    So variance of z$_1$ (the PCA vector) can be calculated simply as the avg of the squared elements of  $z_{i1}$ (the elements of the PCA vector, which are themselves linear combination of the columns of the matrix $X$ based on the PCA loadings)
\end{tcolorbox}

\subsection{Resulting components}
Principal components are new, uncorrelated variables that are linear combinations of the original variables. They are aligned with the directions of maximum variance in the dataset.\\

The maximal-variance z$_1$ is called the \textbf{first principle component}.\\
We call $\phi_1 = \begin{bmatrix}
    \phi_{11} \\
    \phi_{21} \\
    \vdots \\
    \phi_{p1}
\end{bmatrix}$ its loadings.\\ 

We \textbf{constrain the loadings so that their sum of squares is equal to one}, since allowing these elements to be arbitrarily large could result in an arbitrarily large variance: 
\[\sum_{j=1}^{p} \phi_{j1}^2 = 1\]

For constrained optimisation, see next session 10. But instead, here we will solve the same maximisation problem using eigendecomposition.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{images/week_9/image1.png}
    \caption{Enter Caption}
    \label{fig:enter-label}
\end{figure}

\section{PCA as Eigendecomposition of the Variance-Covariance Matrix}

\textit{NB: this equivalently solves the above maximisation problem. Unlike regression application from Wk 8 which used matrix structure and multiplication properties to effectively reengineer the same calculation, this fundamentally leverages an alternate set of properties of eigendecompositions to fundamentally approach the problem from a different angle. Namely that eigendecomposition gives you all the linearly independent vectors within/UNDERLYING? a matrix (IS THIS RIGHT? DO YOU GET P EIGEN VECTORS / LINEARLY INDEPENDENT VECTORS IF YOU HAVE A PXP MATRIX, AS EACH VECTOR HAS TO BE LINEARLY INDEPENDENT, OTHERWISE DETERMINANT COLLAPSES TO 0 AND YOU CANT DO EIGEN DECOMPOSITION??? OR IS IT JUST THAT EIGEN DECOMPOSITION EXTRACTS THE UNDERLYING LINEARLY INDEPENDENT VECTORS / TRANSFORMATIONS / MOVEMENTS OF ANY GIVEN DATA SET, WHICH JUST HAPPENS TO BE THE NATURE OF WHAT A PRINCIPLE COMPONENT IS?}\\

Chat GPT: 
\begin{itemize}
    \item Eigenvectors associated with different eigenvalues are linearly independent. This is a fundamental property of eigenvectors. If a $p \times p$ has $p$ distinct eigenvalues, it will have $p$ linearly independent eigenvectors.
    \item However, if some eigenvalues are repeated (i.e., not all eigenvalues are distinct), the matrix may have fewer than $p$ linearly independent eigenvectors. The matrix is then said to be ``defective,'' and not every defective matrix has a complete basis of eigenvectors.
\end{itemize}
\begin{itemize}
    \item Eigendecomposition of a matrix involves finding eigenvalues and their corresponding eigenvectors. If a \( p \times p \) matrix has \( p \) distinct eigenvalues, it will have \( p \) linearly independent eigenvectors.
    
    \item In PCA, eigendecomposition is used to identify principal components, which are orthogonal and linearly independent directions in the dataset that maximize variance.
    
    \item The determinant of a matrix being zero indicates that the matrix does not have an inverse and is not full rank. A zero determinant is related to the presence of an eigenvalue of zero, but a zero eigenvalue does not necessarily mean the absence of a full set of linearly independent eigenvectors.
    
    \item PCA leverages eigendecomposition to extract linearly independent directions (principal components) that capture the most variance in the data, making it powerful for dimensionality reduction and feature extraction.
\end{itemize}



\subsection{Derive a Variance-Covariance Matrix of $X$}
\begin{tcolorbox}
Variance-covariance matrix of centered matrix $X$, is given by:
\begin{align*}
    S &= \frac{1}{n} \sum_{i=1}^{n} (\mathbf{x}_i --- \bar{\mathbf{x}})(\mathbf{x}_i --- \bar{\mathbf{x}})^T \\
    &= \frac{1}{n} \sum_{i=1}^{n} \mathbf{x}_i \mathbf{x}_i^T
\end{align*}

Where:
\begin{itemize}
    \item $x_i$ is a $p$-length vector of the data to row $i$, 
    \item $\bar{x}$ is a $p$-length vector of the means of the data taken over $i = 1$ to $n$ (which all equal to 0 for centered data, hence second line)
\end{itemize}

Think of each column vector and a transpose vector being multiplied together to collapse into a single scalar value (the covariance between the two columns). This is a single element of the new matrix. So it contains a lot of info! Think of it as a particular form of collapsing of information...

\end{tcolorbox}

Basically, for each row-observation and extracts the squared differences (ie the variation) for each column-variable, by averaging those differences (between $n$ observations, across the variable) to give the variation across each vector expressed in a new matrix (since a vector and a transpose vector multiply out to a matrix, the output is a matrix). \\

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{images/week_9/Screenshot 2024-01-26 181250.png}
    \caption{Enter Caption}
    \label{fig:enter-label}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{images/week_9/Screenshot 2024-01-26 184636.png}
    \caption{Enter Caption}
    \label{fig:enter-label}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{images/week_9/image.png}
    \caption{WHAT IS THIS????}
    \label{fig:enter-label}
\end{figure}

The \textbf{resultant matrix represents the covariance between pairs of variables} in a dataset. 
\begin{itemize}
    \item It expresses the variance of each column (along its diagonal)
    \item and the off-diagonal elements represent the covariance between two variables/columns
    \item formally: each element $s_ij$ of this matrix indicates the covariance between the $i$-th and $j$-th variable.
    \item for a dataset with $p$ variables, this is a $p \times p$ matrix
\end{itemize}, \\

\subsection{Eigendecomposition of Variance-Covariance Matrix}
\begin{tcolorbox}
    We perform eigendecomposition on this $p \times p$ matrix:
    \[S\phi_1 = \lambda_1\phi_1\]
\end{tcolorbox}

Where \begin{itemize}
    \item $\lambda_1$ = the largest eigenvalue 
    \item $\phi_1$ = the corresponding eigenvector --- it gives the loadings of the first principal component.
\end{itemize}

You would then solve: HOW? YOU WOULD SET THE DETERMINANT TO 0 TO GIVE THE CHARACTERISTIC POLYNOMIAL??? --> GIVES YOU THE EIGENVALUES --> FROM THERE PLUG BACK IN TO GET ASSOCIATED EIGENVECTORS???\\

As before, the first principal component is given by:
\begin{align*}
    z_1 &= \phi_{11}x_1 + \phi_{21}x_2 + \ldots + \phi_{p1}x_p \\
    &= X\phi_1
\end{align*}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{images/week_9/Screenshot 2024-01-26 182019.png}
    \caption{Enter Caption}
    \label{fig:enter-label}
\end{figure}

Again, remember that each $x_i$ is a vector of $n$ length representing a variable-column from $X$\\
So adding all these vectors element-wise gives us z$_1$ as a vector.\\
ABOVE EACH X WAS A P LENGTH VECTOR, BUT HERE THEY ARE N LENGTH???z\\

Similarly, multiplying a matrix $X$ by a vector $\phi_1$ will produce a vector z$_1$.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{images/week_9/Screenshot 2024-01-26 182601.png}
    \caption{Enter Caption}
    \label{fig:enter-label}
\end{figure}

WHAT DOES THIS REPRESENT --- IT SEEMS TO BE THAT THE PRINCIPAL COMPONENT VECTOR MULTIPLIED BY THE ASSOCIATED LOADINGS VECTOR PRODUCES AN NXP MATRIX\ldots{} BUT I THOUGHT THE PRINCIPAL COMPONENT VECTOR WAS A SUM OF THE PRODUCTS OF THE LOADINGS AND VARIABLES\ldots{} SO THE LOADINGS INFO IS EMBEDDED IN THE PRINCIPLE COMPONENT, WHY WOULD WE WANT TO MUTLIPLY THEM TOGHETER?\\
IS THIS THE WAY TO 'EXPAND OUT THE PRINCIPLE COMPONENTS' IE TO MOVE BACK FROM PRINCIPLE COMPONENT VECTORS TO THE ORIGINAL DATA (OR AT LEAST REPRESENT THE HIGHEST VARIANCE VERSIONS OF IT) ?\\
THIS WOULD MAKE SENSE: THE IF YOU HAVE THE 1) LOADINGS, 2) PRINCIPLE COMPONENT --> YOU SHOULD BE ABLE TO INFER BACK TO THE ORIGINAL DATA 


\subsection{Beyond the First Principal Component}
\begin{itemize}
    \item Next, we should choose the linear combination of the columns of $X$ that has maximal variance out of all the linear combinations that are \textit{uncorrelated} with the first principal component Z$_1$.
    \item This ensures that your next column captures the most of the remaining variance in the data without duplicating information you already have
    \item this is the same as taking the eigenvector corresponding to the second-largest eigenvalue of the variance-covariance matrix.
\end{itemize}

\begin{align*}
    S\phi_2 &= \lambda_2\phi_2  \rightarrow z_2 = X\phi_2 \\
    S\phi_3 &= \lambda_3\phi_3  \rightarrow z_3 = X\phi_3 \\
    \vdots 
\end{align*}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{images/week_9/image2.png}
    \caption{Enter Caption}
    \label{fig:enter-label}
\end{figure}

\subsection{Summary steps}
\begin{tcolorbox}
Taking the first principle component = data matrix multiplied by eigenvector corresponding to largest eigenvalue of the variance-covariance matrix of X
    \begin{enumerate}
        \item ?????
    \end{enumerate}
\end{tcolorbox}

\section{Interpretation}
First principal component is a projection of our $n \times p$ matrix onto one dimention (i.e. into one $n$-length vector.\\

This is the most efficient way to store our data matrix in one vector: we have max'd variation captured.

\section{How this works}
Eigenvectors represent directions in the data space, and eigenvalues indicate the magnitude of variance along these directions.

Here $\phi$ is a vector that, when multiplied by $S$, changes only by a scalar factor $\lambda$.\\

Here eigendecomposition identifies the principal components of the data (ie new, uncorrelated variables, that are linear combinations of the original variables). They are alignes with the directions of maximum variance in the dataset.\\

The eigenvector associated with the largest eigenvalue points in the direction of greatest variance; i.e. z$_1$.\\

The second largest eigenvector is orthogonal to the first, and points in the direction of the second greatest variance. etc.



\section{Real World applications}

By performing eigendecomposition, you can understand the underlying structure of the data, reduce noise, and simplify the dataset while retaining most of the important information.\\

In many real-world applications, this leads to better performance in machine learning models, as it reduces overfitting and computational cost.





\section{lab 10: More PCA}

Reconstituting data

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{images/week_9/reconstitute1.png}
    \caption{Enter Caption}
    \label{fig:enter-label}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{images/week_9/reconstitute2.png}
    \caption{Enter Caption}
    \label{fig:enter-label}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{images/week_9/reconstitute3.png}
    \caption{Enter Caption}
    \label{fig:enter-label}
\end{figure}
