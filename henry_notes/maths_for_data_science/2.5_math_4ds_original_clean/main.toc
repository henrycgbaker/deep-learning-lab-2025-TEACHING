\contentsline {part}{I\hspace {1em}Probability Theory}{6}{part.1}%
\contentsline {chapter}{\numberline {1}Probability Theory}{7}{chapter.1}%
\contentsline {chapter}{\numberline {2}Conditional Probability and Random Variables}{10}{chapter.2}%
\contentsline {section}{\numberline {2.1}Conditional Probability}{10}{section.2.1}%
\contentsline {section}{\numberline {2.2}Bayes Rule (w/LOTP combo)}{11}{section.2.2}%
\contentsline {section}{\numberline {2.3}Independence of Events}{12}{section.2.3}%
\contentsline {section}{\numberline {2.4}Random Variables}{13}{section.2.4}%
\contentsline {section}{\numberline {2.5}Common Distributions \& their PMFs}{14}{section.2.5}%
\contentsline {section}{\numberline {2.6}Common Distributions \& their CDFs}{15}{section.2.6}%
\contentsline {chapter}{\numberline {3}Joint Random Variables}{16}{chapter.3}%
\contentsline {section}{\numberline {3.1}(Joint) Random Variables and their Distributions}{16}{section.3.1}%
\contentsline {section}{\numberline {3.2}Expectation}{17}{section.3.2}%
\contentsline {section}{\numberline {3.3}Variance}{18}{section.3.3}%
\contentsline {section}{\numberline {3.4}Joint Distributions --- how 2 r.v.s interact}{18}{section.3.4}%
\contentsline {subsection}{\numberline {3.4.1}PMF}{18}{subsection.3.4.1}%
\contentsline {subsection}{\numberline {3.4.2}CDF}{19}{subsection.3.4.2}%
\contentsline {subsection}{\numberline {3.4.3}Marginal PMF of X}{19}{subsection.3.4.3}%
\contentsline {subsection}{\numberline {3.4.4}Conditional PMF of Y}{20}{subsection.3.4.4}%
\contentsline {part}{II\hspace {1em}Calculus}{24}{part.2}%
\contentsline {chapter}{\numberline {4}Calculus I}{25}{chapter.4}%
\contentsline {section}{\numberline {4.1}Differentiation Rules}{25}{section.4.1}%
\contentsline {section}{\numberline {4.2}Power Series}{27}{section.4.2}%
\contentsline {section}{\numberline {4.3}Maximum Likelihood Estimation (MLE)}{27}{section.4.3}%
\contentsline {subsection}{\numberline {4.3.1}Theoretical Overview}{28}{subsection.4.3.1}%
\contentsline {subsection}{\numberline {4.3.2}Steps for Maximum Likelihood Estimation}{28}{subsection.4.3.2}%
\contentsline {subsection}{\numberline {4.3.3}Worked Example: Direct Approach (Without Logging)}{28}{subsection.4.3.3}%
\contentsline {subsubsection}{Express likelihood function}{28}{section*.5}%
\contentsline {paragraph}{Specify the model}{28}{section*.6}%
\contentsline {paragraph}{Write the PMF of $X_i$}{29}{section*.7}%
\contentsline {paragraph}{Generic PMF}{29}{section*.8}%
\contentsline {paragraph}{Probabilities of each $X_i$}{29}{section*.9}%
\contentsline {paragraph}{Joint probabilities of all $X_i$}{29}{section*.10}%
\contentsline {subsubsection}{Maximise $\theta $}{29}{section*.11}%
\contentsline {subsection}{\numberline {4.3.4}Log-Likelihood Approach}{30}{subsection.4.3.4}%
\contentsline {paragraph}{Take the log}{30}{section*.12}%
\contentsline {paragraph}{Generic likelihood given $n$ events with $m$ trials}{30}{section*.13}%
\contentsline {paragraph}{Log-likelihood}{30}{section*.14}%
\contentsline {paragraph}{Derive the log}{30}{section*.15}%
\contentsline {paragraph}{Set derivative to 0, solve for $\theta $}{31}{section*.16}%
\contentsline {subsection}{\numberline {4.3.5}Practice Exercises}{31}{subsection.4.3.5}%
\contentsline {subsection}{\numberline {4.3.6}Summary: What is MLE?}{32}{subsection.4.3.6}%
\contentsline {chapter}{\numberline {5}Power Series}{33}{chapter.5}%
\contentsline {section}{\numberline {5.1}Taylor Series Approximation}{33}{section.5.1}%
\contentsline {subsection}{\numberline {5.1.1}First Condition $p(0) = f(0)$}{34}{subsection.5.1.1}%
\contentsline {subsection}{\numberline {5.1.2}2nd Condition $p'(0) = f'(0)$}{34}{subsection.5.1.2}%
\contentsline {subsection}{\numberline {5.1.3}Third Condition: $p''(0) = f''(0)$}{34}{subsection.5.1.3}%
\contentsline {subsection}{\numberline {5.1.4}Fourth Condition: $p'''(0) = f'''(0)$}{34}{subsection.5.1.4}%
\contentsline {subsection}{\numberline {5.1.5}Putting it Together}{35}{subsection.5.1.5}%
\contentsline {subsection}{\numberline {5.1.6}Example:}{35}{subsection.5.1.6}%
\contentsline {section}{\numberline {5.2}Integration}{36}{section.5.2}%
\contentsline {chapter}{\numberline {6}Continuous Random Variables I}{37}{chapter.6}%
\contentsline {section}{\numberline {6.1}Continuous r.v.s: relationship between PDF-CDF}{37}{section.6.1}%
\contentsline {subsection}{\numberline {6.1.1}an r.v. has a continuous distribution if its CDF is differentiable}{37}{subsection.6.1.1}%
\contentsline {subsection}{\numberline {6.1.2}PDF of $X$ is the Derivative of the CDF}{37}{subsection.6.1.2}%
\contentsline {subsection}{\numberline {6.1.3}Probability of a Continuous Random Variable}{38}{subsection.6.1.3}%
\contentsline {subsection}{\numberline {6.1.4}Valid PDFs}{39}{subsection.6.1.4}%
\contentsline {section}{\numberline {6.2}Expectation of a Continuous r.v}{39}{section.6.2}%
\contentsline {section}{\numberline {6.3}E.g. 1: Uniform Distribution, Continuous}{40}{section.6.3}%
\contentsline {subsection}{\numberline {6.3.1}PDF}{40}{subsection.6.3.1}%
\contentsline {subsection}{\numberline {6.3.2}CDF}{40}{subsection.6.3.2}%
\contentsline {subsection}{\numberline {6.3.3}Mean}{41}{subsection.6.3.3}%
\contentsline {section}{\numberline {6.4}E.g.2: Normal Distribution}{41}{section.6.4}%
\contentsline {subsection}{\numberline {6.4.1}PDF}{41}{subsection.6.4.1}%
\contentsline {subsection}{\numberline {6.4.2}CDF}{41}{subsection.6.4.2}%
\contentsline {subsection}{\numberline {6.4.3}Parameters}{42}{subsection.6.4.3}%
\contentsline {subsection}{\numberline {6.4.4}Standardization}{43}{subsection.6.4.4}%
\contentsline {subsection}{\numberline {6.4.5}Features of the Normal}{44}{subsection.6.4.5}%
\contentsline {subsection}{\numberline {6.4.6}Benchmarks of the Normal}{44}{subsection.6.4.6}%
\contentsline {section}{\numberline {6.5}E.g. 3: Exponential}{45}{section.6.5}%
\contentsline {subsection}{\numberline {6.5.1}PDF}{45}{subsection.6.5.1}%
\contentsline {subsection}{\numberline {6.5.2}CDF}{45}{subsection.6.5.2}%
\contentsline {subsection}{\numberline {6.5.3}Modelling purpose}{45}{subsection.6.5.3}%
\contentsline {subsection}{\numberline {6.5.4}Features: memorylessness}{45}{subsection.6.5.4}%
\contentsline {section}{\numberline {6.6}Continuous variables applied to Probability}{46}{section.6.6}%
\contentsline {subsection}{\numberline {6.6.1}Joint Distribution of Continuous r.v.s}{46}{subsection.6.6.1}%
\contentsline {subsubsection}{CDF}{46}{section*.17}%
\contentsline {subsubsection}{PDF}{46}{section*.18}%
\contentsline {subsection}{\numberline {6.6.2}Marginal Distribution of Continuous r.v.s}{48}{subsection.6.6.2}%
\contentsline {subsection}{\numberline {6.6.3}Conditional PDF}{48}{subsection.6.6.3}%
\contentsline {subsection}{\numberline {6.6.4}Bayes Rule and LOTP for continuous r.v.s}{49}{subsection.6.6.4}%
\contentsline {subsection}{\numberline {6.6.5}Can combine discreet and continuous r.v.s}{49}{subsection.6.6.5}%
\contentsline {chapter}{\numberline {7}Continuous Random Variables II}{50}{chapter.7}%
\contentsline {section}{\numberline {7.1}Covariance}{50}{section.7.1}%
\contentsline {subsection}{\numberline {7.1.1}Covariance Definition}{50}{subsection.7.1.1}%
\contentsline {subsection}{\numberline {7.1.2}Some Covariance rules}{51}{subsection.7.1.2}%
\contentsline {section}{\numberline {7.2}Correlation}{51}{section.7.2}%
\contentsline {subsection}{\numberline {7.2.1}Correlation Imposes Linearity}{51}{subsection.7.2.1}%
\contentsline {subsection}{\numberline {7.2.2}Further Explanation: Proof of Cov > Corr}{53}{subsection.7.2.2}%
\contentsline {subsection}{\numberline {7.2.3}Example proving independence}{54}{subsection.7.2.3}%
\contentsline {section}{\numberline {7.3}Law of Large Numbers}{55}{section.7.3}%
\contentsline {section}{\numberline {7.4}Central Limit Theorem}{56}{section.7.4}%
\contentsline {subsection}{\numberline {7.4.1}Calculating the Standardised Sample Mean}{56}{subsection.7.4.1}%
\contentsline {subsection}{\numberline {7.4.2}Convergence to Standard Normal}{56}{subsection.7.4.2}%
\contentsline {subsection}{\numberline {7.4.3}Further Notes on CLT}{58}{subsection.7.4.3}%
\contentsline {subsection}{\numberline {7.4.4}Example: Normal Approximation to the Binomial}{58}{subsection.7.4.4}%
\contentsline {section}{\numberline {7.5}Lab: EM Algorithm}{59}{section.7.5}%
\contentsline {section}{\numberline {7.6}Context; Set up}{59}{section.7.6}%
\contentsline {section}{\numberline {7.7}Attempting MLE}{60}{section.7.7}%
\contentsline {subsection}{\numberline {7.7.1}MLE for $mu_1$}{60}{subsection.7.7.1}%
\contentsline {section}{\numberline {7.8}Bayes rule to the Rescue}{61}{section.7.8}%
\contentsline {subsection}{\numberline {7.8.1}Gammas}{61}{subsection.7.8.1}%
\contentsline {subsection}{\numberline {7.8.2}Back to the maximisation problem}{62}{subsection.7.8.2}%
\contentsline {section}{\numberline {7.9}The EM Algorithm}{63}{section.7.9}%
\contentsline {part}{III\hspace {1em}Linear Algebra}{64}{part.3}%
\contentsline {chapter}{\numberline {8}Linear Algebra I}{65}{chapter.8}%
\contentsline {section}{\numberline {8.1}Data Structures}{65}{section.8.1}%
\contentsline {subsection}{\numberline {8.1.1}Basics}{65}{subsection.8.1.1}%
\contentsline {subsection}{\numberline {8.1.2}Compact Notation}{66}{subsection.8.1.2}%
\contentsline {subsection}{\numberline {8.1.3}Transpose}{66}{subsection.8.1.3}%
\contentsline {section}{\numberline {8.2}Basic Transformations}{66}{section.8.2}%
\contentsline {subsection}{\numberline {8.2.1}Adding Matrices}{66}{subsection.8.2.1}%
\contentsline {subsection}{\numberline {8.2.2}Multiplying a Matrix by a Scalar}{66}{subsection.8.2.2}%
\contentsline {section}{\numberline {8.3}Multiplying Vectors}{66}{section.8.3}%
\contentsline {section}{\numberline {8.4}Multiplying Matrices}{67}{section.8.4}%
\contentsline {section}{\numberline {8.5}Mutliplying Matrices with Vectors}{69}{section.8.5}%
\contentsline {section}{\numberline {8.6}Transpose Facts}{70}{section.8.6}%
\contentsline {section}{\numberline {8.7}Systems of Equation}{70}{section.8.7}%
\contentsline {section}{\numberline {8.8}Identity Matrix}{71}{section.8.8}%
\contentsline {section}{\numberline {8.9}The Inverse of a Matrix}{71}{section.8.9}%
\contentsline {section}{\numberline {8.10}Vector Norm}{71}{section.8.10}%
\contentsline {section}{\numberline {8.11}Lab: Regression Using Matrix Algebra}{72}{section.8.11}%
\contentsline {section}{\numberline {8.12}Linear Regression}{72}{section.8.12}%
\contentsline {subsection}{\numberline {8.12.1}Set up}{72}{subsection.8.12.1}%
\contentsline {subsection}{\numberline {8.12.2}Objective Function for Least Squares}{72}{subsection.8.12.2}%
\contentsline {subsection}{\numberline {8.12.3}System of Equations = inefficient approach}{72}{subsection.8.12.3}%
\contentsline {subsection}{\numberline {8.12.4}Matrix approach = efficient}{73}{subsection.8.12.4}%
\contentsline {section}{\numberline {8.13}Penalised Regression}{74}{section.8.13}%
\contentsline {subsection}{\numberline {8.13.1}L1 Norm (Lasso Regression)}{74}{subsection.8.13.1}%
\contentsline {subsection}{\numberline {8.13.2}L2 Norm (Ridge Regression)}{75}{subsection.8.13.2}%
\contentsline {subsection}{\numberline {8.13.3}Summary}{75}{subsection.8.13.3}%
\contentsline {section}{\numberline {8.14}Bringing together the above expanded cost function with the L2 Norm constraint, to give the Ridge Regression objective function we try to minimise)}{75}{section.8.14}%
\contentsline {chapter}{\numberline {9}Linear Algebra II}{77}{chapter.9}%
\contentsline {section}{\numberline {9.1}Linear Dependence}{77}{section.9.1}%
\contentsline {subsection}{\numberline {9.1.1}Examples of proving linear (in)dependence}{78}{subsection.9.1.1}%
\contentsline {subsubsection}{E.g.1 Proving Linear Dependence: finding non-trivial solution}{79}{section*.19}%
\contentsline {subsubsection}{E.g.2 Proving Linear Independence: only a trivial solution}{80}{section*.20}%
\contentsline {subsubsection}{E.g.3 Proving Linear Dependence (Mixed Case)}{80}{section*.21}%
\contentsline {subsubsection}{Determining Linear Dependence or Independence}{81}{section*.22}%
\contentsline {section}{\numberline {9.2}The Span}{81}{section.9.2}%
\contentsline {subsection}{\numberline {9.2.1}in terms of Vector Space / Basis vectors}{81}{subsection.9.2.1}%
\contentsline {subsection}{\numberline {9.2.2}in terms of the Spanning Set}{81}{subsection.9.2.2}%
\contentsline {section}{\numberline {9.3}The Determinant}{83}{section.9.3}%
\contentsline {subsection}{\numberline {9.3.1}Uses}{83}{subsection.9.3.1}%
\contentsline {subsection}{\numberline {9.3.2}Calculation}{83}{subsection.9.3.2}%
\contentsline {section}{\numberline {9.4}Matrix Inverse}{84}{section.9.4}%
\contentsline {subsection}{\numberline {9.4.1}Adjugate Matrix}{85}{subsection.9.4.1}%
\contentsline {section}{\numberline {9.5}To know}{85}{section.9.5}%
\contentsline {subsection}{\numberline {9.5.1}Conditions under which a matrix is invertible:}{86}{subsection.9.5.1}%
\contentsline {subsubsection}{Proof by contradiction:}{86}{section*.23}%
\contentsline {section}{\numberline {9.6}Eigenvalues and Eigenvectors}{87}{section.9.6}%
\contentsline {subsection}{\numberline {9.6.1}Conditions for existence of (non-trivial) Eigenvector/values}{88}{subsection.9.6.1}%
\contentsline {subsubsection}{Proof for finding eigenvalues}{88}{section*.24}%
\contentsline {subsubsection}{Explanation}{89}{section*.25}%
\contentsline {subsection}{\numberline {9.6.2}Finding Eigenvalues/vectors}{90}{subsection.9.6.2}%
\contentsline {subsection}{\numberline {9.6.3}Eigenspace}{90}{subsection.9.6.3}%
\contentsline {subsection}{\numberline {9.6.4}Putting it all together}{91}{subsection.9.6.4}%
\contentsline {section}{\numberline {9.7}Eigen decomposition}{91}{section.9.7}%
\contentsline {subsection}{\numberline {9.7.1}Definition}{91}{subsection.9.7.1}%
\contentsline {subsection}{\numberline {9.7.2}The conditions for eigendecomposition}{92}{subsection.9.7.2}%
\contentsline {subsection}{\numberline {9.7.3}Calculation}{92}{subsection.9.7.3}%
\contentsline {subsection}{\numberline {9.7.4}Why?}{92}{subsection.9.7.4}%
\contentsline {section}{\numberline {9.8}Singular Value Decomposition of a Matrix}{92}{section.9.8}%
\contentsline {subsection}{\numberline {9.8.1}SVD Definition}{92}{subsection.9.8.1}%
\contentsline {subsection}{\numberline {9.8.2}SVD Is Profoundly Informative About the Structure and Dimensionality of Your Data}{93}{subsection.9.8.2}%
\contentsline {section}{\numberline {9.9}Lab: PCA as Eigendecomposition}{93}{section.9.9}%
\contentsline {section}{\numberline {9.10}Set Up: PCA applied to image compression}{94}{section.9.10}%
\contentsline {section}{\numberline {9.11}PCA as a Variance-Maximisation Problem}{94}{section.9.11}%
\contentsline {subsection}{\numberline {9.11.1}As variance in a matrix}{94}{subsection.9.11.1}%
\contentsline {subsection}{\numberline {9.11.2}As a maximisation problem}{95}{subsection.9.11.2}%
\contentsline {subsection}{\numberline {9.11.3}Resulting components}{96}{subsection.9.11.3}%
\contentsline {section}{\numberline {9.12}PCA as Eigendecomposition of the Variance-Covariance Matrix}{96}{section.9.12}%
\contentsline {subsection}{\numberline {9.12.1}Derive a Variance-Covariance Matrix of $X$}{97}{subsection.9.12.1}%
\contentsline {subsection}{\numberline {9.12.2}Eigendecomposition of Variance-Covariance Matrix}{99}{subsection.9.12.2}%
\contentsline {subsection}{\numberline {9.12.3}Beyond the First Principal Component}{101}{subsection.9.12.3}%
\contentsline {subsection}{\numberline {9.12.4}Summary steps}{102}{subsection.9.12.4}%
\contentsline {section}{\numberline {9.13}Interpretation}{102}{section.9.13}%
\contentsline {section}{\numberline {9.14}How this works}{102}{section.9.14}%
\contentsline {section}{\numberline {9.15}Real World applications}{103}{section.9.15}%
\contentsline {section}{\numberline {9.16}lab 10: More PCA}{103}{section.9.16}%
\contentsline {part}{IV\hspace {1em}Optimisation}{105}{part.4}%
\contentsline {chapter}{\numberline {10}Optimisation}{106}{chapter.10}%
\contentsline {section}{\numberline {10.1}Second Derivative Test --- determines if we've found a min / max}{106}{section.10.1}%
\contentsline {section}{\numberline {10.2}Constrained Optimization (in 2-variable setting)}{106}{section.10.2}%
\contentsline {section}{\numberline {10.3}Min or Max (for all multivariate calculus?)}{107}{section.10.3}%
\contentsline {section}{\numberline {10.4}Matrix Optimization: the Gradient}{107}{section.10.4}%
\contentsline {subsection}{\numberline {10.4.1}Eg $f$ takes some input in \( \mathbb {R}^{m} \text { (a vector)} \rightarrow \) outputs some real value \( \in \mathbb {R} \) (a scalar)}{108}{subsection.10.4.1}%
\contentsline {subsection}{\numberline {10.4.2}E.g. 2 \( f \) takes some input in $\mathbb {R}^{m \times n}$ (a matrix) $ \rightarrow $ outputs some real value $\in \mathbb {R}$ (a scalar}{108}{subsection.10.4.2}%
\contentsline {section}{\numberline {10.5}Constrained Optimization Using the Gradient}{108}{section.10.5}%
\contentsline {section}{\numberline {10.6}Optimization as Eigen decomposition}{109}{section.10.6}%
\contentsline {section}{\numberline {10.7}Manual Constrained Optimisation using Lagrange multiplier}{110}{section.10.7}%
\contentsline {subsection}{\numberline {10.7.1}Examples}{111}{subsection.10.7.1}%
\contentsline {subsection}{\numberline {10.7.2}Example from Final Review}{111}{subsection.10.7.2}%
\contentsline {subsubsection}{1) Set up the Lagrangian for your Optimisation Problem}{111}{section*.30}%
\contentsline {subsection}{\numberline {10.7.3}2) suppose you have the following variance-covariance matrix of the returns of your 2 assets, reduce your Lagrangian as much as possible}{112}{subsection.10.7.3}%
\contentsline {part}{V\hspace {1em}Appendices}{115}{part.5}%
\contentsline {chapter}{\numberline {A}Common Distributions}{116}{appendix.A}%
\contentsline {section}{\numberline {A.1}Hypergeometric}{122}{section.A.1}%
\contentsline {chapter}{\numberline {B}Cheat Sheet I}{123}{appendix.B}%
\contentsline {section}{\numberline {B.1}Session 1: Probability Theory}{123}{section.B.1}%
\contentsline {section}{\numberline {B.2}Session 2: Conditional Probability \& Random Variables}{124}{section.B.2}%
\contentsline {subsection}{\numberline {B.2.1}Conditional Probability}{124}{subsection.B.2.1}%
\contentsline {subsection}{\numberline {B.2.2}Random Variables}{126}{subsection.B.2.2}%
\contentsline {section}{\numberline {B.3}Session 3: Joint r.v.s}{127}{section.B.3}%
\contentsline {subsection}{\numberline {B.3.1}Independence of joint r.v.s}{127}{subsection.B.3.1}%
\contentsline {subsection}{\numberline {B.3.2}Expectation}{128}{subsection.B.3.2}%
\contentsline {subsection}{\numberline {B.3.3}Variance}{128}{subsection.B.3.3}%
\contentsline {subsection}{\numberline {B.3.4}Marginal \& Conditional Joint PMFs}{128}{subsection.B.3.4}%
\contentsline {section}{\numberline {B.4}Session 4: Calculus}{129}{section.B.4}%
\contentsline {section}{\numberline {B.5}MLE}{131}{section.B.5}%
\contentsline {section}{\numberline {B.6}Taylor Series Approximation}{132}{section.B.6}%
\contentsline {chapter}{\numberline {C}Cheat Sheet II}{133}{appendix.C}%
\contentsline {section}{\numberline {C.1}Wk 6 --- Continuour r.v.s meets probability}{133}{section.C.1}%
\contentsline {subsection}{\numberline {C.1.1}Continuous r.vs}{133}{subsection.C.1.1}%
\contentsline {subsection}{\numberline {C.1.2}expectation of continuous r.v}{133}{subsection.C.1.2}%
\contentsline {subsection}{\numberline {C.1.3}Uniform, continuous}{134}{subsection.C.1.3}%
\contentsline {subsection}{\numberline {C.1.4}Normal}{134}{subsection.C.1.4}%
\contentsline {subsection}{\numberline {C.1.5}Standardisation}{134}{subsection.C.1.5}%
\contentsline {subsection}{\numberline {C.1.6}Exponential}{134}{subsection.C.1.6}%
\contentsline {subsection}{\numberline {C.1.7}Joint Distributions of Continuous r.v.s}{134}{subsection.C.1.7}%
\contentsline {subsection}{\numberline {C.1.8}Bayes Rule and LOTP for continuous r.v.s}{135}{subsection.C.1.8}%
\contentsline {section}{\numberline {C.2}wk 7 --- Continuous R.vs II}{135}{section.C.2}%
\contentsline {subsection}{\numberline {C.2.1}Covariance}{135}{subsection.C.2.1}%
\contentsline {subsubsection}{Some Covariance rules}{135}{section*.42}%
\contentsline {subsection}{\numberline {C.2.2}Correlation}{135}{subsection.C.2.2}%
\contentsline {subsection}{\numberline {C.2.3}Law of Large Numbers: as $n$ grows large, the sample mean $\bar {X}$ converges to the true mean $\mu $}{136}{subsection.C.2.3}%
\contentsline {subsection}{\numberline {C.2.4}Central Limit Theorem = that the standardised sample mean (standardised $\bar {X}$) converges in distribution to the standard Normal as $n \rightarrow \infty $}{136}{subsection.C.2.4}%
\contentsline {subsection}{\numberline {C.2.5}Example: Normal Approximation to the Binomial}{136}{subsection.C.2.5}%
\contentsline {section}{\numberline {C.3}Lab 7 --- EM Algorithm}{137}{section.C.3}%
\contentsline {section}{\numberline {C.4}Wk 8 --- Matrix Algebra}{137}{section.C.4}%
\contentsline {section}{\numberline {C.5}Lab 8 --- Regression}{138}{section.C.5}%
\contentsline {subsection}{\numberline {C.5.1}Linear Regression}{138}{subsection.C.5.1}%
\contentsline {subsection}{\numberline {C.5.2}Penalised regression}{139}{subsection.C.5.2}%
\contentsline {section}{\numberline {C.6}Wk 9 --- Linear Algebra II}{139}{section.C.6}%
\contentsline {section}{\numberline {C.7}Lab 9 --- PCA}{140}{section.C.7}%
\contentsline {subsection}{\numberline {C.7.1}As Variance Max}{140}{subsection.C.7.1}%
\contentsline {subsection}{\numberline {C.7.2}As Eigendecomposition}{141}{subsection.C.7.2}%
\contentsline {section}{\numberline {C.8}Wk 10 --- Optimisation}{141}{section.C.8}%
