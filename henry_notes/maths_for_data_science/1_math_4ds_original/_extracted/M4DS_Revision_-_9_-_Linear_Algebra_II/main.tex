\documentclass[a4paper,11pt]{article} 

\usepackage[top = 2.5cm, bottom = 2.5cm, left = 2.5cm, right = 2.5cm]{geometry} 
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{multirow} 
\usepackage{booktabs}
\usepackage{graphicx} 
\usepackage{setspace}
\setlength{\parindent}{0in}
\usepackage{enumerate}
\usepackage{float}
\usepackage{fancyhdr}
\usepackage{amsmath}
\usepackage{tcolorbox}
\usepackage{graphicx}

\DeclareMathOperator*{\argmin}{arg\,min}


\pagestyle{fancy} 
\fancyhf{}
\lhead{\footnotesize M4DS Finals revision}
\rhead{\footnotesize Henry Baker} 
\cfoot{\footnotesize \thepage} 


\begin{document}


\thispagestyle{empty}
\begin{tabular}{p{15.5cm}} 
{\large \bf Mid Terms Fall 2023  \\ Henry Baker}
\hline 
\\
\end{tabular} 

\vspace*{0.3cm} 
\begin{center} 
	{\Large \bf M4DS Mid Terms Revision: Session 9 \\ Linear Algebra II}
	\vspace{2mm}
	
\end{center}  
\vspace{0.4cm}

\section{Linear Dependence}

Linear dependence means redundant information: at least one of the vectors can be expressed as a linear combination of the others. \\

A set of vectors is linearly independent if and only if there is \textbf{no} redundancy in the information they provide.\\

If even one row (or column) of a matrix is linearly dependent on others, then the entire set of rows (or columns) is considered linearly dependent. For a matrix to be completely linearly independent, every single row (or column) must be linearly independent of the others.\\

For square matrices, another way to think about this is in terms of the determinant. A square matrix is linearly independent (and hence invertible) if and only if its determinant is non-zero. If even one row or column is linearly dependent, the determinant of the matrix will be zero, indicating linear dependence.\\

\begin{tcolorbox}
\textbf{TL;DR}: Linear Dependence: there exists a set of scalar coefficients, not all zero, such that a weighted sum of the columns (or rows) equals a zero vector 
\[c_1v_1 + c_2v_2 + \cdots + c_nv_n = 0\]
\end{tcolorbox}

\begin{itemize}
    \item Let the sets $S = {v_1, v_2, \cdots, v_n} $ be a set of vectors.
    \item Members of the set $s$ are \textbf{linearly dependent if at least one of the members of the set can be written as a linear combination of the other members.}
    \item i.e. if no scalar multiple of one vector can be added to a scalar multiple of the other to produce the zero vector, unless both scalars are zero. 
    \item In simpler terms, one vector cannot be expressed as a multiple of the other.
    \item formally, Linear Dependence: if and only if
    \[c_1v_1 + c_2v_2 + \cdots + c_nv_n = 0\]
    \textit{where at least one of the $c_i$s is non-zero (non-trivial solution)}
    \item intuitively: \begin{itemize}
        \item for Linear Dependence, although it looks like we are constraining oursleves by setting the condition "= 0", we are in fact not. We can just throw in some given/arbitrary scalar values for $c$ that make this true, i.e. we can simply just rescale some of the vectors such that they cancel out other (and thus reach 0). This is because they are ultimately acting on the same dimensional plane; you can rescale them to express the movement of the other(s). \textbf{Thus, by saying that some rescaled combination of vectors come to 0, we are saying that they are acting on / moving across / spanning the same dimensions; this is thus redundant information, and thus the columns are linearly dependent.}
        \item Whereas, for Linear Independence, we are in fact \textbf{more constrained: there is NO way to just scale vectors such that the above equation is true, in that they don't operate on the same dimensions, so can't just be rescaled to counter the movements/effects of another(s)} (and thus reach 0). Thus by NOT coming to 0, we are specifying that each column here must provide new information re: movements through dimensional space. Each has to be unique, in that you cannot rescale another vector to express that same information (if you could, you could rescale that vector in the opposite direction to the initial vector to reach 0.) Thus by saying it cannot = 0, we are saying it has to be unique; thus linear independence.
        
        \item Eg - two Linearly Independent vectors:
        \begin{bmatrix}
            4 \\
            0
        \end{bmatrix}
        and
        \begin{bmatrix}
            0 \\
            3
        \end{bmatrix}
        no way to express a function that gets us from 4, 0 to 0, 3; \textbf{the only unique solution to get them to equal each other is by setting both $C_1$ and $C_2$ to 0} - which is the trivial example which doesn't satisfy linear dependence.
        \item Linearly Independent vectors represent arrows pointing in different dimensions; they are different pieces of information.
        
    \end{itemize}
\end{itemize}\\
\\
The set $\left\{\begin{bmatrix}2 \\ 3\end{bmatrix}, \begin{bmatrix}4 \\ 6\end{bmatrix}\right\}$ is linearly dependent. \\

The set $\left\{\begin{bmatrix}0 \\ 3\end{bmatrix}, \begin{bmatrix}4 \\ 0\end{bmatrix}\right\}$ is linearly independent.\\

The set $\left\{\begin{bmatrix}0 \\ 3\end{bmatrix}, \begin{bmatrix}4 \\ 0\end{bmatrix}, \begin{bmatrix}2 \\ 2\end{bmatrix}\right\}$ is linearly dependent.\\

\subsection{Examples of proving linear (in)dependence}

\begin{tcolorbox}
    1) set up the equation $c_1\mathbf{v}_1 + c_2\mathbf{v}_2 \cdot + c_n\mathbf{v}_n = \mathbf{0}$\\
    2) solve the system of equations: take one $c_nv_n$ over to the other side so you can write one of the $c_nv_n$ as a function of the others, then plug back in, etc.  
\end{tcolorbox}

\subsubsection{E.g.1 Proving Linear Dependence: finding non-trivial solution}
\begin{tcolorbox}
\begin{align*}
\textbf{Vectors:} \quad 
&\mathbf{v}_1 = \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix}, \quad 
\mathbf{v}_2 = \begin{bmatrix} 2 \\ 4 \\ 6 \end{bmatrix}, \quad 
\mathbf{v}_3 = \begin{bmatrix} -1 \\ -2 \\ -3 \end{bmatrix} 
\end{align*}

\textbf{Step 1}: Set up the equation \[c_1\mathbf{v}_1 + c_2\mathbf{v}_2 + c_3\mathbf{v}_3 = \mathbf{0}\]
This leads to the system: 
\begin{align*}
1c_1 + 2c_2 - 1c_3 &= 0 \\
2c_1 + 4c_2 - 2c_3 &= 0 \\
3c_1 + 6c_2 - 3c_3 &= 0 \\
\end{align*}
\end{tcolorbox}

\textbf{Step 2}: Solve the system:
\begin{itemize}
    \item \textbf{Finding a Non-Trivial Solution:}
    \begin{itemize}
        \item e.g: let's assign $c_2 = 1$ and $c_3 = 0$...
        \item ...then from the first equation: $c_1 + 2(-1) + -1(0) = 0$...
        \item ...gives $c_1 = 2$.
        \item Thus $c_1 = 2; c_2 = -1; c_3 = 0$ is a valid solution
        \item when dependent, a non-trivial solution always exists (ie other than $c_1 = c_2 = c_3 = 0$)
    \end{itemize}
    \item \textbf{Observation:}
    \begin{itemize}
        \item each row is a multiple of the first row: not independent.
        \item example of a homogeneous system, where all the equations are multiples of each other
    \end{itemize}
    \end{itemize}
We were able to find the non-zero coefficients $c_1, c_2, c_3$ that satisfy the linear combination $c_1\mathbf{v}_1 + c_2\mathbf{v}_2 + c_3\mathbf{v}_3 = \mathbf{0}$.\\
\\
We found a non-trivial solution where not all $c_i$ are 0: so they are linearly dependent. 
\\
\subsubsection{E.g.2 Proving Linear Independence: only a trivial solution}
\begin{tcolorbox}
\begin{align*}
\textbf{Vectors:} \quad 
&\mathbf{v}_1 = \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}, \quad 
\mathbf{v}_2 = \begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix}, \quad 
\mathbf{v}_3 = \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix} 
\end{align*}

\textbf{Step 1}: Set up the equation \[c_1\mathbf{v}_1 + c_2\mathbf{v}_2 + c_3\mathbf{v}_3 = \mathbf{0}\]
This leads to the system: 
\begin{align*}
1c_1 + 0c_2 + 0c_3 &= 0 \\
0c_1 + 1c_2 + 0c_3 &= 0 \\
0c_1 + 0c_2 + 1c_3 &= 0 \\
\end{align*}

Simplify:
\begin{align*}
c_1 &= 0 \\
c_2 &= 0 \\
c_3 &= 0 \\
\end{align*}
\end{tcolorbox}

\textbf{Step 2}: Solve the system:\\
Since the only solution to this system is the trivial solution $c_1 = c_2 = c_3 = 0$, the vectors are linearly independent.

\textbf{Observation}: \\
Can see that there is no way to multiply one of the vectors to map it onto another = independent.

\subsubsection{E.g.3 Proving Linear Dependence (Mixed Case)}
\begin{tcolorbox}
\begin{align*}
\textbf{Vectors:} \quad 
&\mathbf{v}_1 = \begin{bmatrix} 1 \\ 3 \\ 4 \end{bmatrix}, \quad 
\mathbf{v}_2 = \begin{bmatrix} 2 \\ 6 \\ 8 \end{bmatrix}, \quad 
\mathbf{v}_3 = \begin{bmatrix} 0 \\ 1 \\ 2 \end{bmatrix} 
\end{align*}

\textbf{Step 1}: Set up the equation \[c_1\mathbf{v}_1 + c_2\mathbf{v}_2 + c_3\mathbf{v}_3 = \mathbf{0}\]
This leads to the system: 
\begin{align*}
c_1 + 2c_2 &= 0 \\
3c_1 + 6c_2 + c_3 &= 0 \\
\end{align*}
\end{tcolorbox}

\textbf{Step 2}: Solve the system:\\
\textbf{Finding a Non-Trivial Solution:}
\begin{itemize}
    \item Solving the first equation for $c_1$ gives $c_1 = -2c_2$.
    \item Substitute $c_1$ into the second equation: $3(-2c_2) + 6c_2 + c_3 = 0$ simplifies to $c_3 = 0$.
    \item Substitute $c_1$ and $c_3$ into the third equation: it holds true for any value of $c_2$.
\end{itemize}

We found a non-trivial solution where not all $c_i$ are 0: so the vectors are linearly dependent.

\textbf{Observation}: \\
Can see that $v_1$ and $v_2$ are linearly dependent.

\subsubsection{Determining Linear Dependence or Independence}
\begin{tcolorbox}
\textbf{Given Vectors:} \\
\mathbf{v}_1 = \begin{bmatrix} 0 \\ 3 \end{bmatrix}, \quad
\mathbf{v}_2 = \begin{bmatrix} 4 \\ 0 \end{bmatrix}, \quad
\mathbf{v}_3 = \begin{bmatrix} 2 \\ 2 \end{bmatrix}

\textbf{Step 1}: Set up the equation for linear combination: \\
\[ c_1\mathbf{v}_1 + c_2\mathbf{v}_2 + c_3\mathbf{v}_3 = \mathbf{0} \]
This leads to the system of linear equations:
\begin{align*}
0c_1 + 4c_2 + 2c_3 &= 0 \\
3c_1 + 0c_2 + 2c_3 &= 0
\end{align*}
\end{tcolorbox}


\textbf{Step 2}: Solve the system: 
\begin{itemize}
    \item From the first equation, we get: \( 2c_2 + c_3 = 0 \rightarrow c_3 = -2c_2 \). 
    \item Substituting \( c_3 = -2c_2 \) into the second equation gives \( 3c_1 - 4c_2 = 0 \rightarrow 3c_1 = 4c_2 \).
\end{itemize}


\textbf{Conclusion}: \\
The system does not provide a clear non-trivial solution for \( c_1, c_2, c_3 \), suggesting that the vectors are linearly independent under typical circumstances.



\section{The Span}
\subsection{in terms of Vector Space / Basis vectors}

We define the span of a the set of vectors $S$ as \textbf{all linear combination of the vectors in the set}. 

\begin{itemize}
    \item the span of $v_1$ and $v_2$ incl all vectors that can be formed by taking $a \cdot v_1 + b \cdot v_2$, where both vectors are linearly independent (independence not a hard requirement, but otherwise, we are just including  redundant info if you think about it geometrically)
    \item the span of 2 linearly independent vectors $v_1 \in \mathbb{R}^2$ and $v_2 \in \mathbb{R}^2$ is $\mathbb{R}^2$ : this means that any vector in the 2D space can be expressed as a linear combination of $v_1$ or $v_2$
    \item this implies that $v_1$ and $v_2$ form a basis for $R^2$ since they can generate the entire 2D space through their linear combos (i.e. they provide a complete representation of the 2D space)
\end{itemize}

\subsection{in terms of the Spanning Set}
Going the other way, if $V$ is a vector space and $S$ a set of vectors in $V$, then we say that \textbf{$S$ is a spanning set for $V$ if all the dimensions of $V$ can be represented by linear combinations of $S$}.

\begin{itemize}
    \item a Spanning Set $S$ of a vector space $V$ is a set of vectors, such that every vector in $V$ can be expressed as a linear combo of the vectors in $S$.
    \item if $S$ spans $V$, it means that $S$ contains enough linearly independent vectors to cover the entire vector space $V$
    \item generally a spanning set $S$ must contain as least as many elements as the linearly independent vectors from $V$... but a Spanning set not a minimal thing: as long as it contains $n$ linearly independent vectors then it can also include additional vectors, but they will be redundant info.
\end{itemize}



\textbf{There are exactly $n$ linearly independent vectors in $\mathbb{R}^n$} (no more, no less!)
\begin{itemize}
    \item ... 2 in $\mathbb{R}^2$
    \item ... 3 in $\mathbb{R}^3$
    \item .... $n$ in $\mathbb{R}^n$
    \item these vectors = the basis for the space
    \item Geometrically: visualise $\mathbb{R}^2$ vector space: every movement can be made up of linear combination is movements along $y$ and along $x$ - these are the fundamental building blocks which are \textbf{orthogonal (linearly independent)} - everything else is linearly dependent.
    \item  linearly dependent vectors present redundant information, in that they are \textbf{colinear} (geometric interpretation in R2) or \textbf{co-planar} (geometric interpretation of R2+) to the basis of the space, 
    \item i.e. one of the vectors of the set can be represented as a combination of others \textbf{(it doesn't give us any new dimensions)}
    \item A set of vectors is said to be linearly independent if no vector in the set can be written as a linear combination of the others: none of the vectors in the set is redundant in terms of the information it provides about the vector space.
\end{itemize}

 The \textbf{dimension of a vector space $V$ is defined as the maximum number of linearly independent vectors in $V$}. It represents the number of coordinates needed to specify any vector in the space uniquely.
\begin{itemize}
    \item For a set $S$ to span a vector space $V$, it must contain at least as many vectors as the dimension of $V$.
    \item However, having exactly as many vectors in $S$ as the dimension of $V$ does not automatically guarantee that $S$ spans $V$. Those vectors also need to be linearly independent.
\end{itemize}

\begin{tcolorbox}
    A Spanning Set $S$ must contain at last as many elements as the linearly independent vectors from $V$.\\
   \\ 
   There are exactly $n$ linearly independent vectors in $R^n$\\
   \\
    Dimension of a vector space $V$ is defined as the maximum number of linearly independent vectors in $V$. It represents the number of coordinates needed to specify any vector in the space uniquely
\end{tcolorbox}

\section{The Determinant}

The determinant is a scalar value that can be computed from the elements of a square matrix.\\
\\
Often denoted as $det(\textbf{A})$ or $|\textbf{A}|$\\
= a way to multiply entries of the matrix to produce a single number.
\\

\subsection{Uses}
\begin{itemize}
    \item \textbf{Linear Transformations} - the absolute value of the determinant of a matrix representing a linear transformation reflects how the transformation changes the area (in 2D) or volume (in 3D) of shapes. A negative determinant indicates that the transformation also involves a reflection.
    \item \textbf{Eigenvalues and Characteristic Polynomial}: The characteristic polynomial of a matrix, used to find its eigenvalues, is derived from its determinant. (I think this is related to the above, as a determinant tells you nature of the transformation, and eigenvalue-eigenvector pairings do the same??)
    \item \textbf{to determine if a matrix has an inverse} (is non-singular). A square matrix is singular (non-invertible) if and only if its determinant is zero. (see Savov p. 169-171)
    \item \textbf{to check for linear independence} - related to checking if has inverse: as a matrix has an inverse if and only if its columns are linearly independent (see Savov p. 167)
    \item \textbf{to compute areas and volumes} - the absolute value of the determinant of a matrix formed by vectors representing the sides of a parallelogram (in 2D) or a parallelepiped (in 3D) gives the area (in 2D) or volume (in 3D) of that geometric shape. (see Savov p. 161)
    \item \textbf{solving systems of equations} (see Cramer’s Rule, Savov p. 166)   
\end{itemize}

\subsection{Calculation}
\begin{tcolorbox}
    For a 1x1 Matrix (scalar):
\[ A = \begin{bmatrix} a \end{bmatrix} \] 
the determinant is simply the value of that single element:
\[ \det(A) = a \]
\end{tcolorbox}

\begin{tcolorbox}
For a 2x2 Matrix:
\[ A = \begin{bmatrix} a & b \\ c & d \end{bmatrix} \]
\[ \det(A) = ad - bc \]
\end{tcolorbox}

\begin{tcolorbox}
For a 3x3 Matrix:
\[ A = \begin{bmatrix} a & b & c \\ d & e & f \\ g & h & i \end{bmatrix} \]
\[ \det(A) = a(ei - fh) - b(di - fg) + c(dh - eg) \]
\end{tcolorbox}


The matrices bigger than 3x3 ,the determinant calculation becomes complex and typically calculated using methods such as expansion by minors or Laplace expansion.\\

go through https://www.khanacademy.org/math/linear-algebra/vectors-and-spaces/linear-independence/v/linear-algebra-introduction-to-linear-independence from 7 mins onwards

\begin{itemize}
    \item general intutition product of top left x bottom right, minus product of top right x bottom left...
    \item ... but this only works properly for 2x2 matrix, after that it gets complicated...
    \item ... for 3x3: take each element in term as a constant, multiply the remaining minor, alternately subtract then add each element.
\end{itemize}

\begin{tcolorbox}
General formula: \[{det}(A) = \sum_{j=1}^{n} (-1)^{1+j}a_{1j}M_{1j}\] 
\textit{where $M_ij$ is called the minor associated with $a_{i}$: the determinant of the submatrix generated by removing row i and column j from the matrix A.}
\end{tcolorbox}

In the determinant formula, the $(-1)^{1+j}$ just reverses the sign starting with +, then -, then +, etc. This means we go up to the power of $+j$ where $j$ is the number of columns in the matrix, so 1+[odd value column] is to raise it to an even power.

2x2 Matrix:
\begin{align*}
    \begin{vmatrix}
    a_{11} & a_{12} \\
    a_{21} & a_{22} \\
    \end{vmatrix}
    = a_{11}a_{22} - a_{12}a_{21}
\end{align*}

3x3 Matrix
\begin{align*}
    \text{det}(A) = a \cdot \text{det}\left|
    \begin{array}{cc}
    e & h \\
    f & i \\
    \end{array}
    \right| - b \cdot \text{det}\left|
    \begin{array}{cc}
    d & g \\
    f & i \\
    \end{array}
    \right| + c \cdot \text{det}\left|
    \begin{array}{cc}
    d & g \\
    e & h \\
    \end{array}
    \right|    
\end{align*}
\[ \rightarrow \det(A) = a(ei - fh) - b(di - fg) + c(dh - eg) \]

Resources for determinants: Savov, p. 158-161

\section{Matrix Inverse}
\[A^{-1} A = I\]
Not all matrices invertible: only exists if and only if: 
\[det(A) \neq 0\]
Inverse defined: 
\[A^{-1} = \frac{1}{\text{det}(A)} \text{adj}(A)\]
Where adj(A) is the adjugate matrix\\


The inverse of a 2x2 matrix \( A = \begin{pmatrix} a & b \\ c & d \end{pmatrix} \) is given by
\[
A^{-1} = \frac{1}{\det(A)} \begin{pmatrix} d & -b \\ -c & a \end{pmatrix}
\]
where \( \det(A) = ad - bc \) is the determinant of \( A \). The matrix \( A \) is invertible if and only if \( \det(A) \neq 0 \).

Given the matrix \( A = \begin{pmatrix} 3 & 0 \\ 0 & 3 \end{pmatrix} \).

The determinant of \( A \) is calculated as
\[
\det(A) = 3 \times 3 - 0 \times 0 = 9
\]

The formula for the inverse of a 2x2 matrix is
\[
A^{-1} = \frac{1}{\det(A)} \begin{pmatrix} d & -b \\ -c & a \end{pmatrix}
\]

Applying this to matrix \( A \):
\[
A^{-1} = \frac{1}{9} \begin{pmatrix} 3 & 0 \\ 0 & 3 \end{pmatrix} = \begin{pmatrix} \frac{1}{3} & 0 \\ 0 & \frac{1}{3} \end{pmatrix}
\]

Thus, the inverse of matrix \( A \) is \( A^{-1} = \begin{pmatrix} \frac{1}{3} & 0 \\ 0 & \frac{1}{3} \end{pmatrix} \).\\

OR AN EASIER WAY is just to eyeball it for easy matrices:
\[
\begin{pmatrix} 3 & 0 \\ 0 & 3 \end{pmatrix} \begin{pmatrix} \frac{1}{3} & 0 \\ 0 & \frac{1}{3} \end{pmatrix} = \begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix}
\]
So, what do you have to multiply the values, to get to 1, 0, 0, 1.


\subsection{Adjugate Matrix}
= the tranpose of the matrix of cofactors:
\[adj(A) = C^T\]
... see Savov p169-171 for matrix of cofactors

\section{To know}
\begin{tcolorbox}
\textbf{
    \begin{enumerate}
    \item a matrix has an inverse, if and only if it has a non-zero determiant
    \item a matrix has an inverse, if and only if its columns are linearly independent
    \item thus, a matrix has a non-zero determinant if and only if its columns are linearly independent.
\end{enumerate}}

\textit{This is actually intuitive: if you think of linear dependence as about vectors/columns providing information about the vector space, and the determinant tells you about the nature of the linear transformation.... if you have redundant (dependent) columns then this means the matrix is not a clean representation of a linear transformation, qhich is they it has implications for its determinant.}\\

So the direction of causality is:\\
\textbf{Linear Independence $\rightarrow$ Non-zero Determinant $\rightarrow$ has Inverse}

\end{tcolorbox}


\subsection{Conditions under which a matrix is invertible:}

\textbf{\begin{enumerate}
    \item matrix is square
    \item columns are linearly independent (full rank)
    \item non-zero determinant
\end{enumerate}}

Given the matrix \( A = \begin{pmatrix} 1 & 0 \\ 2 & 0 \end{pmatrix} \), the determinant of \( A \) is calculated as
\[
\det(A) = 1 \times 0 - 0 \times 2 = 0
\]
Since the determinant of \( A \) is zero, the columns are not linearly independent, the matrix \( A \) is not invertible.\\

\subsubsection{Proof by contradiction:}

Suppose a linearly dependent Matrix \textit{is invertible}...\\

Let $A$ be a square matrix with dependent columns: $a_1, a_2 \cdots a_n$ 
\begin{align*}
    A = \begin{bmatrix}
    a_1 & a_2 & \cdots & a_n
    \end{bmatrix}_{i}
\end{align*}


By definition of Linear Independence:
\[c_1a_1 + c_2a_2 + \ldots + c_na_n = 0\]
and $c_i \neq 0$ for some $i$ \\

We can write this equivalently as:
\[\quad A\mathbf{c} = \mathbf{0}, \text{where } \mathbf{c} = \begin{bmatrix}
c_1 \\
c_2 \\
\vdots \\
c_n
\end{bmatrix}
\]
This equation means that $c$ is a vector that, when multiplied by $A$, results in the zero vector. This is the definition of a vector being in the null space of $A$.\\

If $A$ was invertible, then we could multiply both sides by its inverse:
\begin{align*}
    A^{-1} A\mathbf{c} = A^{-1} \mathbf{0} \\
    I\mathbf{c} = \mathbf{0} \\
    \mathbf{c} = \mathbf{0}
\end{align*}

BUT, by the definition of linear dependence, $c$ cannot be the zero vector. \\
So we have a contradiction. \\
So $A$ must be Linearly independent.

\section{Eigenvalues and Eigenvectors}

\begin{tcolorbox}
    Eigenvalue-vector pairing effectively reduces matrix to a single scalar ($\lambda$), along a particular path (provided by the vector).\\

    Eigenvectors represent directions in the data space, and eigenvalues indicate the magnitude of variance along these directions.\\


    The matrix describes a transformation / a journey / a movement, and that lambda value is the most direct route to replicate that journey (but it is specific / associated with that vector)
\end{tcolorbox}

Definition of an eigenvector-eigenvalue pairing:
\[Av = \lambda v\]
Where $v$ is an eigenvector, and $\lambda$ is an eigenvalue (a scalar)\\

I.e.

\begin{itemize}
    \item if you multiply the matrix by a specific vector (eigenvector) you get the same result as multiplying that same vector by a single number (eigenvalue).
    \item = a specific vector-scalar pairing that reduces the matrix to a single number (for a given vector)
    \item Think of the eigenvector as a projected movement in a specific direction, and the eigenvalue as the extent / magnitude / degree of that movement.
    \item we are reducing the matrix to a single scalar value (its essence), which it can be reduced to when transforming a specific vector.
    \item Alternatively, think of $A$ as some transformation of $v$: it projects $v$ from one place to another. \textbf{If $A$ is the journey, then the eigenvalue $\lambda$ is the most direct route; the essence of the transformation} (the eigen = the self)
\end{itemize}

For example:
\begin{align*}
    \underbrace{\begin{bmatrix}
        1 & 2 \\
        4 & 3
    \end{bmatrix}}_{A}
    \underbrace{\begin{bmatrix}
        1 \\
        2
    \end{bmatrix}}_{v}
    &= \underbrace{5}_{\lambda}
    \underbrace{\begin{bmatrix}
        1 \\
        2
    \end{bmatrix}}_{v}
\end{align*}

In this case, $v = \begin{bmatrix} 1 \\ 2 \end{bmatrix} \text{ is an eigenvector and } \lambda = 5 \text{ is an eigenvalue for } A = \begin{bmatrix} 1 & 2 \\ 4 & 3 \end{bmatrix}$\\

\textit{NB Chat GPT gave the below automatically when I fed it the above, which is interesting, it's presenting the data matrix A, a given vector, the eigen value, and the eigen vectors}

For example:
\begin{align*}
    \underbrace{\begin{bmatrix}
        1 & 2 \\
        4 & 3
    \end{bmatrix}}_{A}
    \underbrace{\begin{bmatrix}
        v_1 \\
        v_2
    \end{bmatrix}}_{v}
    &= 5
    \underbrace{\begin{bmatrix}
        \lambda_1 \\
        \lambda_2
    \end{bmatrix}}_{\lambda}
\end{align*}

\subsection{Conditions for existence of (non-trivial) Eigenvector/values}

Not every matrix can be decomposed into eigenvalue-vector pairs.The conditions for the existence of non-trivial eigenvectors and eigenvalues for a square matrix \( A \) are:

\begin{enumerate}
    \item The matrix \( A \) must be a \textbf{square} matrix. The existence of eigenvalues and eigenvectors is a fundamental property of square matrices. Everything comes from there??
    \item There must exist a \textbf{scalar \( \lambda \) such that   \(\det \mathbf{(\lambda I - A)} = 0 \)}, where \( I \) is the identity matrix of the same dimension as \( A \). This is derived from the \textbf{characteristic equation \(\det(\lambda I - A) = 0 \)}.
    \item For each eigenvalue \( \lambda \), there must exist a non-zero solution \( \mathbf{v} \) to the equation \( (A - \lambda I)\mathbf{v} = 0 \).
    \item Eigenvectors must be non-zero. The zero vector is not considered an eigenvector.
    \item The algebraic multiplicity of an eigenvalue must be at least as great as its geometric multiplicity, which is the number of linearly independent eigenvectors corresponding to that eigenvalue.
\end{enumerate}



\subsubsection{Proof for finding eigenvalues}

Start with how we defined the eigenvector and eigenvalue:
\[Av = \lambda v \]
Simply rearrange: 
\[0 = \lambda v - Av\]
Now, let's replace $\lambda$ with $\lambda I_n $ (this is the same): 
\[0 = \lambda I_n v - Av \]
Which allows us to write: 
\begin{align*}
0 &= (\lambda I_n - A)v\\
0 &= \underbrace{(\lambda_n I - A)}_{\text{call this B} \mathbf{v}} \\
0 &= \mathbf{Bv}
\end{align*}

This is trivially satisfied by $v = 0$. \\
We're looking for some non-trivial solution; i.e. some non-zero $v$\\

But more importantly, this is \textit{precisely} the definition of \textbf{linear dependence for the columns of B}, where our current vector $v$ is the vector of constants $c_1, c_2, \dots, c_n$. \\

\textbf{If a matrix has linearly dependent columns, then its determinant is zero; if we want a nontrivial solution to the above (IS THIS THE DEFINITION / EXISTENCE OF EIGEN VECTORS?), we require:}

\[det(\lambda I_n - A) = 0\]

\begin{tcolorbox}
    \[det(\lambda I_n - A) = 0\]
    Same as
    \[det(A - \lambda I_n) = 0\]

This is the eigenvalue equation. \\
It is a generic form of the characteristic equation of a matrix (IS IT?). \\
Eigenvalues are the values of λ that satisfy the equation. \\

\textit{where:
\begin{itemize}
    \item $A$ = square $n \times n$ matrix; whose eigenvalues we are interested in finding.
    \item $\lambda$ = (scalar) eigenvalue of matrix $A$; goal is to finv values of $\lambda$ that satisfy this equation.
    \item $I_n$ = identify matrix of same size as $A$ 
    \item \textbf{$\rightarrow \lambda I_n - A)$ = matrix formed by subtracting the matrix $A$ from the scalar $\lambda$ multiplied by the identity matrix $I_n$}. The result is a matrix where the diagonal elements are $\lambda - a_{ij}$ where $a_{ij}$ are diagonal elements of $A$. WHAT ABOUT NON-DIAGONAL ELEMENTS
    \item the determinant of that matrix
    \item set that determinant to 0 $\rightarrow$ becomes the characteristic equation. Solve to find eigenvalues of $A$.   
\end{itemize}}

\textbf{Essentially: looking for values of $\lambda$ that make the determinant of $( \lambda I_n - A)$ equal to zero.}

\end{tcolorbox}

\subsubsection{Explanation}
The determinant of a square matrix is a scalar value that provides important information about the matrix, including whether its columns (or rows) are linearly independent. When a matrix has linearly dependent columns, its determinant is zero. \\

\begin{itemize}
    \item \textbf{Linear Dependence:} If the columns (or rows) of a matrix are linearly dependent, it means that at least one column (or row) can be expressed as a linear combination of the others. In other words, there exists a set of scalar coefficients, not all zero, such that a weighted sum of the columns (or rows) equals a zero vector. \[c_1v_1 + c_2v_2 + \cdots + c_nv_n = 0\]
    \item \textbf{Determinant and Linear Transformations:} The determinant can be interpreted as a scaling factor of the linear transformation represented by the matrix. If a matrix has linearly dependent columns, it means that the space it maps to is "flattened" in at least one dimension (since one column is a combination of others). This "flattening" means the matrix compresses the space into a lower dimension, resulting in a volume of zero in the original space.
    \item \textbf{Geometric Interpretation:} Geometrically, the determinant of a matrix can be thought of as the volume of the parallelepiped spanned by its columns (in the case of a 3x3 matrix) or rows. If columns are linearly dependent, this parallelepiped collapses into a lower-dimensional space (like a line or a plane), which has zero volume in the context of the original space.
    \item \textbf{Properties of the Determinant Function:} The determinant function has a property that if two columns (or rows) of a matrix are identical, or one column is a scalar multiple of another, the determinant is zero. This is a specific case of linear dependence. In a more general sense, any linear combination leading to a redundant or dependent column (or row) results in a zero determinant.
\end{itemize}

\subsection{Finding Eigenvalues/vectors}
\begin{tcolorbox}    
Start with condition: 
\[det(\lambda I_n - A) = 0\]
\end{tcolorbox}

Applying this to our matrix:
\begin{align*}
\det \left( \lambda \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} - \begin{bmatrix} 1 & 2 \\ 4 & 3 \end{bmatrix} \right) = 0
\end{align*}
Computing this yields the \textbf{characteristic polynomial}:
\[\lambda^2 - 4\lambda - 5 = 0\]
Giving us $\lambda = 5, \lambda = -1$.\\

These are the eigenvalues.\\

\begin{tcolorbox}
To find eigenvectors from these values:
\begin{itemize}
    \item For each eigenvalue λ you have found, substitute it back into the eigenvalue equation \[(A - \lambda I)v = 0\] 
    \item where $A$ is the original matrix.
    \item $v$ is the associated eigenvector we are trying to solve for.
    \item Solve the linear system of equations for $v$
\end{itemize}
\end{tcolorbox}

\subsection{Eigenspace}
From Eigenvalues \rightarrow Eigenvectors. \\

There will be more than one eigenvector corresponding to a given eigenvalue $\lambda$. \\
\begin{tcolorbox}
The \textbf{eigenspace of $\lambda$, $E_\lambda$, is all vectors $v$ that satsify the condition $Av = \lambda v$}\\
\end{tcolorbox}

So, we find all vectors $v$ such that 

\begin{align*}
    \begin{bmatrix}
        1 & 2 \\
        4 & 3
    \end{bmatrix}
    \begin{bmatrix}
        v_1 \\
        v_2
    \end{bmatrix}
    &= 5
    \begin{bmatrix}
        v_1 \\
        v_2
    \end{bmatrix}
\end{align*}

Which is the span of 
\[ \begin{bmatrix}
    1 \\
    2
\end{bmatrix}\]
See below for guidance:
\subsubsection*{Step 1: Matrix Multiplication}
First, perform the matrix multiplication on the left side of the equation.
\begin{align*}
    \begin{bmatrix}
        1 & 2 \\
        4 & 3
    \end{bmatrix}
    \begin{bmatrix}
        v_1 \\
        v_2
    \end{bmatrix}
    &= \begin{bmatrix}
        1 \cdot v_1 + 2 \cdot v_2 \\
        4 \cdot v_1 + 3 \cdot v_2
    \end{bmatrix} \\
    &= \begin{bmatrix}
        v_1 + 2v_2 \\
        4v_1 + 3v_2
    \end{bmatrix}
\end{align*}

\subsubsection*{Step 2: Scalar Multiplication}
Next, perform the scalar multiplication on the right side of the equation.
\begin{align*}
    5
    \begin{bmatrix}
        v_1 \\
        v_2
    \end{bmatrix}
    &= \begin{bmatrix}
        5v_1 \\
        5v_2
    \end{bmatrix}
\end{align*}

\subsubsection*{Step 3: Setting Up the Equation}
Now, equate the results of the left and right side multiplications.
\begin{align*}
    \begin{bmatrix}
        v_1 + 2v_2 \\
        4v_1 + 3v_2
    \end{bmatrix}
    &= \begin{bmatrix}
        5v_1 \\
        5v_2
    \end{bmatrix}
\end{align*}

\subsubsection*{Step 4: Solving the System of Equations}
This results in a system of linear equations:
\begin{align*}
    v_1 + 2v_2 &= 5v_1 \\
    4v_1 + 3v_2 &= 5v_2
\end{align*}
These equations can be solved to find the values of \( v_1 \) and \( v_2 \).

\subsection{Putting it all together}

\[
\underbrace{\begin{bmatrix}
    1 & 2 \\
    4 & 3
\end{bmatrix}}_{A}
\underbrace{\begin{bmatrix}
    v_1 \\
    v_2
\end{bmatrix}}_{\mathbf{v}}
= \underbrace{5}_{\lambda} 
\underbrace{\begin{bmatrix}
    v_1 \\
    v_2
\end{bmatrix}}_{\mathbf{v}}
\]

We have found one eigenvector-eigenvalue combination for the matrix \( A \) (there was another for \( \lambda = -1 \)).\\

see Khan academi links p23

\section{Eigen decomposition}

\subsection{Definition}

We can use this eigenvalue-vector pairings to find the \textbf{eigendecomposition} of $A$

\[A = Q\Lambda Q^{-1}\]
Where:
\begin{itemize}
    \item \( A \) is a square matrix.
    \item $\Lambda$ is a diagonal matrix with eigenvalues of $A$ on the main diagonal (and 0 everywhere else). \textit{(An $n \times n$ square will have $n$ eigenvalues, so $\Lambda$ will also be $n \times n$)}
    \item $Q$ is a matrix where the columns are eigenvectors of $A$, correspeonding to the eigenvalues in $\Lambda$ 
\end{itemize}


\subsection{The conditions for eigendecomposition}

\begin{enumerate}
\item The matrix \( A \) must be \textbf{square ($n \times n$}.
\item The matrix must \textbf{have \( n \) linearly independent eigenvectors}, where \( n \) is the size of the matrix.
\item The matrix \( A \) is diagonalizable if it is similar to a diagonal matrix \( \Lambda \), i.e., there exists an invertible matrix \( Q \) such that \( Q^{-1}AQ = \Lambda \). This is possible if and only if \( A \) has \( n \) independent eigenvectors. (this comes from having $n$ independent eigenvectors condition above.)
\end{enumerate}

\subsection{Calculation}
Taking our matrix \( A \) from before, we write:
\begin{align*}
A &= 
\underbrace{\begin{bmatrix}
    1 & -1 \\
    2 & 1 
\end{bmatrix}}_{Q}
\underbrace{\begin{bmatrix}
    5 & 0 \\
    0 & -1 
\end{bmatrix}}_{\Lambda}
\underbrace{\begin{bmatrix}
    1 & -1 \\
    2 & 1 
\end{bmatrix}^{-1}}_{Q^{-1}} \\
\end{align*}

\begin{itemize}
    \item recalling our solutions \( \lambda_1 = 5 \) and \( \lambda_2 = -1 \), 
    \item and the corresponding eigenvectors \( v_1 = 
        \begin{bmatrix}
            1 \\
            2 
        \end{bmatrix} \) (which we found above) 
    \item and \( v_2 = 
        \begin{bmatrix}
            -1 \\
            1 
        \end{bmatrix} \) 
\end{itemize}

\subsection{Why?}
Reducing a matrix to its essence is efficient for computing further transformations 

\section{Singular Value Decomposition of a Matrix}
\subsection{SVD Definition}

Eigendecomposition only works for square matrices. \\
SVD allows us to get a matrix of any dimension to its essence.

\begin{tcolorbox}

\\]

where \(A \in \mathbb{R}^{m \times n}\)
\begin{itemize}
    \item $U$ is a matrix containing left singular vectors of A (contains the $m$ eigenvectors of the square matrix $AA^T$) ($n \times n$)
    \item $V^T$ is a matrix containing the right singular vectors of A (contains the $n$ eigenvectors of the square matrix $A^T A$ ($m \times n$)
    \item $\Sigma$ is a matrix where the diagonal entries are the singular values of $A$ (square roots of the eigenvalues of $A^TA$ (or $AA^T$) on the diagonal. ($n \times n$)
\end{itemize}
\end{tcolorbox}

\begin{tcolorbox}
\textbf{Dimensions:}\\

Matrix $A$: Suppose $A$ is an $m \times n$ matrix.

Matrix $U$: $U$ is an $m \times m$ orthogonal matrix. The columns of $U$ are the left singular vectors of $A$.

Matrix $\Sigma$: $\Sigma$ is an $m \times n$ diagonal matrix. The non-zero elements of $\Sigma$ (which are on the diagonal) are the singular values of $A$. If $m > n$, the additional rows will be filled with zeros. If $n > m$, there will be additional zero columns.

Matrix $V^T$: $V^T$ is the transpose of an $n \times n$ orthogonal matrix $V$. The columns of $V$ (or the rows of $V^T$) are the right singular vectors of $A$.

So, in the SVD $A = U\Sigma V^T$:
\begin{itemize}
    \item $U$ aligns with the row dimension of $A$.
    \item $\Sigma$ matches the overall dimensions of $A$, but is diagonal.
    \item $V^T$ aligns with the column dimension of $A$.
\end{itemize}
\end{tcolorbox}

Think of $AA^T$ or $A^TA$ as "squaring" the matrix in two ways: 1) it is like $A^2$ in some ways, 2) it crucially transforms a non-square matrix into a square, so it can be decomposed.\\

This is why we use singular values of $A$ in $Sigma$'s diagonal: singular values are square roots of the eigenvalues of the "squared $A$" matrix ($AA^T$ or $A^TA$)

\subsection{SVD Is Profoundly Informative About the Structure and Dimensionality of Your Data}
\begin{itemize}
    \item SVD like an an x-ray of a matrix:
    \begin{itemize}
        \item \textbf{$U$ and $V^T$ represent how much it rotates an object} when multiplied by it, as they contain the singular vectors (like eigenvectors, but for non-square matrices)
        \item \textbf{$\Sigma$ represents associated scaling}, as it contains the associated singular values across its diagonal. (NB remember, these are the square roots of the A's eigenvalues)
    \end{itemize}
    \item Large singular values correspond to large "strength" of the transformation that $A$ makes along the subspace created by the associated left and right singular vectors. 
    \begin{itemize}
        \item NB: a 0 = redundant
        \item the \textbf{number of non-zero singular values gives the rank of the matrix, or the number of linearly independent columns}
        \item if \textbf{many singular values are zero (or very small), it suggests redundancy} in the data: data can be represented in a lower dimensional space without much loss of info
    \end{itemize}
\end{itemize}

\end{document}