\documentclass[a4paper,11pt]{article} 

\usepackage[top = 2.5cm, bottom = 2.5cm, left = 2.5cm, right = 2.5cm]{geometry} 
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{multirow} 
\usepackage{booktabs}
\usepackage{graphicx} 
\usepackage{setspace}
\setlength{\parindent}{0in}
\usepackage{enumerate}
\usepackage{float}
\usepackage{fancyhdr}
\usepackage{amsmath}
\usepackage{tcolorbox}
\usepackage{graphicx}

\DeclareMathOperator*{\argmin}{arg\,min}


\pagestyle{fancy} 
\fancyhf{}
\lhead{\footnotesize M4DS Finals revision}
\rhead{\footnotesize Henry Baker} 
\cfoot{\footnotesize \thepage} 


\begin{document}


\thispagestyle{empty}
\begin{tabular}{p{15.5cm}} 
{\large \bf Finals Fall 2023  \\ Henry Baker}
\hline 
\\
\end{tabular} 

\vspace*{0.3cm} 
\begin{center} 
	{\Large \bf M4DS Finals Revision: Session 7 \\ Calculus Meets Probability / Continuous Random Variables II}
	\vspace{2mm}
	
\end{center}  
\vspace{0.4cm}

\section{Covariance}
    \begin{itemize}
        \item Covariance between two r.v.s $X$ and $Y$ is a measure of the amount they $\textit{vary together}$.
        \item If the variables tend to show similar behavior (i.e., both increase or decrease together), the covariance is positive. If one variable tends to increase when the other decreases, the covariance is negative. If the variables do not show any consistent relationship, the covariance is close to zero.
    \end{itemize}
    
\subsection{Covariance Definition} 
\begin{tcolorbox}
\begin{itemize}
    \item Covariance is just how two variables move together.
    \item Covariance is the "expectation of the product, minus the product of the expectations"
    \item Independent r.v.s: no pattern of how they move together $\rightarrow$ covariance is 0
\end{itemize}
\end{tcolorbox}


\[\text{Cov}(X, Y) = \mathbb{E}((X - \mathbb{E}X)(Y - \mathbb{E}Y))\]

Break down:
    \begin{itemize}
        \item $\mathbb{E}(X)$ and $\mathbb{E}(Y)$ = expected values / means of the r.v.s.
        \item $(X - \mathbb{E}X)$ and $(Y - \mathbb{E}Y)$ = deviation from the mean (how far each individual observation of the variables is from their average values.)
        \item $(X - \mathbb{E}X)(Y - \mathbb{E}Y)$ = product of deviations (for each pairs of observations of $X$ and $Y$. IS THIS BASIS OF LEAST SQUARED ERRORS????
        \item $ \mathbb{E}(X - \mathbb{E}X)(Y - \mathbb{E}Y)$ = expected value (or average) of these products over all pairs of observations. \textbf{Quantifies the average product of deviations, thus giving a measure of how much $X$ and $Y$ co-vary}.
    \end{itemize}

By linearity of expectation: 
\begin{tcolorbox}
\textbf{Covariance is the "expectation of the product, minus the product of the expectations"}
\[\text{Cov}(X, Y) = \mathbb{E}[XY] - \mathbb{E}[X]\mathbb{E}[Y]\]
\end{tcolorbox}
\\
***
\\
NB: definition of independence from wk 3:
\begin{itemize}
    \item Continuous r.v.s: $P(X \leq x,Y \leq y) = P(X \leq x)P(Y \leq y)$
    \item Discrete r.v.s: $P(X = x,Y = y) = P(X = x)P(Y = y)$
\end{itemize}
\\
This is related? Here rather than probabilities, we are dealing with expectations...: \\

\begin{align*}
    \text{Independence} &: \text{Covariance} = 0 \\
    \text{Covariance}= 0 &: \mathbb{E}[XY] - \mathbb{E}[X]\mathbb{E}[Y] = 0 \\
    &: \mathbb{E}[XY] = \mathbb{E}[X]\mathbb{E}[Y] 
\end{align*}

\subsection{Some Covariance rules}
\begin{enumerate}
    \item $\text{Cov}(X, X) = \text{Var}(X)$
    \item $\text{Cov}(X, Y) = \text{Cov}(Y, X)$
    \item $\text{Cov}(X, c) = 0$ for any constant $c$
    \item $\text{Cov}(aX, Y) = a \cdot \text{Cov}(X, Y)$ for any constant $a$
    \item $\text{Cov}(X + Y, Z) = \text{Cov}(X, Z) + \text{Cov}(Y, Z)$
    \item $\text{Var}(X + Y) = \text{Var}(X) + \text{Var}(Y) + 2 \cdot \text{Cov}(X, Y)$
    \item \textbf{additional:} $E(EX)$: expectation of a constant is just a constant .
\end{enumerate}

\section{Correlation}

\[\text{Corr}(X, Y) = \frac{\text{Cov}(X, Y)}{\sqrt{\text{Var}(X) \cdot \text{Var}(Y)}}\]

This scaling puts correlation between -1:1 (for ease of interpretation) \\

\subsection{Correlation Imposes Linearity}

\begin{tcolorbox}
    \begin{itemize}
        \item Independence $\rightarrow$ uncorrelated
        \item Uncorrelated $\rightarrow \times \rightarrow$ Independence
        \item Non-linearity will mess you up. E.g if you're trying to fit a curve through the parabola: you'd get a straight line
        \item if you have any non-linearity $\rightarrow$ throw some other things (other than correlation) before you conclude that they are independent
    \end{itemize}
\end{tcolorbox}

If $X$ and $Y$ are independent $\rightarrow$ uncorrelated\\

Proof:
\begin{align*}
    \text{Cov definition:    } Cov(X,Y) &= E(XY) - E(X)E(Y) \\
     \text{for independent i.v.s:    } E(XY) &= E(X)E(Y) \\
     \text{thus Cov &= 0}
\end{align*}
BUT\\
If $X$ and $Y$ are uncorrelated $\rightarrow$ not necessarily independent.\\
\begin{tcolorbox}
Consider: 
\begin{align*}
    X &\sim \mathcal{N}(0,1) \\
    Y &= X^2
\end{align*}
These are perfectly dependent ($Y$ is a function of $X$), but uncorrelated: \\
\begin{align*}
    Cov(X,Y) &= E(XY) - E(X)E(Y) \\
    &= E(X \cdot X^2) - E(X)E(Y) \\
    &= X^3 - E(X)E(Y) \\
    &= 0 - 0\cdot E(Y) \\
    &= 0
\end{align*}

\textbf{the important point is that E(X) = 0 since it is Standard Normal distribution.}\\
\end{tcolorbox}\\

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{Screenshot 2024-01-25 at 11.18.55.png}
    \caption{Enter Caption}
    \label{fig:enter-label}
\end{figure}

These plots display how correlation imposes linearity (unlike dependence)

\subsection{Further Explanation: Proof of Cov > Corr}

Two random variables $X$ and $Y$ are said to be \textbf{independent} if the occurrence of an event related to $X$ does not affect the probability of an event related to $Y$, and vice versa. \\

Mathematically, independence means that for any events $A$ and $B$: \[P(X \in A, Y \in B) = P(X \in A)P(Y \in B)\]

\textbf{Uncorrelated} random variables are those for which there is no linear relationship between them. The covariance between uncorrelated variables is zero.\\

\textbf{Covariance}, denoted as $\text{Cov}(X, Y)$, measures the joint variability of two random variables. It's defined as 
\[\text{Cov}(X, Y) = E[(X - E[X])(Y - E[Y])]\] 
which simplifies to 
\[E(XY) - E(X)E(Y)\]
\textbf{For independent r.v.s, the expectation of their product is the product of their expectations.} 
\[E(XY) = E(X)E(Y)\] 

Substituting this into the covariance formula: 
\begin{align*}
    Cov(X, Y) &= E(XY) - E(X)E(Y) \\
    &= E(X)E(Y) - E(X)E(Y) \\
    &= 0
\end{align*}
Since covariance is zero, $X$ and $Y$ are uncorrelated.\\

\textbf{Implication of Zero Covariance}: 
    Zero covariance implies that there is no linear relationship between $X$ and $Y$. In other words, knowing the value of $X$ gives no information about the value of $Y$, and vice versa, which is consistent with the definition of independence.\\
    
The \textbf{correlation coefficient}, often denoted as $\rho$, is a normalized measure of the strength and direction of the linear relationship between two variables.\\

It's defined as \[\rho_{X,Y} = \frac{\text{Cov}(X, Y)}{\sigma_X \sigma_Y}\]
where $\sigma_X$ and $\sigma_Y$ are the standard deviations of $X$ and $Y$, respectively.\\

If $\text{Cov}(X, Y) = 0$, then $\rho_{X,Y} = 0$, indicating no linear correlation.\\

In summary, if two random variables are independent, they do not affect each other, and therefore, their covariance (and thus their correlation) is zero. This means independent variables are always uncorrelated. However, the reverse is not always true; uncorrelated variables are not necessarily independent.\\

Correlation is just a measure of covariance and linearity, whereas dependence measures something different.\\

\subsection{Example proving independence}

Suppose $X$ and $Y$ are r.v.s where $X$ can take values with 1 or 2 with equal probability, and $Y$ can take values 3 or 4 with equal probablity. Assume $X$ and $Y$ are independent.
\begin{align*}
E(X) &= \frac{1}{2}(1) + \frac{1}{2}(2) = 1.5 \\
E(Y) &= \frac{1}{2}(3) + \frac{1}{2}(4) = 3.5 \\
\end{align*}

$E(XY)$ can be calculated by considering all combinations of $X$ and $Y$\\
We have four combinations: (1, 3), (1, 4), (2, 3), and (2, 4). Each combination occurs with a probability of 1/4, since the probabilities of X and Y are each 1/2.

\begin{align*}
E(XY) &= \frac{1}{4}(1 \times 3) + \frac{1}{4}(1 \times 4) + \frac{1}{4}(2 \times 3) + \frac{1}{4}(2 \times 4) = 5.25 \\
\text{Cov}(X, Y) &= E(XY) - E(X)E(Y) = 5.25 - (1.5 \times 3.5) = 0
\end{align*}
Since the covariance is 0, it suggests that $X$ and $Y$ are uncorrelated


\section{Law of Large Numbers}
Assume i.i.d rvs $X_1$, $X_2$, $X_3 \cdots$ with mean $\mu$ and variance $\sigma^2$. \\
We take sample of size $n$ and define the sample mean as:
\begin{align*}
\bar{X}_n = \frac{X_1 + X_2 + \ldots + X_n}{n}  
\end{align*}

\begin{tcolorbox}
Assume the daily temperatures for five consecutive days are represented by random variables $X_1, X_2, X_3, X_4,$ and $X_5$. These could be, for instance:

$X_1 = 3^\circ\text{C}$ (Temperature on Day 1)

$X_2 = 5^\circ\text{C}$ (Temperature on Day 2)

$X_3 = 7^\circ\text{C}$ (Temperature on Day 3)

$X_4 = 2^\circ\text{C}$ (Temperature on Day 4)

$X_5 = 4^\circ\text{C}$ (Temperature on Day 5)

The average temperature over these five days, denoted as $\bar{X}_5$, is calculated as:

\[
\bar{X}_5 = \frac{X_1 + X_2 + X_3 + X_4 + X_5}{5} = \frac{3 + 5 + 7 + 2 + 4}{5} = 4.2^\circ\text{C}
\]

\end{tcolorbox}

This sample mean is itself an r.v. \\
What are its expectation and variance? \\
\\
\textbf{Expectation}
\begin{align*}
E(\bar{X}_n) &= E\left(\frac{1}{n}(X_1 + \ldots + X_n)\right) \\
&= \frac{1}{n}(E(X_1) + \ldots + E(X_n)) \\
&= \frac{1}{n}(\mu_1 + \ldots + \mu_n) \\
&= \frac{1}{n}(n\mu) \\
&= \mu
\end{align*}

\textbf{Variance}
\begin{align*}
\text{Var}(\bar{X}_n) &= \frac{1}{n^2} \text{Var}(X_1 + \ldots + X_n) \\
&= \frac{\sigma^2}{n}
\end{align*}

\begin{tcolorbox}
    Law of Large Numbers: as $n$ grows large, the sample mean $\bar{X}$ converges to the true mean $\mu$.
    \begin{itemize}
        \item Sample mean (if i.i.d): $\bar{X}_n = \frac{X_1 + X_2 + \ldots + X_n}{n}  $
        \item sample mean is itself r.v.: \begin{itemize}
         \item Expectation = $\mu$ (i.e. the population mean - the sample mean and  the population mean converge as $n$ grows)
        \item Variance = $(\frac{\sigma}{n})^2 = \frac{\sigma^2}{n}$
        \item Standard Deviation $ = \sqrt{\frac{\sigma^2}{n}} = \frac{\sigma}{\sqrt{n}}$
        \end{itemize}
    \end{itemize}
\end{tcolorbox}


Law of Large Numbers: as $n$ grows large, the sample mean $\bar{X}$ converges to the true mean $\mu$.\\
\\
NB: LLN does not contradict a coin toss (or other r.v. being memoryless: convergence takes place through swamping: past tosses are “swamped” by the infinitely many tosses yet to come.\\

\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{Screenshot 2024-01-25 at 11.19.54.png}
    \caption{Enter Caption}
    \label{fig:enter-label}
\end{figure}

\section{Central Limit Theorem}
\textbf{CLT = that the standardised sample mean (standardised $\bar{X}$) converges in distribution to the standard Normal as $n \rightarrow \infty$}

i.e. given a sufficiently large sample size, the sampling distribution of the sample means will be approximately normally distributed, regardless of the shape of the population distribution\\

\subsection{Calculating the Standardised Sample Mean}

\begin{tcolorbox}
Calculating the Standardised Sample Mean: \begin{enumerate}
    \item subtract expectation  $\mu$
    \item dividing by  standard deviation $ \frac{\sigma}{\sqrt{n}}$
\end{enumerate}
\textit{NB: the variance of the sampling distribution as defined above was $\frac{\sigma^2}{n}$ so the standard deviation is in turn the square root of that:  $\sqrt{\frac{\sigma^2}{n}} \rightarrow \frac{\sigma}{\sqrt{n}}$}

\subsection{Convergence to Standard Normal}

LLN says this quantity (the Standardised Sample Mean) converges in distribution to the Standard Normal as $\n \rightarrow \infty$:
\begin{align*}
\frac{\bar{X}_n - \mu}{\frac{\sigma}{\sqrt{n}}} = \sqrt{n} \left(\frac{\bar{X}_n - \mu}{\sigma}\right)
\end{align*}
\end{tcolorbox}

\begin{align}
    \sqrt{n}\left(\frac{\bar{X}_n - \mu}{\sigma}\right) \xrightarrow{d} \mathcal{N}(0, 1)
\end{align}

This holds regardless of distribution of $X$s

\begin{figure}[H]
      \centering
      \includegraphics[width=1\linewidth]{Mathematics for Data Science - Lecture 7_ Continuous Random Variables (cont.).pdf}
      \caption{}
      \label{fig:enter-label}
  \end{figure}

\begin{tcolorbox}
    This shows: \begin{itemize}
        \item for sample size $n=1$ it is just the PDF of the underlying distribution:
        \item practical e.g: if we were sampling number of passengers in cars which we take to be geometrically distributed, when sample size $n=1$, each sample mean is just the value of the underlying r.v. so it acts as just taking observations of the underlying r.v.
        \item when $n > 1$ we are now starting to get sample means which will move towards the centre of the distribution (which is the true population mean $\mu$ according to Law of Large Numbers).
        \item the distribution of these sample means approach Normal distribution as we increase $n$

        \item the variance of this sample mean distribution is the squared population standard deviation divided by $n$: Variance = $\frac{\sigma^2}{n}$. This formula shows that the variance of the sample means decreases as the sample size $n$ increases. In other words, larger samples lead to a narrower spread in the sampling distribution of the mean.

        \item NB: $n=30$ is the magic number: here everything starts to approach Normal.
    \end{itemize}
\end{tcolorbox}

\subsection{Further Notes on CLT}

NB: you can un-standardise the sample mean to restate the CLT as: 
\begin{align*}
    \bar{X}_n \xrightarrow{d} \mathcal{N}\left(\mu, \frac{\sigma^2}{n}\right) \quad \text{as} \quad n \to \infty
\end{align*}

NB: CLT works just as well for sum rather than the mean:
\begin{align*}
    \sum_{i=1}^{n} X_i \xrightarrow{d} \mathcal{N}(n\mu, n\sigma^2) \quad \text{as} \quad n \to \infty
\end{align*}

While CLT works for any distribution with finite mean and variance, underlying distirbution matters for how large $n$ has to be before the Normal approximation starts to look accurate. \\
\\
As shortcut: can write CLT in approximate form:
\[\bar{X}_n \approx \mathcal{N}(\mu, \frac{\sigma^2}{n})\]

\subsection{Example: Normal Approximation to the Binomial}
Recalling that the Binomial $(n, p)$ is the sum of $n$ Bernoullis with
probability $p$, we can even use the Normal distribution to
approximate the Binomial.
\[\sum_{i=1}^{n} X_i \approx \mathcal{N}(n\mu, n\sigma^2)\]

and recalling that the mean of a Bernoulli is $p$ and its variance is
$p(1 − p)$, we can use the CLT to say:
\[\sum_{i=1}^{n} X_i \approx \mathcal{N}(np, np(1 - p))\]

\end{document}