\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Philosophical Foundations: The Computational Theory of Mind}{2}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Cybernetics and Early Neural Models (1940s--1950s)}{3}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}The Birth of Artificial Intelligence (1956)}{5}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}The Perceptron and Supervised Learning}{6}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}The First AI Winter (1970s)}{7}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Knowledge-Based Systems and Expert Systems (1980s)}{8}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}The Connectionist Revival (1980s)}{10}{section.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8}The Second AI Winter (Late 1980s--Early 1990s)}{11}{section.8}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {9}Unsupervised Learning}{11}{section.9}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {10}Reinforcement Learning}{12}{section.10}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {11}The Statistical ML Renaissance (1990s--2000s)}{13}{section.11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.1}Support Vector Machines}{14}{subsection.11.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.2}Ensemble Methods}{14}{subsection.11.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.3}Probabilistic Graphical Models}{15}{subsection.11.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {12}The Deep Learning Revolution (2010s--Present)}{15}{section.12}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.1}Why Now? The Convergence of Factors}{15}{subsection.12.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.2}Transformers and Language Models}{16}{subsection.12.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {13}Summary: Recurring Themes in AI History}{18}{section.13}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {14}The Optimisation Framework}{19}{section.14}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {15}Maximum Likelihood Estimation (MLE)}{19}{section.15}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {15.1}The i.i.d.\ Assumption}{20}{subsection.15.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {15.2}From Likelihood to Negative Log-Likelihood}{20}{subsection.15.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {16}Linear Regression as MLE}{21}{section.16}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {16.1}The Probabilistic Model}{21}{subsection.16.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {16.2}Deriving the NLL}{22}{subsection.16.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {17}Residual Sum of Squares and the OLS Solution}{23}{section.17}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {17.1}The Analytic Solution}{23}{subsection.17.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {17.2}What OLS Gives Us}{24}{subsection.17.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {18}KL Divergence and Cross-Entropy}{25}{section.18}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {18.1}Kullback-Leibler Divergence}{25}{subsection.18.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {19}Variance of the OLS Estimator}{26}{section.19}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {19.1}Deriving the Variance}{26}{subsection.19.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {19.2}Homoskedastic Errors}{27}{subsection.19.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {19.3}Heteroskedastic Errors}{27}{subsection.19.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {20}Bayesian Inference and MAP Estimation}{28}{section.20}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Bayes' Rule: combining prior beliefs with observed data to form posterior beliefs. The posterior balances what we believed before (prior) with what the data tells us (likelihood).}}{28}{figure.1}\protected@file@percent }
\newlabel{fig:bayes-rule}{{1}{28}{Bayes' Rule: combining prior beliefs with observed data to form posterior beliefs. The posterior balances what we believed before (prior) with what the data tells us (likelihood)}{figure.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {20.1}Frequentist vs Bayesian Perspectives}{29}{subsection.20.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {20.2}Maximum A Posteriori (MAP) Estimation}{29}{subsection.20.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {20.3}Interpreting Uncertainty: Credible vs Confidence Intervals}{30}{subsection.20.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {21}Empirical Risk Minimisation}{30}{section.21}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {21.1}Common Loss Functions}{31}{subsection.21.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {21.2}The Problem with 0-1 Loss}{31}{subsection.21.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Surrogate loss functions compared to 0-1 loss. All surrogate losses upper bound the 0-1 loss while being differentiable and (mostly) convex.}}{32}{figure.2}\protected@file@percent }
\newlabel{fig:surrogate-losses}{{2}{32}{Surrogate loss functions compared to 0-1 loss. All surrogate losses upper bound the 0-1 loss while being differentiable and (mostly) convex}{figure.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {21.3}Classification Errors}{32}{subsection.21.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Confusion matrix structure showing the four possible outcomes of binary classification.}}{33}{figure.3}\protected@file@percent }
\newlabel{fig:confusion-matrix}{{3}{33}{Confusion matrix structure showing the four possible outcomes of binary classification}{figure.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {22}Logistic Regression}{33}{section.22}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The sigmoid (logistic) function $\sigma (z) = 1/(1 + e^{-z})$ maps any real number to $(0, 1)$. At $z=0$, $\sigma (0) = 0.5$. The function saturates at 0 and 1 for large $|z|$.}}{34}{figure.4}\protected@file@percent }
\newlabel{fig:sigmoid}{{4}{34}{The sigmoid (logistic) function $\sigma (z) = 1/(1 + e^{-z})$ maps any real number to $(0, 1)$. At $z=0$, $\sigma (0) = 0.5$. The function saturates at 0 and 1 for large $|z|$}{figure.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {22.1}Training: Binary Cross-Entropy}{34}{subsection.22.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {22.2}Decision Boundaries}{35}{subsection.22.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Linear decision boundary separating two classes. The boundary is the hyperplane where $x^\top \beta = 0$. Points on one side are classified as positive, points on the other as negative.}}{35}{figure.5}\protected@file@percent }
\newlabel{fig:decision-boundary}{{5}{35}{Linear decision boundary separating two classes. The boundary is the hyperplane where $x^\top \beta = 0$. Points on one side are classified as positive, points on the other as negative}{figure.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {23}The Bias-Variance Tradeoff}{35}{section.23}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {23.1}Example: Shrinkage Estimators}{36}{subsection.23.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {24}Summary}{38}{section.24}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {25}Supervised Learning: A Quick Recap}{39}{section.25}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Regression vs classification in supervised learning. Left: regression fits a continuous function through the data. Right: classification finds a decision boundary separating classes.}}{39}{figure.6}\protected@file@percent }
\newlabel{fig:regression-vs-classification}{{6}{39}{Regression vs classification in supervised learning. Left: regression fits a continuous function through the data. Right: classification finds a decision boundary separating classes}{figure.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {25.1}OLS Recap}{39}{subsection.25.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {26}From Linear to Nonlinear: Polynomial Regression}{40}{section.26}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {26.1}The Problem: Choosing $M$}{40}{subsection.26.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Effect of polynomial degree on fit. Higher degrees fit training data perfectly but generalise poorly. This illustrates the fundamental tension in model selection.}}{40}{figure.7}\protected@file@percent }
\newlabel{fig:choosing-M}{{7}{40}{Effect of polynomial degree on fit. Higher degrees fit training data perfectly but generalise poorly. This illustrates the fundamental tension in model selection}{figure.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {26.2}Why Polynomials Are Attractive (In Theory)}{41}{subsection.26.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {26.3}Challenges with Polynomial Regression: Numerical Instability}{41}{subsection.26.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Numerical instability in high-degree polynomials: small perturbations in data cause large changes in the fitted curve, particularly near the edges of the domain.}}{42}{figure.8}\protected@file@percent }
\newlabel{fig:numerical-instability}{{8}{42}{Numerical instability in high-degree polynomials: small perturbations in data cause large changes in the fitted curve, particularly near the edges of the domain}{figure.8}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {26.3.1}The Condition Number}{42}{subsubsection.26.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {27}Decomposing Prediction Error}{43}{section.27}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {27.1}The Bias-Variance Tradeoff: A First Look}{43}{subsection.27.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Bias-variance tradeoff: as model complexity increases, bias decreases but variance increases. The optimal complexity minimises total error (MSE). This U-shaped curve is fundamental to model selection.}}{43}{figure.9}\protected@file@percent }
\newlabel{fig:bias-variance-MSE}{{9}{43}{Bias-variance tradeoff: as model complexity increases, bias decreases but variance increases. The optimal complexity minimises total error (MSE). This U-shaped curve is fundamental to model selection}{figure.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {27.2}Evaluation Metrics: Defining ``Risk''}{43}{subsection.27.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Confusion matrix showing Type I errors (false positives) and Type II errors (false negatives). These frame the loss in terms of incorrect predictions.}}{44}{figure.10}\protected@file@percent }
\newlabel{fig:confusion-matrix}{{10}{44}{Confusion matrix showing Type I errors (false positives) and Type II errors (false negatives). These frame the loss in terms of incorrect predictions}{figure.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {27.3}Population Risk vs Empirical Risk}{45}{subsection.27.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {27.4}Three Levels of Optimality}{46}{subsection.27.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {27.5}Approximation vs Estimation Error}{47}{subsection.27.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {27.5.1}Approximation Error}{47}{subsubsection.27.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {27.5.2}Estimation Error}{48}{subsubsection.27.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {27.5.3}The Fundamental Tradeoff}{48}{subsubsection.27.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {27.6}Estimating Generalisation Error}{49}{subsection.27.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {28}Regularisation}{50}{section.28}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {28.1}The Mechanics of Regularisation}{50}{subsection.28.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {28.2}Uses of Regularisation}{51}{subsection.28.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {28.3}Ridge Regression (L2 Regularisation)}{51}{subsection.28.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {28.3.1}Ridge as Rescaled OLS}{51}{subsubsection.28.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {28.4}Lasso Regression (L1 Regularisation)}{51}{subsection.28.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {28.4.1}Lasso as Soft Thresholding}{52}{subsubsection.28.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {28.5}Elastic Net}{52}{subsection.28.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {29}Multiple Perspectives on Regularisation}{52}{section.29}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {29.1}Perspective 1: Necessity (Invertibility)}{53}{subsection.29.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {29.2}Perspective 2: Bias-Variance Tradeoff}{53}{subsection.29.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {29.3}Perspective 3: Bayesian Interpretation (MAP)}{54}{subsection.29.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Bayes' theorem: the posterior $p(\theta | \mathcal  {D})$ is proportional to the prior $p(\theta )$ times the likelihood $p(\mathcal  {D} | \theta )$. Regularisation enters through the prior.}}{54}{figure.11}\protected@file@percent }
\newlabel{fig:bayes-theorem}{{11}{54}{Bayes' theorem: the posterior $p(\theta | \mathcal {D})$ is proportional to the prior $p(\theta )$ times the likelihood $p(\mathcal {D} | \theta )$. Regularisation enters through the prior}{figure.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {29.4}Perspective 4: Geometric Interpretation}{55}{subsection.29.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {29.5}Perspective 5: Measurement Error}{55}{subsection.29.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {30}Model Selection and Validation}{56}{section.30}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {30.1}Validation Sets}{56}{subsection.30.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {30.2}Cross-Validation}{56}{subsection.30.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Cross-validation error as a function of model complexity (indexed on x-axis). The U-shaped curve shows the bias-variance tradeoff: too simple models underfit (high error on left), too complex models overfit (high error on right). The optimal complexity minimises CV error.}}{57}{figure.12}\protected@file@percent }
\newlabel{fig:cv-error}{{12}{57}{Cross-validation error as a function of model complexity (indexed on x-axis). The U-shaped curve shows the bias-variance tradeoff: too simple models underfit (high error on left), too complex models overfit (high error on right). The optimal complexity minimises CV error}{figure.12}{}}
\@writefile{toc}{\contentsline {section}{\numberline {31}Regularised Polynomial Regression}{57}{section.31}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Regularised polynomial regression. The right panels show L2 regularisation producing smooth fits even with high-degree polynomials, avoiding the oscillatory behaviour of unregularised fits. Panel (c) is particularly notable: a smooth model that does not exhibit typical polynomial wiggles. Regularisation allows us to get the best of both worlds-expressivity without instability.}}{58}{figure.13}\protected@file@percent }
\newlabel{fig:regularised-polynomial}{{13}{58}{Regularised polynomial regression. The right panels show L2 regularisation producing smooth fits even with high-degree polynomials, avoiding the oscillatory behaviour of unregularised fits. Panel (c) is particularly notable: a smooth model that does not exhibit typical polynomial wiggles. Regularisation allows us to get the best of both worlds-expressivity without instability}{figure.13}{}}
\@writefile{toc}{\contentsline {section}{\numberline {32}Summary}{59}{section.32}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {33}Cross-Validation}{60}{section.33}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {33.1}The Core Problem}{60}{subsection.33.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {33.2}K-Fold Cross-Validation}{61}{subsection.33.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces 5-fold cross-validation: the data is split into 5 folds, and each fold serves as the validation set exactly once while the remaining 4 folds form the training set.}}{61}{figure.14}\protected@file@percent }
\newlabel{fig:kfold-cv}{{14}{61}{5-fold cross-validation: the data is split into 5 folds, and each fold serves as the validation set exactly once while the remaining 4 folds form the training set}{figure.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {33.3}LOOCV for Linear Regression}{62}{subsection.33.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {33.4}The One Standard Error Rule}{63}{subsection.33.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {33.5}The Optimism of Training Error}{63}{subsection.33.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {33.6}Grouped Cross-Validation}{64}{subsection.33.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces \textbf  {Left}: Group K-fold keeps all observations from a given group together in the same fold across all CV iterations. \textbf  {Right}: Time series split respects temporal ordering, always using past data to predict future data.}}{64}{figure.15}\protected@file@percent }
\newlabel{fig:cv-grouping}{{15}{64}{\textbf {Left}: Group K-fold keeps all observations from a given group together in the same fold across all CV iterations. \textbf {Right}: Time series split respects temporal ordering, always using past data to predict future data}{figure.15}{}}
\@writefile{toc}{\contentsline {section}{\numberline {34}Frequentist vs Bayesian Risk}{65}{section.34}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {35}Generalisation Bounds}{66}{section.35}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {35.1}Error Decomposition Recap}{67}{subsection.35.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {35.2}Uses of Bounds}{68}{subsection.35.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {35.3}Building Blocks: Concentration Inequalities}{68}{subsection.35.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Union bound: $P(A \cup B) \leq P(A) + P(B)$. The bound is loose when events overlap significantly (we double-count the intersection), but it avoids computing complex intersections.}}{69}{figure.16}\protected@file@percent }
\newlabel{fig:union-bound}{{16}{69}{Union bound: $P(A \cup B) \leq P(A) + P(B)$. The bound is loose when events overlap significantly (we double-count the intersection), but it avoids computing complex intersections}{figure.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {35.4}First Generalisation Bound}{69}{subsection.35.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {35.5}Limitations of This Bound}{70}{subsection.35.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {36}Measuring Hypothesis Class Complexity}{70}{section.36}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {36.1}Intrinsic Dimensionality and the Manifold Hypothesis}{71}{subsection.36.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {36.2}VC Dimension}{71}{subsection.36.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Linear classifiers in 2D can shatter 3 points in general position: for any labelling of the 3 points, we can find a line that separates them correctly. Hence $\text  {VC}(\text  {linear classifiers in } \mathbb  {R}^2) \geq 3$.}}{72}{figure.17}\protected@file@percent }
\newlabel{fig:shattering-1}{{17}{72}{Linear classifiers in 2D can shatter 3 points in general position: for any labelling of the 3 points, we can find a line that separates them correctly. Hence $\text {VC}(\text {linear classifiers in } \mathbb {R}^2) \geq 3$}{figure.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces However, 4 points in general position \textit  {cannot} be shattered by linear classifiers in 2D. The ``XOR'' labelling (opposite corners have the same label) cannot be achieved by any line. Hence $\text  {VC}(\text  {linear classifiers in } \mathbb  {R}^2) = 3$.}}{72}{figure.18}\protected@file@percent }
\newlabel{fig:shattering-2}{{18}{72}{However, 4 points in general position \textit {cannot} be shattered by linear classifiers in 2D. The ``XOR'' labelling (opposite corners have the same label) cannot be achieved by any line. Hence $\text {VC}(\text {linear classifiers in } \mathbb {R}^2) = 3$}{figure.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces The function $f(x) = \text  {sign}(\sin (\omega x))$ has only \textbf  {one parameter} ($\omega $) but \textbf  {infinite VC dimension}. By choosing $\omega $ large enough, the function can oscillate rapidly enough to shatter arbitrarily many points on the real line.}}{73}{figure.19}\protected@file@percent }
\newlabel{fig:shattering-3}{{19}{73}{The function $f(x) = \text {sign}(\sin (\omega x))$ has only \textbf {one parameter} ($\omega $) but \textbf {infinite VC dimension}. By choosing $\omega $ large enough, the function can oscillate rapidly enough to shatter arbitrarily many points on the real line}{figure.19}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {36.3}The VC Bound}{73}{subsection.36.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {37}Structural Risk Minimisation}{74}{section.37}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {38}Generalisation in Linear Regression}{74}{section.38}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {38.1}OLS Estimation Error}{74}{subsection.38.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {38.2}Singular Value Decomposition (SVD)}{74}{subsection.38.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces SVD dimensions: $X$ is $n \times p$, decomposed into $U$ ($n \times r$), $\Sigma $ ($r \times r$), and $V^\top $ ($r \times p$).}}{75}{figure.20}\protected@file@percent }
\newlabel{fig:svd-dims}{{20}{75}{SVD dimensions: $X$ is $n \times p$, decomposed into $U$ ($n \times r$), $\Sigma $ ($r \times r$), and $V^\top $ ($r \times p$)}{figure.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces Geometric interpretation of SVD: $V^\top $ rotates input vectors, $\Sigma $ scales along principal axes, $U$ rotates to output space. SVD decomposes any linear transformation into rotation-scale-rotation.}}{75}{figure.21}\protected@file@percent }
\newlabel{fig:svd-transform}{{21}{75}{Geometric interpretation of SVD: $V^\top $ rotates input vectors, $\Sigma $ scales along principal axes, $U$ rotates to output space. SVD decomposes any linear transformation into rotation-scale-rotation}{figure.21}{}}
\@writefile{toc}{\contentsline {section}{\numberline {39}Low vs High Dimensional Regimes}{76}{section.39}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {39.1}Low-Dimensional Regime: $p \ll n$}{76}{subsection.39.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces Low-dimensional regime: as $n$ increases, the excess risk decreases smoothly. More data leads to rapid improvement in generalisation.}}{77}{figure.22}\protected@file@percent }
\newlabel{fig:low-dim}{{22}{77}{Low-dimensional regime: as $n$ increases, the excess risk decreases smoothly. More data leads to rapid improvement in generalisation}{figure.22}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {39.2}High-Dimensional Regime: $p > n$}{77}{subsection.39.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces High-dimensional regime: the relationship between model complexity and error follows different dynamics. Note the different scaling compared to the low-dimensional case.}}{78}{figure.23}\protected@file@percent }
\newlabel{fig:high-dim}{{23}{78}{High-dimensional regime: the relationship between model complexity and error follows different dynamics. Note the different scaling compared to the low-dimensional case}{figure.23}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {39.3}The Interpolation Threshold and Double Descent}{78}{subsection.39.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {24}{\ignorespaces Classical bias-variance tradeoff: total error (black) is the sum of squared bias (red, decreasing with complexity) and variance (blue, increasing with complexity). The optimal complexity balances these. Double descent challenges this picture in the overparameterised regime.}}{79}{figure.24}\protected@file@percent }
\newlabel{fig:bias-variance}{{24}{79}{Classical bias-variance tradeoff: total error (black) is the sum of squared bias (red, decreasing with complexity) and variance (blue, increasing with complexity). The optimal complexity balances these. Double descent challenges this picture in the overparameterised regime}{figure.24}{}}
\@writefile{toc}{\contentsline {section}{\numberline {40}Bias-Variance Decomposition}{80}{section.40}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {25}{\ignorespaces Effect of model complexity on fit. (a) Low complexity (degree 2): underfitting, high bias. (b) Medium complexity (degree 14): good fit. (c) High complexity (degree 20): overfitting, high variance. (d) Training error decreases monotonically with complexity; test error is U-shaped.}}{80}{figure.25}\protected@file@percent }
\newlabel{fig:model-complexity}{{25}{80}{Effect of model complexity on fit. (a) Low complexity (degree 2): underfitting, high bias. (b) Medium complexity (degree 14): good fit. (c) High complexity (degree 20): overfitting, high variance. (d) Training error decreases monotonically with complexity; test error is U-shaped}{figure.25}{}}
\@writefile{toc}{\contentsline {section}{\numberline {41}Summary}{82}{section.41}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {42}Recap: OLS in Different Regimes}{83}{section.42}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {42.1}Low-Dimensional Regime: $p \ll n$}{83}{subsection.42.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {42.2}High-Dimensional Regime: $p \gg n$}{84}{subsection.42.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {42.3}Comparing the Two Regimes}{85}{subsection.42.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {42.4}The Regularisation Paradox}{85}{subsection.42.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {43}The Double Descent Phenomenon}{86}{section.43}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {26}{\ignorespaces Double descent: test error peaks at the interpolation threshold ($p \approx n$), then \textbf  {decreases} as $p \gg n$. This contradicts the classical U-shaped bias-variance tradeoff curve.}}{86}{figure.26}\protected@file@percent }
\newlabel{fig:double-descent}{{26}{86}{Double descent: test error peaks at the interpolation threshold ($p \approx n$), then \textbf {decreases} as $p \gg n$. This contradicts the classical U-shaped bias-variance tradeoff curve}{figure.26}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {43.1}The Three Regimes}{86}{subsection.43.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {43.2}Explaining the Phenomenon: The Manifold Hypothesis}{87}{subsection.43.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {44}Modern Analysis: The $k$-Split Perspective}{88}{section.44}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {44.1}The Interpolation Threshold}{88}{subsection.44.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {27}{\ignorespaces At $p \approx n$, the model achieves perfect interpolation-zero training error. The system $X\beta = y$ transitions from overdetermined (unique solution, typically positive training error) to underdetermined (infinitely many solutions, zero training error).}}{88}{figure.27}\protected@file@percent }
\newlabel{fig:interpolation-threshold}{{27}{88}{At $p \approx n$, the model achieves perfect interpolation-zero training error. The system $X\beta = y$ transitions from overdetermined (unique solution, typically positive training error) to underdetermined (infinitely many solutions, zero training error)}{figure.27}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {44.2}SVD to the Rescue}{88}{subsection.44.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {44.3}Splitting Signal from Noise: The $k$-Split}{89}{subsection.44.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {28}{\ignorespaces SVD naturally orders dimensions by importance. The first $k$ singular values (left portion) capture structured signal; the remainder (right portion) capture noise-like variation. The split occurs where singular values transition from ``large'' to ``small.''}}{90}{figure.28}\protected@file@percent }
\newlabel{fig:k-split}{{28}{90}{SVD naturally orders dimensions by importance. The first $k$ singular values (left portion) capture structured signal; the remainder (right portion) capture noise-like variation. The split occurs where singular values transition from ``large'' to ``small.''}{figure.28}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {44.4}Two Perspectives on the Manifold Hypothesis}{90}{subsection.44.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {45}Benign Overfitting: When Interpolation Works}{91}{section.45}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {45.1}The Risk Bound}{92}{subsection.45.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {45.2}Understanding the Two Terms}{92}{subsection.45.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {45.3}When is Overfitting Benign?}{93}{subsection.45.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {45.4}Why SVD Makes This Automatic}{93}{subsection.45.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {46}Implications for High-Dimensional Models}{94}{section.46}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {46.1}The Importance of Data Structure}{95}{subsection.46.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {47}Practical Implications}{95}{section.47}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {48}Summary}{97}{section.48}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {49}Motivation: A New Way to Think About Similarity}{98}{section.49}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {50}An Alternative View of Regression}{98}{section.50}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {50.1}Two Equivalent Formulations of Ridge Regression}{98}{subsection.50.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {50.2}What Do These Matrices Represent?}{99}{subsection.50.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {50.3}Similarity as Dot Product}{99}{subsection.50.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {29}{\ignorespaces The dot product $x \cdot y = \|x\|\|y\|\cos \theta $ captures both magnitude and direction. When $\theta $ is small (similar directions), $\cos \theta $ is large and the dot product is large.}}{99}{figure.29}\protected@file@percent }
\newlabel{fig:dot-product}{{29}{99}{The dot product $x \cdot y = \|x\|\|y\|\cos \theta $ captures both magnitude and direction. When $\theta $ is small (similar directions), $\cos \theta $ is large and the dot product is large}{figure.29}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {50.4}Regression as Similarity-Weighted Averaging}{100}{subsection.50.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {50.5}The Importance of Defining Similarity Correctly}{101}{subsection.50.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {30}{\ignorespaces All four datasets have the same correlation coefficient, but their structures differ dramatically. Linear correlation (based on Euclidean distance) cannot distinguish them.}}{101}{figure.30}\protected@file@percent }
\newlabel{fig:covariance-issues}{{30}{101}{All four datasets have the same correlation coefficient, but their structures differ dramatically. Linear correlation (based on Euclidean distance) cannot distinguish them}{figure.30}{}}
\@writefile{toc}{\contentsline {section}{\numberline {51}Feature Expansion and the Kernel Trick}{101}{section.51}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {51.1}Feature Expansion}{101}{subsection.51.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {51.2}The Computational Problem}{102}{subsection.51.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {51.3}The Kernel Trick}{102}{subsection.51.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {51.4}The Gram Matrix}{103}{subsection.51.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {52}Common Kernels}{103}{section.52}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {52.1}Polynomial Kernel}{103}{subsection.52.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {52.2}Gaussian (RBF) Kernel}{104}{subsection.52.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {52.2.1}The RBF Kernel is Infinite-Dimensional}{105}{subsubsection.52.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {31}{\ignorespaces Left: Linear regression (rigid, global). Right: RBF kernel regression allows locally-weighted similarity, adapting flexibly to the data structure.}}{106}{figure.31}\protected@file@percent }
\newlabel{fig:kernels-comparison}{{31}{106}{Left: Linear regression (rigid, global). Right: RBF kernel regression allows locally-weighted similarity, adapting flexibly to the data structure}{figure.31}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {52.3}Other Common Kernels}{106}{subsection.52.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {53}Combining Kernels}{106}{section.53}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {32}{\ignorespaces Rules for constructing valid kernels from simpler ones. Any combination that preserves positive semi-definiteness yields a valid kernel.}}{107}{figure.32}\protected@file@percent }
\newlabel{fig:kernel-manipulation}{{32}{107}{Rules for constructing valid kernels from simpler ones. Any combination that preserves positive semi-definiteness yields a valid kernel}{figure.32}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {33}{\ignorespaces Combining kernels: Gaussian (local), periodic (cyclical), and mixed. Adjusting $\sigma ^2$ combines wide global structure with high-frequency local variation.}}{108}{figure.33}\protected@file@percent }
\newlabel{fig:combining-kernels}{{33}{108}{Combining kernels: Gaussian (local), periodic (cyclical), and mixed. Adjusting $\sigma ^2$ combines wide global structure with high-frequency local variation}{figure.33}{}}
\@writefile{toc}{\contentsline {section}{\numberline {54}Kernel Methods in Practice}{108}{section.54}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {54.1}Kernel Ridge Regression}{108}{subsection.54.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {54.2}K-Nearest Neighbours (KNN)}{109}{subsection.54.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {54.3}Comparing Global and Local Methods}{110}{subsection.54.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {34}{\ignorespaces Kernels can induce low-rank global structure (capturing overall trends) while allowing for local variation (adapting to fine-grained patterns). This balances smoothness with flexibility.}}{110}{figure.34}\protected@file@percent }
\newlabel{fig:kernel-structure-1}{{34}{110}{Kernels can induce low-rank global structure (capturing overall trends) while allowing for local variation (adapting to fine-grained patterns). This balances smoothness with flexibility}{figure.34}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {35}{\ignorespaces Different features may demand different notions of similarity. By combining or learning kernel parameters, models can discover which features (or combinations) are most indicative of similarity.}}{110}{figure.35}\protected@file@percent }
\newlabel{fig:kernel-structure-2}{{35}{110}{Different features may demand different notions of similarity. By combining or learning kernel parameters, models can discover which features (or combinations) are most indicative of similarity}{figure.35}{}}
\@writefile{toc}{\contentsline {section}{\numberline {55}The Curse of Dimensionality}{110}{section.55}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {55.1}Why Distance Fails in High Dimensions}{111}{subsection.55.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {36}{\ignorespaces To capture 10\% of data as $d$ increases, the neighbourhood radius $\epsilon $ must grow toward the boundary of the space.}}{111}{figure.36}\protected@file@percent }
\newlabel{fig:vol-dim-table}{{36}{111}{To capture 10\% of data as $d$ increases, the neighbourhood radius $\epsilon $ must grow toward the boundary of the space}{figure.36}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {37}{\ignorespaces Volume concentration in high dimensions: most volume lies near the boundary, and the concept of ``local neighbourhood'' becomes meaningless.}}{112}{figure.37}\protected@file@percent }
\newlabel{fig:vol-dim-graph}{{37}{112}{Volume concentration in high dimensions: most volume lies near the boundary, and the concept of ``local neighbourhood'' becomes meaningless}{figure.37}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {55.2}Implications for Machine Learning}{112}{subsection.55.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {56}Summary}{113}{section.56}\protected@file@percent }
\newlabel{fig:kernel-meme}{{56}{113}{Summary}{Item.177}{}}
\@writefile{toc}{\contentsline {section}{\numberline {57}Machine Learning in Context}{114}{section.57}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {38}{\ignorespaces ML systems exist within broader social and institutional contexts. The model is just one component in a sociotechnical system that includes data collection, deployment decisions, and feedback loops.}}{114}{figure.38}\protected@file@percent }
\newlabel{fig:ml-context}{{38}{114}{ML systems exist within broader social and institutional contexts. The model is just one component in a sociotechnical system that includes data collection, deployment decisions, and feedback loops}{figure.38}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {57.1}Encoded Bias: Which Patterns Should We Replicate?}{115}{subsection.57.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {39}{\ignorespaces Language models encode societal biases. This example from Google Translate (Turkish to English) shows how gender-neutral Turkish pronouns are translated using stereotypical gender associations-``he'' for doctor/soldier, ``she'' for nurse/teacher. Which patterns should we replicate, and which not?}}{115}{figure.39}\protected@file@percent }
\newlabel{fig:bias-example}{{39}{115}{Language models encode societal biases. This example from Google Translate (Turkish to English) shows how gender-neutral Turkish pronouns are translated using stereotypical gender associations-``he'' for doctor/soldier, ``she'' for nurse/teacher. Which patterns should we replicate, and which not?}{figure.39}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {57.2}Feedback Loops: Predictions as Self-Fulfilling Prophecies}{115}{subsection.57.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {58}Types of Harm}{116}{section.58}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {59}What is Fairness?}{117}{section.59}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {59.1}Legitimacy: The Prior Question}{118}{subsection.59.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {59.2}Relative Treatment: Fairness Metrics}{119}{subsection.59.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {59.3}Procedural Fairness: The Right to Reasons}{119}{subsection.59.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {60}Types of Automation}{119}{section.60}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {61}Problems in Data-Driven Systems}{121}{section.61}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {61.1}The Healthcare Algorithm Example}{123}{subsection.61.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {62}Agency, Recourse, and Culpability}{123}{section.62}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {62.1}The Problem of Recourse}{124}{subsection.62.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {62.2}Culpability: Who Is Responsible?}{124}{subsection.62.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {63}The Limits of Technical Solutions}{125}{section.63}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {64}Looking Ahead: Quantitative Fairness}{126}{section.64}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {65}Summary}{127}{section.65}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {66}Classification and Risk Scores}{128}{section.66}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {66.1}Risk Scores and Estimation}{128}{subsection.66.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {66.2}Ideal Model versus Reality}{128}{subsection.66.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {67}Evaluating Classifiers}{128}{section.67}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {67.1}Accuracy and Its Limitations}{128}{subsection.67.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {67.2}Cost-Sensitive Learning}{129}{subsection.67.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {40}{\ignorespaces Confusion matrix with cost annotations. The cost matrix $c_{ij}$ weights different outcomes: $c_{00}$ and $c_{11}$ represent correct predictions (typically zero cost), while $c_{01}$ (false positive) and $c_{10}$ (false negative) represent errors with potentially different costs.}}{129}{figure.40}\protected@file@percent }
\newlabel{fig:confusion-matrix}{{40}{129}{Confusion matrix with cost annotations. The cost matrix $c_{ij}$ weights different outcomes: $c_{00}$ and $c_{11}$ represent correct predictions (typically zero cost), while $c_{01}$ (false positive) and $c_{10}$ (false negative) represent errors with potentially different costs}{figure.40}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {67.3}Receiver Operating Characteristic (ROC) Curves}{130}{subsection.67.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {67.3.1}ROC Curve Construction}{130}{subsubsection.67.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {41}{\ignorespaces ROC curve interpretation. Bottom-left: nothing predicted positive (threshold = 1). Top-right: everything predicted positive (threshold = 0). Diagonal: random classifier (AUC = 0.5). Closer to the top-left corner indicates better performance.}}{131}{figure.41}\protected@file@percent }
\newlabel{fig:roc-curve}{{41}{131}{ROC curve interpretation. Bottom-left: nothing predicted positive (threshold = 1). Top-right: everything predicted positive (threshold = 0). Diagonal: random classifier (AUC = 0.5). Closer to the top-left corner indicates better performance}{figure.41}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {67.3.2}Model Comparison with ROC Curves}{131}{subsubsection.67.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {67.3.3}Area Under the Curve (AUC)}{131}{subsubsection.67.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {68}Discrimination in Classification}{132}{section.68}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {68.1}How Discrimination Arises}{132}{subsection.68.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {42}{\ignorespaces Accumulation of slight predictivity: Groups A and B may be similar on any single feature, making group membership hard to predict from one feature alone. But with many features, each slightly predictive of group membership, we can predict group membership extremely well.}}{133}{figure.42}\protected@file@percent }
\newlabel{fig:sensitive-features}{{42}{133}{Accumulation of slight predictivity: Groups A and B may be similar on any single feature, making group membership hard to predict from one feature alone. But with many features, each slightly predictive of group membership, we can predict group membership extremely well}{figure.42}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {68.2}Accumulation of Slight Predictivity}{133}{subsection.68.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {68.3}Approaches to Addressing Discrimination}{133}{subsection.68.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {69}Quantitative Fairness Criteria}{134}{section.69}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {69.1}Independence (Demographic Parity)}{134}{subsection.69.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {69.1.1}Implications for Fairness}{134}{subsubsection.69.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {69.2}Separation (Equalised Odds)}{135}{subsection.69.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {69.2.1}Equal Treatment Among Similarly Situated Individuals}{135}{subsubsection.69.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {43}{\ignorespaces Separation requires operating at the same point on the ROC curve for both groups. Only the intersection of ROC curves for different groups satisfies separation-we must select a threshold that achieves equal TPR and FPR across groups.}}{136}{figure.43}\protected@file@percent }
\newlabel{fig:roc-separation}{{43}{136}{Separation requires operating at the same point on the ROC curve for both groups. Only the intersection of ROC curves for different groups satisfies separation-we must select a threshold that achieves equal TPR and FPR across groups}{figure.43}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {44}{\ignorespaces When groups have different base rates (different proportions of actual positives), achieving equal error rates typically requires using different thresholds for each group.}}{136}{figure.44}\protected@file@percent }
\newlabel{fig:separation-thresholds}{{44}{136}{When groups have different base rates (different proportions of actual positives), achieving equal error rates typically requires using different thresholds for each group}{figure.44}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {69.3}Sufficiency (Calibration)}{137}{subsection.69.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {45}{\ignorespaces Calibration: A model predicting 70\% probability should be correct approximately 70\% of the time, and this should hold for each group separately. A well-calibrated model has predicted probabilities that match observed frequencies.}}{137}{figure.45}\protected@file@percent }
\newlabel{fig:calibration}{{45}{137}{Calibration: A model predicting 70\% probability should be correct approximately 70\% of the time, and this should hold for each group separately. A well-calibrated model has predicted probabilities that match observed frequencies}{figure.45}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {69.3.1}Understanding Calibration}{137}{subsubsection.69.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {70}Impossibility Results}{138}{section.70}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {70.1}Independence versus Sufficiency}{138}{subsection.70.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {70.2}Independence versus Separation}{138}{subsection.70.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {46}{\ignorespaces Different fairness criteria lead to different operating points on ROC curves and therefore different models. The choice of fairness criterion determines which model and threshold combination is ``optimal.''}}{139}{figure.46}\protected@file@percent }
\newlabel{fig:roc-fairness}{{46}{139}{Different fairness criteria lead to different operating points on ROC curves and therefore different models. The choice of fairness criterion determines which model and threshold combination is ``optimal.''}{figure.46}{}}
\@writefile{toc}{\contentsline {section}{\numberline {71}Fairness is Not a Technical Problem}{139}{section.71}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {71.1}Different Criteria, Different Models}{139}{subsection.71.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {71.2}POSIWID: The Purpose of a System is What it Does}{139}{subsection.71.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {72}Summary}{140}{section.72}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {73}Decision Trees}{141}{section.73}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {47}{\ignorespaces Decision tree structure. Each path from root to leaf defines a region through a sequence of threshold conditions. Note the interaction effects captured along branches-for example, ``4 cylinders $\times $ continent'' or ``horsepower $\times $ low-med-high'' combinations.}}{141}{figure.47}\protected@file@percent }
\newlabel{fig:decision-tree}{{47}{141}{Decision tree structure. Each path from root to leaf defines a region through a sequence of threshold conditions. Note the interaction effects captured along branches-for example, ``4 cylinders $\times $ continent'' or ``horsepower $\times $ low-med-high'' combinations}{figure.47}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {73.1}Constructing a Decision Tree}{141}{subsection.73.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {73.2}Properties of Decision Trees}{143}{subsection.73.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {48}{\ignorespaces High variance illustrated: small changes in training data lead to very different tree structures. The same underlying relationship is modelled with dramatically different decision boundaries.}}{144}{figure.48}\protected@file@percent }
\newlabel{fig:high-variance}{{48}{144}{High variance illustrated: small changes in training data lead to very different tree structures. The same underlying relationship is modelled with dramatically different decision boundaries}{figure.48}{}}
\@writefile{toc}{\contentsline {section}{\numberline {74}Splitting Strategies}{144}{section.74}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {74.1}Categorical Features: Splitting by Unique Value}{145}{subsection.74.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {49}{\ignorespaces Categorical split: one branch per unique value. Each branch captures observations with that specific category.}}{145}{figure.49}\protected@file@percent }
\newlabel{fig:categorical-split}{{49}{145}{Categorical split: one branch per unique value. Each branch captures observations with that specific category}{figure.49}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {74.2}Continuous Features: Splitting by Threshold}{145}{subsection.74.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {50}{\ignorespaces Threshold split for a continuous feature. The prediction function is flat within each terminal node and discontinuous at boundaries-this is the characteristic ``stepped'' shape of tree predictions.}}{146}{figure.50}\protected@file@percent }
\newlabel{fig:threshold-split}{{50}{146}{Threshold split for a continuous feature. The prediction function is flat within each terminal node and discontinuous at boundaries-this is the characteristic ``stepped'' shape of tree predictions}{figure.50}{}}
\@writefile{toc}{\contentsline {section}{\numberline {75}Optimising Trees}{146}{section.75}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {75.1}The Non-Differentiability Challenge}{146}{subsection.75.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {75.2}Greedy Recursive Splitting}{147}{subsection.75.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {51}{\ignorespaces Greedy recursive splitting: at each node, we choose the locally optimal split without regard to how this affects future splits or the overall tree structure.}}{148}{figure.51}\protected@file@percent }
\newlabel{fig:greedy-splitting}{{51}{148}{Greedy recursive splitting: at each node, we choose the locally optimal split without regard to how this affects future splits or the overall tree structure}{figure.51}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {75.3}Evaluating Split Quality}{148}{subsection.75.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {75.4}Greed Eventually Overfits}{149}{subsection.75.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {76}Pruning}{149}{section.76}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {76.1}Pre-Pruning: Early Stopping}{149}{subsection.76.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {76.2}Post-Pruning: Cost-Complexity Pruning}{149}{subsection.76.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {52}{\ignorespaces Cost-complexity pruning: the fully grown tree (left) has near-zero training error but high test error. As we prune (increase $\alpha $), test error initially improves before eventually increasing again when the tree becomes too simple.}}{150}{figure.52}\protected@file@percent }
\newlabel{fig:pruning}{{52}{150}{Cost-complexity pruning: the fully grown tree (left) has near-zero training error but high test error. As we prune (increase $\alpha $), test error initially improves before eventually increasing again when the tree becomes too simple}{figure.52}{}}
\@writefile{toc}{\contentsline {section}{\numberline {77}Ensemble Methods}{150}{section.77}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {77.1}Bagging (Bootstrap Aggregating)}{151}{subsection.77.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {77.1.1}Bootstrap Sampling Properties}{151}{subsubsection.77.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {77.1.2}Out-of-Bag (OOB) Estimates}{151}{subsubsection.77.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {53}{\ignorespaces Bagging in action. (a) A single tree shows high variance with jagged, overfit predictions. As we aggregate more trees (b--d), the ensemble prediction smooths out, reducing variance while maintaining the ability to capture the underlying signal.}}{152}{figure.53}\protected@file@percent }
\newlabel{fig:bagging}{{53}{152}{Bagging in action. (a) A single tree shows high variance with jagged, overfit predictions. As we aggregate more trees (b--d), the ensemble prediction smooths out, reducing variance while maintaining the ability to capture the underlying signal}{figure.53}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {77.1.3}Why Bagging Works}{152}{subsubsection.77.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {77.1.4}Variance Estimation with Bagging}{152}{subsubsection.77.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {77.2}Random Forests}{153}{subsection.77.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {54}{\ignorespaces Random forest performance as trees are added. Feature subsampling decorrelates the trees, so adding more trees continues to improve performance. The ensemble captures complex structure that individual trees miss.}}{154}{figure.54}\protected@file@percent }
\newlabel{fig:random-forest}{{54}{154}{Random forest performance as trees are added. Feature subsampling decorrelates the trees, so adding more trees continues to improve performance. The ensemble captures complex structure that individual trees miss}{figure.54}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {77.2.1}Why Feature Subsampling Helps}{154}{subsubsection.77.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {77.3}Correlation Between Trees and the Path to Boosting}{155}{subsection.77.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {78}Summary}{155}{section.78}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {79}Motivation: Correlated Errors in Ensembles}{156}{section.79}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {79.1}Ensemble Prediction Function}{156}{subsection.79.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {79.2}The Boosting Solution}{157}{subsection.79.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {80}The Boosting Idea}{157}{section.80}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {55}{\ignorespaces Green: single shallow decision tree. Red: combined trees. Each individual tree is not particularly expressive-often just depth 1--2-but when combined, they capture complex patterns. Cross-validation is used to select tree depth and number of iterations.}}{158}{figure.55}\protected@file@percent }
\newlabel{fig:boosting}{{55}{158}{Green: single shallow decision tree. Red: combined trees. Each individual tree is not particularly expressive-often just depth 1--2-but when combined, they capture complex patterns. Cross-validation is used to select tree depth and number of iterations}{figure.55}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {80.1}Weak Learners and Strong Learners}{158}{subsection.80.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {80.2}Generic Boosting Loss Function}{159}{subsection.80.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {80.3}The Double Optimisation Process}{159}{subsection.80.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {81}Least Squares Boosting}{160}{section.81}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {82}AdaBoost}{161}{section.82}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {82.1}Binary Classification Encoding}{161}{subsection.82.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {82.2}The Exponential Loss Function}{161}{subsection.82.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {56}{\ignorespaces Comparison of loss functions for classification. The exponential loss (used by AdaBoost) penalises errors more aggressively than log loss. Both are differentiable surrogates for the 0-1 loss.}}{162}{figure.56}\protected@file@percent }
\newlabel{fig:loss-functions}{{56}{162}{Comparison of loss functions for classification. The exponential loss (used by AdaBoost) penalises errors more aggressively than log loss. Both are differentiable surrogates for the 0-1 loss}{figure.56}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {82.3}Comparing Loss Functions}{162}{subsection.82.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {82.4}The AdaBoost Algorithm}{163}{subsection.82.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {83}Gradient Boosting}{164}{section.83}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {83.1}Relationship to Other Methods}{164}{subsection.83.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {83.2}Gradient Descent in Function Space}{165}{subsection.83.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {83.3}Why Fit the Negative Gradient?}{165}{subsection.83.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {83.4}Hyperparameter Tuning}{166}{subsection.83.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {84}XGBoost: Extreme Gradient Boosting}{166}{section.84}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {85}Model Interpretation}{168}{section.85}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {85.1}Feature Importance}{168}{subsection.85.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {57}{\ignorespaces Feature importance for spam classification, where features are word occurrences. ``George'' is highly predictive-when the model splits on this feature, accuracy improves substantially.}}{169}{figure.57}\protected@file@percent }
\newlabel{fig:feature-importance-spam}{{57}{169}{Feature importance for spam classification, where features are word occurrences. ``George'' is highly predictive-when the model splits on this feature, accuracy improves substantially}{figure.57}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {58}{\ignorespaces Feature importance for digit classification (3 vs 8), where features are pixel values. Intuitively, the middle pixels are most useful for distinguishing these digits.}}{169}{figure.58}\protected@file@percent }
\newlabel{fig:feature-importance-digits}{{58}{169}{Feature importance for digit classification (3 vs 8), where features are pixel values. Intuitively, the middle pixels are most useful for distinguishing these digits}{figure.58}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {85.2}Partial Dependence Plots}{170}{subsection.85.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {59}{\ignorespaces Partial dependence plots showing how predictions change as individual features vary. Each curve shows the average prediction when that feature is set to a specific value, averaging over all other features.}}{170}{figure.59}\protected@file@percent }
\newlabel{fig:partial-dependence}{{59}{170}{Partial dependence plots showing how predictions change as individual features vary. Each curve shows the average prediction when that feature is set to a specific value, averaging over all other features}{figure.59}{}}
\@writefile{toc}{\contentsline {section}{\numberline {86}Summary}{171}{section.86}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {87}Overview}{173}{section.87}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {87.1}Explanation vs Prediction}{173}{subsection.87.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {88}Data Leakage}{174}{section.88}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {89}Random vs Non-Random Sampling}{175}{section.89}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {89.1}Why Random Sampling?}{175}{subsection.89.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {89.2}The Challenge of Heteroskedastic Noise}{175}{subsection.89.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {89.3}Arguments for Random Sampling}{175}{subsection.89.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {89.4}The Case for Non-Uniform (Adaptive) Sampling}{176}{subsection.89.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {90}Active Learning}{176}{section.90}\protected@file@percent }
\newlabel{sec:active-learning}{{90}{176}{Active Learning}{section.90}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {90.1}Criteria for Selecting Data Points}{177}{subsection.90.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {90.2}Uncertainty Sampling}{177}{subsection.90.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {60}{\ignorespaces Uncertainty sampling in action: points near the decision boundary (where the model is uncertain) are selected for labelling. These are precisely the points where additional labels provide the most information for refining the boundary.}}{178}{figure.60}\protected@file@percent }
\newlabel{fig:uncertainty-sampling}{{60}{178}{Uncertainty sampling in action: points near the decision boundary (where the model is uncertain) are selected for labelling. These are precisely the points where additional labels provide the most information for refining the boundary}{figure.60}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {90.3}Bayesian Active Learning by Disagreement (BALD)}{178}{subsection.90.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {91}Correcting for Non-Uniform Sampling}{179}{section.91}\protected@file@percent }
\newlabel{sec:ipw}{{91}{179}{Correcting for Non-Uniform Sampling}{section.91}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {91.1}The Problem}{180}{subsection.91.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {91.2}Inverse Probability Weighting (IPW)}{180}{subsection.91.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {92}Leverage Score Sampling}{180}{section.92}\protected@file@percent }
\newlabel{sec:leverage}{{92}{180}{Leverage Score Sampling}{section.92}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {92.1}Leverage Score Sampling Strategy}{181}{subsection.92.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {93}Random Fourier Features}{182}{section.93}\protected@file@percent }
\newlabel{sec:rff}{{93}{182}{Random Fourier Features}{section.93}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {93.1}The Computational Challenge}{182}{subsection.93.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {93.2}The Random Fourier Features Approximation}{182}{subsection.93.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {61}{\ignorespaces RFF approximation quality improves with more random features. Panel 1 shows the true RBF kernel function we aim to approximate. Subsequent panels show how the approximation improves as we increase the number of random features $R$.}}{183}{figure.61}\protected@file@percent }
\newlabel{fig:rff}{{61}{183}{RFF approximation quality improves with more random features. Panel 1 shows the true RBF kernel function we aim to approximate. Subsequent panels show how the approximation improves as we increase the number of random features $R$}{figure.61}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {62}{\ignorespaces Regression predictions using Random Fourier Features. The approximation quality depends on choosing a sufficient number of random features $R$ relative to the complexity of the underlying function.}}{184}{figure.62}\protected@file@percent }
\newlabel{fig:rff2}{{62}{184}{Regression predictions using Random Fourier Features. The approximation quality depends on choosing a sufficient number of random features $R$ relative to the complexity of the underlying function}{figure.62}{}}
\@writefile{toc}{\contentsline {section}{\numberline {94}Multi-Armed Bandits}{185}{section.94}\protected@file@percent }
\newlabel{sec:bandits}{{94}{185}{Multi-Armed Bandits}{section.94}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {94.1}Contextual Bandits}{185}{subsection.94.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {94.2}The Exploration--Exploitation Trade-off}{185}{subsection.94.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {94.3}Solution 1: $\epsilon $-Greedy}{186}{subsection.94.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {94.4}Solution 2: Upper Confidence Bound (UCB)}{186}{subsection.94.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {94.5}Solution 3: Thompson Sampling}{188}{subsection.94.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {95}Estimating Prevalence: AIPW}{189}{section.95}\protected@file@percent }
\newlabel{sec:aipw}{{95}{189}{Estimating Prevalence: AIPW}{section.95}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {95.1}The Problem}{189}{subsection.95.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {95.2}Targeted Sampling for Prevalence}{190}{subsection.95.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {95.3}The AIPW Estimator}{191}{subsection.95.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {63}{\ignorespaces The AIPW workflow: (1) Fit a model on gold-standard labelled data, (2) Construct pseudo-outcomes that combine model predictions with IPW corrections, (3) Use pseudo-outcomes for population inference.}}{191}{figure.63}\protected@file@percent }
\newlabel{fig:aipw}{{63}{191}{The AIPW workflow: (1) Fit a model on gold-standard labelled data, (2) Construct pseudo-outcomes that combine model predictions with IPW corrections, (3) Use pseudo-outcomes for population inference}{figure.63}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {95.4}Step-by-Step AIPW Process}{191}{subsection.95.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {96}Summary}{193}{section.96}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {97}Overview}{194}{section.97}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {97.1}Why Uncertainty Matters}{194}{subsection.97.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {98}Gaussian Processes}{194}{section.98}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {64}{\ignorespaces A GP defines a distribution over functions, with the shaded region showing uncertainty. The mean function (solid line) is our best estimate, while the shaded region indicates where the true function might plausibly lie.}}{195}{figure.64}\protected@file@percent }
\newlabel{fig:gp-overview}{{64}{195}{A GP defines a distribution over functions, with the shaded region showing uncertainty. The mean function (solid line) is our best estimate, while the shaded region indicates where the true function might plausibly lie}{figure.64}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {98.1}The Bayesian Perspective: Distributions Over Functions}{195}{subsection.98.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {98.2}Components of a Gaussian Process}{196}{subsection.98.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {98.2.1}Mean Function $m(x)$}{196}{subsubsection.98.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {98.2.2}Kernel (Covariance Function) $k(x, x')$}{196}{subsubsection.98.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {98.3}Properties of Gaussian Processes}{197}{subsection.98.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {98.4}GP Inference: From Prior to Posterior}{197}{subsection.98.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {98.4.1}The Process}{197}{subsubsection.98.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {98.4.2}Joint Distribution}{198}{subsubsection.98.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {65}{\ignorespaces Conditioning on observed data transforms the prior into a posterior. The left shows the marginal (prior) distribution; the right shows the conditional (posterior) distribution after observing data.}}{199}{figure.65}\protected@file@percent }
\newlabel{fig:marginals-conditionals}{{65}{199}{Conditioning on observed data transforms the prior into a posterior. The left shows the marginal (prior) distribution; the right shows the conditional (posterior) distribution after observing data}{figure.65}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {98.5}Predictive Distribution}{199}{subsection.98.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {98.5.1}Understanding the Mean $\mu _*$}{199}{subsubsection.98.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {98.5.2}Understanding the Variance $\Sigma _*$}{200}{subsubsection.98.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {98.6}Connection to Kernel Ridge Regression}{200}{subsection.98.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {98.7}Visualising GP Behaviour}{201}{subsection.98.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {66}{\ignorespaces Progression of a GP as data is observed: functions inconsistent with observations are down-weighted. The shaded region represents the confidence interval; sample functions are drawn from the posterior.}}{201}{figure.66}\protected@file@percent }
\newlabel{fig:gp-progression}{{66}{201}{Progression of a GP as data is observed: functions inconsistent with observations are down-weighted. The shaded region represents the confidence interval; sample functions are drawn from the posterior}{figure.66}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {98.8}Variance Properties: Why GPs Excel at Uncertainty}{202}{subsection.98.8}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {67}{\ignorespaces Two GPs with different kernel hyperparameters. Crosses indicate training data. The solid line shows the mean prediction; shaded regions show confidence intervals. Right panel: larger length-scale gives wider confidence bands, indicating that observations influence predictions over a larger range.}}{202}{figure.67}\protected@file@percent }
\newlabel{fig:gp-variance}{{67}{202}{Two GPs with different kernel hyperparameters. Crosses indicate training data. The solid line shows the mean prediction; shaded regions show confidence intervals. Right panel: larger length-scale gives wider confidence bands, indicating that observations influence predictions over a larger range}{figure.67}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {98.9}Posterior vs Posterior Predictive}{203}{subsection.98.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {98.10}Bayesian Optimisation}{203}{subsection.98.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {98.11}GP Summary}{204}{subsection.98.11}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {99}Conformal Inference}{204}{section.99}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {99.1}A Primer on Conformal Inference}{204}{subsection.99.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {99.2}The Algorithm: Split Conformal Prediction}{205}{subsection.99.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {99.3}Why Conformal Inference Works}{205}{subsection.99.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {99.4}Step-by-Step Process}{205}{subsection.99.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {99.5}Example: Non-Normal Errors}{206}{subsection.99.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {99.6}Handling Heteroskedasticity}{206}{subsection.99.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {99.7}Marginal vs Conditional Coverage}{207}{subsection.99.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {100}Comparison: GPs vs Conformal Inference}{207}{section.100}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {101}Summary}{208}{section.101}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {102}Overview}{209}{section.102}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {103}Perceptrons}{209}{section.103}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {103.1}The Algorithm}{209}{subsection.103.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {103.1.1}The Perceptron Learning Rule}{209}{subsubsection.103.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {68}{\ignorespaces Perceptron learning: the decision boundary (dashed line) adjusts iteratively as misclassifications are corrected. The algorithm bounces around but generally moves toward a separating hyperplane.}}{210}{figure.68}\protected@file@percent }
\newlabel{fig:perceptron}{{68}{210}{Perceptron learning: the decision boundary (dashed line) adjusts iteratively as misclassifications are corrected. The algorithm bounces around but generally moves toward a separating hyperplane}{figure.68}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {103.1.2}Connection to Logistic Regression}{211}{subsubsection.103.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {103.2}Limitations of the Perceptron}{211}{subsection.103.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {103.2.1}Linear Separability Requirement}{211}{subsubsection.103.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {69}{\ignorespaces XOR is not linearly separable-no single hyperplane can separate the classes. The perceptron will cycle indefinitely without converging.}}{212}{figure.69}\protected@file@percent }
\newlabel{fig:xor}{{69}{212}{XOR is not linearly separable-no single hyperplane can separate the classes. The perceptron will cycle indefinitely without converging}{figure.69}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {103.2.2}No Margin Maximisation}{212}{subsubsection.103.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {70}{\ignorespaces Multiple valid decision boundaries exist for linearly separable data. The perceptron finds \textit  {some} separating hyperplane, but not necessarily the best one.}}{212}{figure.70}\protected@file@percent }
\newlabel{fig:infinite-hyperplane}{{70}{212}{Multiple valid decision boundaries exist for linearly separable data. The perceptron finds \textit {some} separating hyperplane, but not necessarily the best one}{figure.70}{}}
\@writefile{toc}{\contentsline {section}{\numberline {104}Feed-Forward Neural Networks}{213}{section.104}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {104.1}From Fixed to Learned Features}{213}{subsection.104.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {104.2}Hierarchical Feature Learning}{214}{subsection.104.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {104.3}Why Non-Linearity is Essential}{214}{subsection.104.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {105}Activation Functions}{215}{section.105}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {105.1}Common Activation Functions}{215}{subsection.105.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {71}{\ignorespaces Comparison of common activation functions. Note how sigmoid and tanh saturate (flatten) for large inputs, while ReLU and its variants do not.}}{216}{figure.71}\protected@file@percent }
\newlabel{fig:activations1}{{71}{216}{Comparison of common activation functions. Note how sigmoid and tanh saturate (flatten) for large inputs, while ReLU and its variants do not}{figure.71}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {72}{\ignorespaces Activation function derivatives-crucial for gradient flow during backpropagation. Sigmoid derivatives vanish for large inputs; ReLU derivatives are constant for positive inputs.}}{216}{figure.72}\protected@file@percent }
\newlabel{fig:activations2}{{72}{216}{Activation function derivatives-crucial for gradient flow during backpropagation. Sigmoid derivatives vanish for large inputs; ReLU derivatives are constant for positive inputs}{figure.72}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {105.2}Softmax for Multi-Class Classification}{216}{subsection.105.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {106}Training Neural Networks}{217}{section.106}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {106.1}Stochastic Gradient Descent}{217}{subsection.106.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {106.2}The Credit Assignment Problem}{217}{subsection.106.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {107}Backpropagation}{217}{section.107}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {107.1}Function Composition and the Chain Rule}{218}{subsection.107.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {73}{\ignorespaces Backpropagation computes gradients layer by layer using the chain rule. The fourth function shown is the loss function.}}{218}{figure.73}\protected@file@percent }
\newlabel{fig:backprop}{{73}{218}{Backpropagation computes gradients layer by layer using the chain rule. The fourth function shown is the loss function}{figure.73}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {107.2}The Two-Pass Algorithm}{218}{subsection.107.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {107.3}Backpropagation for an MLP}{219}{subsection.107.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {74}{\ignorespaces MLP structure with alternating linear and non-linear layers. The final layer is the loss function.}}{219}{figure.74}\protected@file@percent }
\newlabel{fig:mlp}{{74}{219}{MLP structure with alternating linear and non-linear layers. The final layer is the loss function}{figure.74}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {75}{\ignorespaces Backpropagation algorithm: gradients are computed iteratively from output to input, reusing computations (dynamic programming).}}{220}{figure.75}\protected@file@percent }
\newlabel{fig:backprop-algo}{{75}{220}{Backpropagation algorithm: gradients are computed iteratively from output to input, reusing computations (dynamic programming)}{figure.75}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {107.4}Computing Individual Layer Derivatives}{220}{subsection.107.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {107.4.1}Numerical Approximation}{220}{subsubsection.107.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {107.4.2}Automatic Differentiation}{221}{subsubsection.107.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {108}MLP Design}{221}{section.108}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {76}{\ignorespaces MLP design recipe: architecture, loss function, and optimiser choices.}}{222}{figure.76}\protected@file@percent }
\newlabel{fig:mlp-recipe}{{76}{222}{MLP design recipe: architecture, loss function, and optimiser choices}{figure.76}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {108.1}Architecture Design}{222}{subsection.108.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {108.2}A Simple MLP Example}{223}{subsection.108.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {108.3}Loss Functions}{223}{subsection.108.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {109}Optimisers}{224}{section.109}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {109.1}SGD with Momentum}{224}{subsection.109.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {109.2}Adam}{224}{subsection.109.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {109.3}BFGS}{225}{subsection.109.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {110}Universal Approximation}{225}{section.110}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {77}{\ignorespaces ReLU networks learn piecewise linear functions, partitioning input space into regions with different linear behaviours. More hidden units create finer partitions.}}{226}{figure.77}\protected@file@percent }
\newlabel{fig:universal}{{77}{226}{ReLU networks learn piecewise linear functions, partitioning input space into regions with different linear behaviours. More hidden units create finer partitions}{figure.77}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {110.1}Theory vs Practice}{226}{subsection.110.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {111}Neural Networks as Gaussian Processes}{226}{section.111}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {78}{\ignorespaces MLPs (red) as samples from a GP (blue) defined by the NTK. Different training trajectories (different minibatch sequences) yield different but related functions.}}{227}{figure.78}\protected@file@percent }
\newlabel{fig:mlp-gp}{{78}{227}{MLPs (red) as samples from a GP (blue) defined by the NTK. Different training trajectories (different minibatch sequences) yield different but related functions}{figure.78}{}}
\@writefile{toc}{\contentsline {section}{\numberline {112}Vanishing and Exploding Gradients}{228}{section.112}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {112.1}The Problem}{228}{subsection.112.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {112.2}Solution: Gradient Clipping (for Exploding Gradients)}{229}{subsection.112.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {79}{\ignorespaces Gradient clipping constrains step size while preserving direction. The gradient magnitude is capped, but the direction of descent is maintained.}}{229}{figure.79}\protected@file@percent }
\newlabel{fig:gradient-clipping}{{79}{229}{Gradient clipping constrains step size while preserving direction. The gradient magnitude is capped, but the direction of descent is maintained}{figure.79}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {112.3}Solution: Non-Saturating Activations (for Vanishing Gradients)}{229}{subsection.112.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {80}{\ignorespaces ReLU and Leaky ReLU maintain gradient flow better than sigmoid. Sigmoid saturates (gradient $\to 0$) for large inputs; ReLU maintains unit gradient for positive inputs.}}{230}{figure.80}\protected@file@percent }
\newlabel{fig:nonsaturating}{{80}{230}{ReLU and Leaky ReLU maintain gradient flow better than sigmoid. Sigmoid saturates (gradient $\to 0$) for large inputs; ReLU maintains unit gradient for positive inputs}{figure.80}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {112.4}Choosing Between ReLU and Leaky ReLU}{231}{subsection.112.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {113}Batch Normalisation}{231}{section.113}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {113.1}The Idea}{231}{subsection.113.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {113.2}Behaviour at Test Time}{232}{subsection.113.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {113.3}Benefits of Batch Normalisation}{232}{subsection.113.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {114}Regularisation}{233}{section.114}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {114.1}Weight Decay (L2 Regularisation)}{233}{subsection.114.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {114.2}Dropout}{233}{subsection.114.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {81}{\ignorespaces Dropout randomly removes connections during training, creating a different ``thinned'' network at each forward pass.}}{234}{figure.81}\protected@file@percent }
\newlabel{fig:dropout}{{81}{234}{Dropout randomly removes connections during training, creating a different ``thinned'' network at each forward pass}{figure.81}{}}
\@writefile{toc}{\contentsline {section}{\numberline {115}Summary}{235}{section.115}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {116}Overview}{236}{section.116}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {117}Neural Network Design Recap}{236}{section.117}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {117.1}Architecture Components}{236}{subsection.117.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {117.2}Loss Functions}{237}{subsection.117.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {117.3}Optimisers}{237}{subsection.117.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {118}Vanishing and Exploding Gradients}{237}{section.118}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {118.1}The Problem}{238}{subsection.118.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {118.2}Gradient Clipping}{238}{subsection.118.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {82}{\ignorespaces Gradient clipping constrains update magnitude while preserving direction, preventing divergence in optimisation. Without clipping, large gradients can cause the optimisation to ``overshoot'' and diverge.}}{239}{figure.82}\protected@file@percent }
\newlabel{fig:gradient-clipping}{{82}{239}{Gradient clipping constrains update magnitude while preserving direction, preventing divergence in optimisation. Without clipping, large gradients can cause the optimisation to ``overshoot'' and diverge}{figure.82}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {118.3}Vanishing Gradients and Activation Functions}{239}{subsection.118.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {83}{\ignorespaces Activation functions and their derivatives. Sigmoid saturates for large inputs, causing vanishing gradients. ReLU maintains gradient of 1 for positive inputs.}}{239}{figure.83}\protected@file@percent }
\newlabel{fig:activation-saturation}{{83}{239}{Activation functions and their derivatives. Sigmoid saturates for large inputs, causing vanishing gradients. ReLU maintains gradient of 1 for positive inputs}{figure.83}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {118.4}Batch Normalisation}{240}{subsection.118.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {118.5}Regularisation}{242}{subsection.118.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {118.5.1}Weight Decay (L2 Regularisation)}{242}{subsubsection.118.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {118.5.2}Dropout}{242}{subsubsection.118.5.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {84}{\ignorespaces Dropout randomly removes neurons during training, forcing redundant representations. Each training iteration uses a different random subnetwork.}}{242}{figure.84}\protected@file@percent }
\newlabel{fig:dropout}{{84}{242}{Dropout randomly removes neurons during training, forcing redundant representations. Each training iteration uses a different random subnetwork}{figure.84}{}}
\@writefile{toc}{\contentsline {section}{\numberline {119}Convolutional Neural Networks}{243}{section.119}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {119.1}Why Image Data is Challenging}{243}{subsection.119.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {119.2}The Convolution Operation}{244}{subsection.119.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {85}{\ignorespaces A convolution filter sliding across an image, computing local weighted sums. The filter ``looks at'' one small region at a time.}}{244}{figure.85}\protected@file@percent }
\newlabel{fig:convolution}{{85}{244}{A convolution filter sliding across an image, computing local weighted sums. The filter ``looks at'' one small region at a time}{figure.85}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {86}{\ignorespaces When objects span multiple regions, convolutions capture features that simple partitioning would miss. The sliding window ensures every local pattern is detected regardless of position.}}{245}{figure.86}\protected@file@percent }
\newlabel{fig:convolution2}{{86}{245}{When objects span multiple regions, convolutions capture features that simple partitioning would miss. The sliding window ensures every local pattern is detected regardless of position}{figure.86}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {87}{\ignorespaces 1D convolution: the filter weights determine which local patterns are detected. Different weights detect different patterns (e.g., edges, gradients, flat regions).}}{245}{figure.87}\protected@file@percent }
\newlabel{fig:convolution1d}{{87}{245}{1D convolution: the filter weights determine which local patterns are detected. Different weights detect different patterns (e.g., edges, gradients, flat regions)}{figure.87}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {88}{\ignorespaces 2D convolution for image processing, extending the same principle to spatial data. The filter has both width and height.}}{245}{figure.88}\protected@file@percent }
\newlabel{fig:convolution2d}{{88}{245}{2D convolution for image processing, extending the same principle to spatial data. The filter has both width and height}{figure.88}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {89}{\ignorespaces Learned filters detect interpretable features such as horizontal and vertical edges, colour gradients, and textures.}}{245}{figure.89}\protected@file@percent }
\newlabel{fig:learned-filters}{{89}{245}{Learned filters detect interpretable features such as horizontal and vertical edges, colour gradients, and textures}{figure.89}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {119.3}Convolution as Sparse Matrix Multiplication}{246}{subsection.119.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {90}{\ignorespaces Convolution as sparse matrix multiplication: zeros correspond to pixels outside the receptive field. We only ``pay attention'' to features within the filter's window.}}{246}{figure.90}\protected@file@percent }
\newlabel{fig:conv-matrix}{{90}{246}{Convolution as sparse matrix multiplication: zeros correspond to pixels outside the receptive field. We only ``pay attention'' to features within the filter's window}{figure.90}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {119.4}Padding and Strides}{246}{subsection.119.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {119.4.1}Padding}{246}{subsubsection.119.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {91}{\ignorespaces Zero-padding allows the filter to process edge regions, preserving spatial dimensions. The added zeros (shown in grey) extend the input so the filter can be centred on boundary pixels.}}{247}{figure.91}\protected@file@percent }
\newlabel{fig:padding}{{91}{247}{Zero-padding allows the filter to process edge regions, preserving spatial dimensions. The added zeros (shown in grey) extend the input so the filter can be centred on boundary pixels}{figure.91}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {119.4.2}Strides}{247}{subsubsection.119.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {92}{\ignorespaces Comparison of stride 1 (top) vs stride 2 (bottom). Larger strides downsample the feature map, reducing computation and introducing some translation invariance.}}{247}{figure.92}\protected@file@percent }
\newlabel{fig:strides}{{92}{247}{Comparison of stride 1 (top) vs stride 2 (bottom). Larger strides downsample the feature map, reducing computation and introducing some translation invariance}{figure.92}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {119.5}Pooling}{248}{subsection.119.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {93}{\ignorespaces Max pooling selects the largest value in each region, providing translation robustness. Small shifts in the input often produce the same pooled output.}}{248}{figure.93}\protected@file@percent }
\newlabel{fig:pooling}{{93}{248}{Max pooling selects the largest value in each region, providing translation robustness. Small shifts in the input often produce the same pooled output}{figure.93}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {119.6}CNN Architecture}{248}{subsection.119.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {94}{\ignorespaces CNN architecture: early layers detect edges, middle layers detect parts (eyes, wheels), deep layers detect objects (faces, cars). This hierarchical feature learning is a key strength of CNNs.}}{249}{figure.94}\protected@file@percent }
\newlabel{fig:cnn-hierarchy}{{94}{249}{CNN architecture: early layers detect edges, middle layers detect parts (eyes, wheels), deep layers detect objects (faces, cars). This hierarchical feature learning is a key strength of CNNs}{figure.94}{}}
\@writefile{toc}{\contentsline {section}{\numberline {120}Recurrent Neural Networks}{250}{section.120}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {120.1}Architecture}{251}{subsection.120.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {120.2}Properties and Capabilities}{251}{subsection.120.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {121}Attention Mechanisms}{252}{section.121}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {95}{\ignorespaces Attention mechanism: queries attend to keys, retrieving weighted combinations of values. The attention weights determine how much each value contributes to the output.}}{253}{figure.95}\protected@file@percent }
\newlabel{fig:attention}{{95}{253}{Attention mechanism: queries attend to keys, retrieving weighted combinations of values. The attention weights determine how much each value contributes to the output}{figure.95}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {121.1}General Attention}{253}{subsection.121.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {96}{\ignorespaces Attention weights visualised: each query attends differently to the available keys. Brighter colours indicate higher attention weights.}}{254}{figure.96}\protected@file@percent }
\newlabel{fig:attention-weights}{{96}{254}{Attention weights visualised: each query attends differently to the available keys. Brighter colours indicate higher attention weights}{figure.96}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {121.2}Scaled Dot-Product Attention}{254}{subsection.121.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {121.2.1}Why Scale by $\sqrt  {d}$?}{255}{subsubsection.121.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {121.2.2}Row-wise Softmax}{255}{subsubsection.121.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {121.3}Self-Attention}{255}{subsection.121.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {97}{\ignorespaces Self-attention: each position computes attention over all positions, enabling global context. Every position can directly attend to every other position in a single layer.}}{256}{figure.97}\protected@file@percent }
\newlabel{fig:self-attention}{{97}{256}{Self-attention: each position computes attention over all positions, enabling global context. Every position can directly attend to every other position in a single layer}{figure.97}{}}
\@writefile{toc}{\contentsline {section}{\numberline {122}Summary}{258}{section.122}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {123}Overview}{259}{section.123}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {124}Principal Component Analysis}{259}{section.124}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {124.1}Dimensionality Reduction as Optimisation}{259}{subsection.124.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {124.2}Low-Dimensional Representation}{261}{subsection.124.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {98}{\ignorespaces PCA projection: data points (blue) in a 2D space projected onto the first principal component (red line). The principal component direction captures maximum variance; projections minimise the perpendicular (orthogonal) distance to this line. The residuals (dashed lines) represent the reconstruction error.}}{262}{figure.98}\protected@file@percent }
\newlabel{fig:pca}{{98}{262}{PCA projection: data points (blue) in a 2D space projected onto the first principal component (red line). The principal component direction captures maximum variance; projections minimise the perpendicular (orthogonal) distance to this line. The residuals (dashed lines) represent the reconstruction error}{figure.98}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {124.3}Mathematical Derivation of PCA}{262}{subsection.124.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {124.4}Terminology: Embeddings}{263}{subsection.124.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {125}Neighbourhood Embeddings}{263}{section.125}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {125.1}Stochastic Neighbour Embedding (SNE)}{264}{subsection.125.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {125.2}t-Distributed SNE (t-SNE)}{265}{subsection.125.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {125.3}UMAP: Uniform Manifold Approximation and Projection}{266}{subsection.125.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {125.3.1}Visual Intuition for UMAP}{267}{subsubsection.125.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {99}{\ignorespaces Original high-dimensional data projected to 2D, with colours indicating cluster membership. The goal is to find a 2D embedding that reveals this cluster structure.}}{267}{figure.99}\protected@file@percent }
\newlabel{fig:embedding1}{{99}{267}{Original high-dimensional data projected to 2D, with colours indicating cluster membership. The goal is to find a 2D embedding that reveals this cluster structure}{figure.99}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {100}{\ignorespaces UMAP constructs a $k$-nearest neighbour graph; shaded circles represent local neighbourhoods. Each point is connected to its closest neighbours, capturing local structure without computing all pairwise distances.}}{267}{figure.100}\protected@file@percent }
\newlabel{fig:embedding2}{{100}{267}{UMAP constructs a $k$-nearest neighbour graph; shaded circles represent local neighbourhoods. Each point is connected to its closest neighbours, capturing local structure without computing all pairwise distances}{figure.100}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {101}{\ignorespaces The embedding preserves neighbourhood structure: connected points in high dimensions remain close in the projection. The lines show how local connectivity is maintained.}}{267}{figure.101}\protected@file@percent }
\newlabel{fig:embedding3}{{101}{267}{The embedding preserves neighbourhood structure: connected points in high dimensions remain close in the projection. The lines show how local connectivity is maintained}{figure.101}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {125.3.2}Handling Varying Local Density}{268}{subsubsection.125.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {102}{\ignorespaces UMAP adapts to varying local densities, using different distance scales in dense versus sparse regions. The varying shaded regions show how neighbourhood size adapts to local density, preserving structure in both dense and sparse areas.}}{268}{figure.102}\protected@file@percent }
\newlabel{fig:embedding4}{{102}{268}{UMAP adapts to varying local densities, using different distance scales in dense versus sparse regions. The varying shaded regions show how neighbourhood size adapts to local density, preserving structure in both dense and sparse areas}{figure.102}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {125.4}Comparing Dimensionality Reduction Methods}{268}{subsection.125.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {126}Autoencoders}{269}{section.126}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {126.1}Architecture}{270}{subsection.126.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {103}{\ignorespaces Autoencoder architecture: input is compressed through the encoder to a bottleneck layer (latent representation $z$), then reconstructed by the decoder. The bottleneck dimension $L$ is smaller than the input dimension $D$, forcing compression.}}{270}{figure.103}\protected@file@percent }
\newlabel{fig:autoencoder}{{103}{270}{Autoencoder architecture: input is compressed through the encoder to a bottleneck layer (latent representation $z$), then reconstructed by the decoder. The bottleneck dimension $L$ is smaller than the input dimension $D$, forcing compression}{figure.103}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {126.2}Relationship to PCA}{271}{subsection.126.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {126.3}Bottleneck Dimension}{271}{subsection.126.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {126.4}Autoencoder Variants}{272}{subsection.126.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {126.5}Applications of Autoencoders}{273}{subsection.126.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {127}Summary}{274}{section.127}\protected@file@percent }
\gdef \@abspage@last{275}
