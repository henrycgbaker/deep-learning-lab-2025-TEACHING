
\thispagestyle{empty}
{\large \bfseries ML Lecture Notes 2024 \hfill Henry Baker}
\vspace{2mm}
\hrule

\vspace*{0.3cm}
\begin{center}
	{\Large \bf ML Lecture Notes: Week 3\\ High-Dimensional Methods \& Regularisation}
	\vspace{2mm}

\end{center}
\vspace{0.4cm}

%══════════════════════════════════════════════════════════════════════════════
\section{Supervised Learning: A Quick Recap}
%══════════════════════════════════════════════════════════════════════════════

\begin{bluebox}[Section Summary]
Supervised learning uses labelled examples to learn a mapping from inputs to outputs. We measure success by comparing predictions to true labels using a loss function. This week, we explore what happens when we expand features to capture nonlinear relationships, and why this expansion creates both opportunities and challenges.
\end{bluebox}

Before diving into high-dimensional methods, let us briefly recall the supervised learning framework. Given inputs $X$ with corresponding labels $y$, we aim to learn a function $f$ that maps inputs to outputs:

\begin{itemize}
    \item \textbf{Classification}: Learn $f(x): \mathcal{X} \to \{0, 1\}$ (or more generally, $\{1, \ldots, K\}$ for $K$ classes)
    \item \textbf{Regression}: Learn $f(x): \mathcal{X} \to \mathbb{R}$
\end{itemize}

Performance is measured by some distance or discrepancy between predicted and actual labels: $d(\hat{f}(x), y)$. For example, in OLS regression we use the squared error $(\hat{f}(x) - y)^2$.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{figures/week_03_highdim/reg-class.png}
    \caption{Regression vs classification in supervised learning. Left: regression fits a continuous function through the data. Right: classification finds a decision boundary separating classes.}
    \label{fig:regression-vs-classification}
\end{figure}

\subsection{OLS Recap}

Recall that OLS gives us the optimal linear predictor:
$$\hat{\beta} = (X^\top X)^{-1} X^\top y$$

\textbf{What this formula says}: To find the best linear coefficients $\hat{\beta}$, we:
\begin{enumerate}
    \item Compute $X^\top X$: This captures how features correlate with each other (the Gram matrix)
    \item Invert $(X^\top X)^{-1}$: This ``undoes'' the feature correlations
    \item Multiply by $X^\top y$: This captures how features correlate with the target
\end{enumerate}

The result $\hat{\beta}$ tells us the independent contribution of each feature to predicting $y$, accounting for correlations between features.

We then plug these coefficients back to get predictions:
$$\hat{y} = X\hat{\beta}$$

In general, we assume the first column of $X$ is a column of ones (the intercept), giving us a matrix of dimension $n \times (p + 1)$. The prediction is then:
$$\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 X_1 + \cdots + \hat{\beta}_p X_p$$

But what if the true relationship is nonlinear? One approach is \textbf{feature expansion}.

%══════════════════════════════════════════════════════════════════════════════
\section{From Linear to Nonlinear: Polynomial Regression}
%══════════════════════════════════════════════════════════════════════════════

\begin{bluebox}[Section Summary]
Feature expansion transforms simple linear models into powerful nonlinear approximators while preserving the tractability of OLS. The key insight: ``linear regression'' means linear in \emph{parameters}, not features. However, high-degree polynomials introduce numerical instability (ill-conditioned matrices) and edge oscillations (Runge's phenomenon). These problems motivate regularisation and alternative basis functions.
\end{bluebox}

OLS gives us the optimal linear predictor:
$$\hat{\beta} = (X^\top X)^{-1} X^\top y \quad \Rightarrow \quad \hat{y} = X\hat{\beta}$$

But what if the true relationship is nonlinear? One approach: \textbf{feature expansion}.

\begin{greybox}[Polynomial Regression]
For a single feature $x$, expand to polynomial basis:
$$f(x) = \beta_0 + \beta_1 x + \beta_2 x^2 + \cdots + \beta_M x^M$$

This is still \textbf{linear in parameters} $\beta$-we're just using transformed features $[1, x, x^2, \ldots, x^M]$.

The design matrix becomes:
$$X = \begin{bmatrix} 1 & x_1 & x_1^2 & \cdots & x_1^M \\ 1 & x_2 & x_2^2 & \cdots & x_2^M \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ 1 & x_n & x_n^2 & \cdots & x_n^M \end{bmatrix}$$
\end{greybox}

\textbf{Unpacking the polynomial design matrix:}
\begin{itemize}
    \item Each row corresponds to one observation $x_i$
    \item Each column corresponds to a power of $x$: the $j$-th column contains $x_i^{j-1}$ for all observations
    \item The matrix has $n$ rows (observations) and $M+1$ columns (including the intercept)
    \item We can apply standard OLS to this matrix, treating each power as a separate ``feature''
\end{itemize}

\begin{bluebox}[Key Insight: ``Linear'' Refers to Parameters]
``Linear regression'' means linear in \textbf{parameters}, not in features. We can model arbitrarily complex relationships by transforming features-polynomials, interactions, logarithms, etc.-while still using the OLS machinery.

The polynomial order $M$ is our \textbf{measure of model complexity}: it determines how well the model can fit the training data.
\end{bluebox}

\subsection{Why Polynomials Are Attractive (In Theory)}

Polynomials can represent a huge class of functions, making them highly flexible and mathematically convenient:

\begin{greybox}[Theoretical Foundations of Polynomial Approximation]
\begin{itemize}
    \item \textbf{Taylor series}: Any smooth (infinitely differentiable) function can be approximated by its Taylor polynomial around any point. For a function $f$ expanded around $a$:
    $$f(x) = \sum_{k=0}^{\infty} \frac{f^{(k)}(a)}{k!}(x-a)^k$$
    Truncating this series gives a polynomial approximation.

    \item \textbf{Weierstrass approximation theorem}: Any continuous function on a closed interval $[a,b]$ can be uniformly approximated by polynomials to arbitrary precision. Formally: for any $\epsilon > 0$, there exists a polynomial $p(x)$ such that $|f(x) - p(x)| < \epsilon$ for all $x \in [a,b]$.
\end{itemize}

These results guarantee that polynomials are ``universal approximators'' for continuous functions-given enough terms, they can approximate any continuous function arbitrarily well.
\end{greybox}

\textbf{Why this matters for machine learning}: These theoretical results suggest that if we use high-enough degree polynomials, we should be able to fit any smooth relationship in our data. However, ``can approximate'' does not mean ``easy to estimate from finite data''-this gap between theoretical expressiveness and practical learnability is central to understanding overfitting.

\subsection{Feature Expansion: Power and Peril}

Feature expansion gives us flexibility, but at a cost. Understanding this tradeoff is essential for effective model building.

\begin{greybox}[The Expansion Explosion]
Starting with $p$ original features, consider common expansions:

\begin{itemize}
    \item \textbf{Polynomial degree $M$}: For a single feature, we get $M+1$ terms. For $p$ features with all interactions up to degree $M$, we get $\binom{p+M}{M}$ terms.
    \item \textbf{Pairwise interactions}: Adding $x_i x_j$ terms gives $\binom{p}{2} = \frac{p(p-1)}{2}$ new features.
    \item \textbf{Degree-2 polynomial with interactions}: Grows as $O(p^2)$.
\end{itemize}

\textbf{Example}: With $p = 100$ original features:
\begin{itemize}
    \item Pairwise interactions: 4,950 additional features
    \item Full degree-2: 5,151 total features
    \item Full degree-3: 176,851 total features
\end{itemize}
\end{greybox}

\textbf{Unpacking the combinatorics}: The formula $\binom{p+M}{M}$ counts the number of ways to distribute $M$ units of ``degree'' among $p$ features (including the option of assigning all to one feature). For example, with $p=2$ features $(x_1, x_2)$ and $M=2$:
\begin{itemize}
    \item Degree 0: constant (1 term)
    \item Degree 1: $x_1, x_2$ (2 terms)
    \item Degree 2: $x_1^2, x_1 x_2, x_2^2$ (3 terms)
\end{itemize}
Total: $\binom{2+2}{2} = 6$ terms.

\begin{bluebox}[Feature Expansion Tradeoffs]
\begin{center}
\begin{tabular}{l|l}
\textbf{Benefits} & \textbf{Costs} \\
\hline
Captures nonlinear relationships & Increases effective dimension \\
Can approximate any smooth function & Triggers curse of dimensionality \\
Remains tractable (OLS still applies) & Ill-conditioned design matrix \\
No need to specify functional form & Overfitting risk increases \\
\end{tabular}
\end{center}
\end{bluebox}

\textbf{When to expand features}:
\begin{itemize}
    \item You have strong reason to believe the relationship is nonlinear
    \item You have enough data to support the additional parameters
    \item You will use regularisation to control complexity
    \item The domain suggests specific transformations (e.g., log-income, interaction between treatment and subgroup)
\end{itemize}

\textbf{When to be cautious}:
\begin{itemize}
    \item $n$ is small relative to $p$
    \item You're already seeing signs of overfitting
    \item Interpretability is important (expanded models are harder to explain)
    \item The original features are already correlated (expansion worsens multicollinearity)
\end{itemize}

\subsection{The Problem: Choosing $M$}

Higher-degree polynomials are more expressive but risk overfitting:
\begin{itemize}
    \item $M = 1$: Straight line (may underfit)
    \item $M = 3$: Cubic (often reasonable)
    \item $M = 15$: Wiggly curve that passes through every training point (overfits)
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{figures/week_03_highdim/choosing M.png}
    \caption{Effect of polynomial degree on fit. Higher degrees fit training data perfectly but generalise poorly. This illustrates the fundamental tension in model selection: low $M$ underfits (misses the pattern), high $M$ overfits (memorises noise).}
    \label{fig:choosing-M}
\end{figure}

\subsection{Numerical Instability in High-Degree Polynomials}

\begin{redbox}[Polynomials Are Global Approximators]
Polynomials are \textbf{global approximators}-changing the polynomial anywhere affects it everywhere. This leads to fundamental problems with \textbf{edges} and \textbf{variance}:

\begin{enumerate}
    \item \textbf{Runge's phenomenon}: High-degree polynomials oscillate wildly near boundaries when interpolating, even for smooth underlying functions
    \item \textbf{High variance at edges}: As polynomial degree increases, behaviour becomes increasingly erratic near domain boundaries
    \item \textbf{Sensitivity to data}: Small changes in data points can cause radically large differences in predictions throughout the entire domain
\end{enumerate}

Because polynomials are global approximators, changes to improve the fit in one part of the domain can have far-reaching effects throughout the entire domain, including unwanted oscillations at the edges.
\end{redbox}

For polynomials of large degrees ($x^k$ where $k$ is large), two main issues contribute to \textbf{numerical instability}:

\begin{greybox}[Sources of Numerical Instability]
\begin{enumerate}
    \item \textbf{Magnitude of polynomial terms}: As the degree $k$ increases, $x^k$ grows rapidly for $|x| > 1$. This leads to extremely large values that are difficult to manage computationally.

    \textit{Example}: For $x = 2$ and $k = 20$, we have $x^k = 2^{20} \approx 10^6$. For $k = 50$, $x^k \approx 10^{15}$.

    \item \textbf{Magnitude of coefficients}: To compensate for large polynomial terms, the fitting process produces very small (or very large) coefficients $\beta_k$. These coefficients must scale the polynomial terms back to fit the data.

    \textit{What happens}: As $x^k$ explodes, $\beta_k$ must shrink correspondingly to keep the fit reasonable.
\end{enumerate}

The combination of very large polynomial terms and small coefficients leads to numerical instability: small changes in data or coefficients produce disproportionately large changes in predictions.
\end{greybox}

\textbf{Why this is problematic}: When we compute $\beta_k \cdot x^k$ where $\beta_k \approx 10^{-10}$ and $x^k \approx 10^{10}$, we're multiplying a very small number by a very large number. Floating-point arithmetic has limited precision (about 16 significant digits in double precision), so this operation loses accuracy. The result might be correct to only a few significant figures, making predictions unreliable.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{figures/week_03_highdim/numerical_instability.png}
    \caption{Numerical instability in high-degree polynomials: small perturbations in data cause large changes in the fitted curve, particularly near the edges of the domain.}
    \label{fig:numerical-instability}
\end{figure}

\subsubsection{Condition Numbers: A Deeper Look}

The \textbf{condition number} formalises the notion of numerical stability. It tells us how sensitive a computation is to small perturbations in the input.

\begin{greybox}[Condition Number: Formal Definition]
The \textbf{condition number} of an invertible matrix $A$ measures how much the solution $x = A^{-1}b$ changes when we perturb $b$. Formally, for any matrix norm:
$$\kappa(A) = \|A\| \cdot \|A^{-1}\|$$

\textbf{Key property}: If we perturb $b$ to $b + \delta b$, the relative change in the solution satisfies:
$$\frac{\|x - \tilde{x}\|}{\|x\|} \leq \kappa(A) \cdot \frac{\|\delta b\|}{\|b\|}$$

The condition number bounds how much \emph{relative error} in the input gets amplified in the output.
\end{greybox}

\textbf{Unpacking the definition}:
\begin{itemize}
    \item $\|A\|$ measures how much $A$ can ``stretch'' a vector
    \item $\|A^{-1}\|$ measures how much $A^{-1}$ can ``stretch'' a vector
    \item Their product $\kappa(A)$ measures the worst-case amplification of errors through the system $Ax = b$
\end{itemize}

\textbf{Intuition}: Think of $\kappa(A)$ as an ``error amplification factor.'' If $\kappa(A) = 10^6$ and your input has $10^{-8}$ relative error (typical for double precision floating point), your output could have $10^{-2}$ relative error-two correct decimal places at best.

For symmetric positive definite matrices (like $X^\top X$ in OLS), the condition number simplifies:
$$\kappa(A) = \frac{\lambda_{\max}(A)}{\lambda_{\min}(A)}$$

\textbf{Why this form?} For symmetric matrices, the matrix norm equals the largest eigenvalue, and the inverse's norm equals the reciprocal of the smallest eigenvalue. The condition number thus measures the ``spread'' of eigenvalues-how elongated the matrix's action is in different directions.

\begin{bluebox}[Condition Number Interpretation]
\begin{itemize}
    \item $\kappa \approx 1$: Well-conditioned, numerically stable
    \item $\kappa \sim 10^{3}$: Moderate-expect to lose about 3 digits of precision
    \item $\kappa \sim 10^{8}$: Ill-conditioned-results may be meaningless in double precision
    \item $\kappa = \infty$: Singular matrix, non-invertible
\end{itemize}

\textbf{Rule of thumb}: In double precision (about 16 digits), you lose roughly $\log_{10}(\kappa)$ digits of precision. If $\kappa \approx 10^{16}$, you have essentially no reliable digits left.
\end{bluebox}

\textbf{Connection to near-singularity}: A matrix is ill-conditioned when it is ``almost singular''-its smallest eigenvalue is tiny relative to the largest. Geometrically, the matrix stretches space enormously in some directions but barely at all in others, making inversion numerically treacherous.

\begin{greybox}[Worked Example: Vandermonde Matrix Conditioning]
Consider fitting a polynomial of degree $M$ to $n$ points $x_1, \ldots, x_n$ uniformly spaced in $[0, 1]$. The design matrix is the \textbf{Vandermonde matrix}:
$$V = \begin{bmatrix} 1 & x_1 & x_1^2 & \cdots & x_1^M \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ 1 & x_n & x_n^2 & \cdots & x_n^M \end{bmatrix}$$

For $n = 20$ uniformly spaced points, the condition number of $V^\top V$ grows rapidly:

\begin{center}
\begin{tabular}{c|c|l}
Degree $M$ & $\kappa(V^\top V)$ & Effective precision \\
\hline
5 & $\sim 10^4$ & $\sim$12 digits \\
10 & $\sim 10^{10}$ & $\sim$6 digits \\
15 & $\sim 10^{16}$ & Nearly singular \\
20 & $> 10^{20}$ & Numerically singular
\end{tabular}
\end{center}

\textbf{Consequence}: With $M = 15$, small changes in $y$ (e.g., rounding errors) cause wild swings in $\hat{\beta}$. The OLS solution becomes meaningless despite the matrix being technically invertible.
\end{greybox}

\textbf{Why does the condition number grow so fast?} The columns of the Vandermonde matrix become increasingly similar as the degree grows. The column $[x_1^{10}, x_2^{10}, \ldots, x_n^{10}]^\top$ is very similar to $[x_1^{11}, x_2^{11}, \ldots, x_n^{11}]^\top$ because raising to a slightly higher power only slightly changes the relative values. This near-collinearity makes $X^\top X$ nearly singular.

\begin{redbox}
When $\kappa(X^\top X) \approx 10^{16}$ (machine precision), numerical solvers may return garbage. Always check the condition number before trusting OLS results with high-degree polynomials or correlated features!
\end{redbox}

\subsection{Runge's Phenomenon}

Even when we can solve the numerical equations exactly, high-degree polynomial interpolation can fail spectacularly.

\begin{greybox}[Runge's Phenomenon]
When interpolating the function $f(x) = \frac{1}{1 + 25x^2}$ on $[-1, 1]$ using a polynomial of degree $n$ through $n+1$ equally spaced points, the interpolation error \textbf{diverges} as $n \to \infty$:
$$\max_{x \in [-1,1]} |f(x) - p_n(x)| \to \infty \quad \text{as } n \to \infty$$

The polynomial fits well in the centre but oscillates wildly near the boundaries.
\end{greybox}

\textbf{Why does this happen?} Polynomials are \textbf{global approximators}-changing the polynomial anywhere affects it everywhere. With equally spaced nodes, the polynomial must ``work harder'' near the edges to pass through the interpolation points, causing oscillations that grow with degree.

\textbf{A more detailed explanation}: The function $f(x) = \frac{1}{1+25x^2}$ is smooth everywhere on the real line, but it has singularities in the complex plane at $x = \pm i/5$. These complex singularities, though invisible on the real line, limit how well polynomials can approximate the function. The approximation struggles most at the endpoints because that's where the ``pull'' from the complex singularities is strongest relative to the constraining data points.

\textbf{Connection to overfitting}: Runge's phenomenon is a deterministic analogue of overfitting. The polynomial exactly matches the data points (zero training error) but performs terribly between them (high ``test'' error). The oscillations are most severe at the boundaries-precisely where we have the least data to constrain the fit.

\begin{bluebox}[Runge's Phenomenon: Key Insights]
\begin{itemize}
    \item High-degree polynomials on equally spaced points can \textbf{diverge} rather than converge
    \item Oscillations are worst at the \textbf{boundaries} of the domain
    \item More data points (with equal spacing) makes things \textbf{worse}, not better
    \item This is fundamentally about the \textbf{global} nature of polynomial approximation
\end{itemize}
\end{bluebox}

\subsubsection{Solutions to Runge's Phenomenon}

\textbf{1. Chebyshev nodes}: Instead of equally spaced points, use nodes clustered near the boundaries:
$$x_k = \cos\left(\frac{2k-1}{2n}\pi\right), \quad k = 1, \ldots, n$$

Chebyshev nodes minimise the maximum interpolation error. With these nodes, polynomial interpolation \emph{converges} for smooth functions.

\textbf{Why Chebyshev nodes work}: They place more points near the boundaries where interpolation error tends to be largest. This gives the polynomial more ``anchor points'' in the problematic regions.

\textbf{2. Regularisation}: Rather than interpolating (passing through all points exactly), fit a \emph{regularised} polynomial that trades off data fidelity against smoothness. This prevents the wild oscillations that occur when forcing exact fit.

\textbf{3. Local methods (splines)}: Use \textbf{piecewise polynomials} that are only responsible for fitting a local region. Cubic splines, for instance, use degree-3 polynomials between each pair of knots, joined smoothly. Changes in one region don't propagate globally.

\textbf{4. Truncate polynomial degree}: Accept that beyond a certain degree, adding more terms hurts rather than helps. Use cross-validation to select the optimal degree.

\begin{redbox}
Runge's phenomenon warns: \textbf{more flexibility is not always better}. A model that perfectly fits training data may be useless for prediction. This motivates the regularisation techniques we develop in Section~\ref{sec:regularisation}.
\end{redbox}

\begin{bluebox}[Alternatives to High-Degree Polynomials]
Because polynomials are global approximators, trying to fit functions with sharp edges or rapid changes leads to high variance and oscillatory behaviour at domain boundaries. This motivates alternative approaches:
\begin{itemize}
    \item \textbf{Splines}: Piecewise polynomials providing \textit{local} approximation
    \item \textbf{Regularisation}: Penalising complexity to control oscillations
    \item \textbf{Kernel methods}: Implicit feature expansion without explicit polynomial terms
\end{itemize}
\end{bluebox}

%══════════════════════════════════════════════════════════════════════════════
\section{The Curse of Dimensionality}
%══════════════════════════════════════════════════════════════════════════════

\begin{bluebox}[Section Summary]
High-dimensional spaces behave counterintuitively: volume concentrates in thin shells near the boundary, distances between points become nearly uniform, and local methods fail because ``local'' neighbourhoods contain almost no data. These phenomena explain why flexible methods that work in low dimensions break down as $p$ grows, and why dimension reduction and regularisation become essential.
\end{bluebox}

When we expand features (e.g., polynomial terms, interactions), the effective dimension of our problem grows rapidly. This triggers a suite of counterintuitive behaviours collectively known as the \textbf{curse of dimensionality}.

The term was coined by Richard Bellman in the context of dynamic programming, where the computational cost grows exponentially with the number of state variables. In machine learning, the curse manifests as a fundamental limitation on what can be learned from finite data in high dimensions.

\subsection{Volume Concentration}

Our low-dimensional intuitions about space fail dramatically in high dimensions.

\begin{greybox}[Volume of High-Dimensional Spheres]
Consider a unit hypercube $[0,1]^p$ in $p$ dimensions. What fraction of its volume lies within a ball of radius $r$ centred at the origin?

The volume of a $p$-dimensional ball of radius $r$ is:
$$V_p(r) = \frac{\pi^{p/2}}{\Gamma(p/2 + 1)} r^p$$

As $p \to \infty$, this volume becomes negligible compared to the hypercube. Almost all the volume of the cube lies in the ``corners''-far from the centre.
\end{greybox}

\textbf{Unpacking the formula}:
\begin{itemize}
    \item $\Gamma(\cdot)$ is the gamma function, a generalisation of factorial: $\Gamma(n+1) = n!$ for integers
    \item The key insight is the $r^p$ factor: even for $r$ close to 1, raising it to a high power makes it tiny
    \item For $r = 0.9$ and $p = 100$: $0.9^{100} \approx 2.7 \times 10^{-5}$
\end{itemize}

\textbf{Consequence}: In high dimensions, data points are almost always near the boundary of any bounded region. There is essentially no ``interior.''

\begin{greybox}[Shell Concentration]
Consider a unit ball in $p$ dimensions. What fraction of its volume lies in a thin shell between radius $1-\epsilon$ and $1$?

$$\frac{V_p(1) - V_p(1-\epsilon)}{V_p(1)} = 1 - (1-\epsilon)^p \to 1 \quad \text{as } p \to \infty$$

For $p = 100$ and $\epsilon = 0.1$: the shell contains over 99.997\% of the volume.
\end{greybox}

\textbf{Derivation}: Since $V_p(r) \propto r^p$, we have:
$$\frac{V_p(1) - V_p(1-\epsilon)}{V_p(1)} = \frac{1^p - (1-\epsilon)^p}{1^p} = 1 - (1-\epsilon)^p$$

For small $\epsilon$ and large $p$, use the approximation $(1-\epsilon)^p \approx e^{-\epsilon p}$, which goes to 0 rapidly as $p$ increases.

\begin{bluebox}[Shell Concentration Intuition]
In high dimensions, essentially \textbf{all volume is near the surface}. If you sample uniformly from a high-dimensional ball, almost every point will be close to the boundary. The ``centre'' of the distribution contains virtually no probability mass.

\textbf{Analogy}: Imagine an orange in 3D-the peel is thin relative to the fruit. Now imagine a 100-dimensional orange: the ``peel'' (outer shell) contains essentially all the volume, and the ``fruit'' (interior) is negligible.
\end{bluebox}

\subsection{Distance Concentration}

\begin{greybox}[Uniform Distance Phenomenon]
Let $X_1, \ldots, X_n$ be i.i.d.\ points uniformly distributed in $[0,1]^p$. As $p \to \infty$:
$$\frac{\max_i \|X_i - X_j\|}{\min_i \|X_i - X_j\|} \to 1$$

All pairwise distances become nearly equal-the notions of ``near'' and ``far'' become meaningless.
\end{greybox}

\textbf{Intuition}: In high dimensions, the expected squared distance between two random points is:
$$\mathbb{E}[\|X - Y\|^2] = \sum_{j=1}^p \mathbb{E}[(X_j - Y_j)^2] = p \cdot \mathbb{E}[(X_1 - Y_1)^2]$$

By the law of large numbers, the average of $p$ independent terms concentrates around its expectation. Hence all distances cluster near $\sqrt{p \cdot \text{const}}$.

\textbf{More precisely}: The variance of $\|X - Y\|^2$ grows as $O(p)$, but its mean grows as $O(p)$. So the coefficient of variation (standard deviation divided by mean) decreases as $O(1/\sqrt{p})$. This means distances become increasingly concentrated around their mean.

\subsection{Implications for Machine Learning}

\begin{redbox}
Local methods (k-nearest neighbours, kernel regression, local polynomial regression) assume that nearby points behave similarly. In high dimensions:
\begin{enumerate}
    \item The ``neighbourhood'' needed to contain $k$ points grows to encompass most of the space
    \item All points are approximately equidistant, so ``nearest'' neighbours aren't meaningfully near
    \item Accurate local estimation would require exponentially many points: $n \propto k^p$
\end{enumerate}
\end{redbox}

\begin{greybox}[Sample Size Requirements]
To maintain a fixed neighbourhood size $r$ that captures a proportion $f$ of the data:
$$r \propto f^{1/p}$$

As $p$ grows, the radius must approach the diameter of the space. To maintain genuine locality, you need sample sizes that grow \textbf{exponentially} in $p$.
\end{greybox}

\textbf{Worked example}: Suppose we want to capture 1\% of the data within our neighbourhood. The required radius is:
\begin{itemize}
    \item In $p = 2$ dimensions: $r = 0.01^{1/2} = 0.1$ (10\% of the range in each dimension)
    \item In $p = 10$ dimensions: $r = 0.01^{1/10} = 0.63$ (63\% of the range in each dimension)
    \item In $p = 100$ dimensions: $r = 0.01^{1/100} = 0.955$ (95.5\% of the range in each dimension)
\end{itemize}

In 100 dimensions, to capture even 1\% of the data, we need a ``neighbourhood'' that spans 95\% of the space in every direction-this is not local at all!

This is why regularisation, dimension reduction, and structured models become essential in high dimensions: we cannot rely on local averaging when every point is isolated in a vast empty space.

\begin{bluebox}[Escaping the Curse]
\begin{enumerate}
    \item \textbf{Regularisation}: Impose structure (smoothness, sparsity) to reduce effective complexity
    \item \textbf{Dimension reduction}: PCA, feature selection, or learned embeddings
    \item \textbf{Structured models}: Linear models, additive models, neural networks with parameter sharing
    \item \textbf{Domain knowledge}: Use problem structure to identify relevant low-dimensional subspaces
\end{enumerate}

The curse of dimensionality is not a death sentence-it's a call to be smarter about how we model high-dimensional data. Real-world data often lies on or near low-dimensional manifolds, making learning possible despite the nominal dimension being high.
\end{bluebox}

%══════════════════════════════════════════════════════════════════════════════
\section{Decomposing Prediction Error}
%══════════════════════════════════════════════════════════════════════════════

\begin{bluebox}[Section Summary]
Prediction error decomposes into \textbf{approximation error} (how well the best function in our hypothesis class matches the truth) and \textbf{estimation error} (how well we can find that best function from finite data). Simple models have high approximation but low estimation error; complex models have the reverse. This formalises the bias-variance tradeoff and motivates regularisation as a principled complexity control.
\end{bluebox}

To understand model selection, we need to formalise what we're trying to minimise. This section develops the theoretical framework for understanding generalisation.

\subsection{The Bias-Variance Tradeoff: A First Look}

Before diving into the formal framework, let us recall the fundamental decomposition:

\begin{greybox}[Bias-Variance Decomposition]
For an estimator $\hat{\theta}$ of a parameter $\theta$:
$$\text{bias}(\hat{\theta}) = \mathbb{E}[\hat{\theta}] - \theta$$
$$\text{MSE}(\hat{\theta}) = \text{Var}(\hat{\theta}) + \text{bias}(\hat{\theta})^2$$
\end{greybox}

\textbf{Unpacking the decomposition}:
\begin{itemize}
    \item \textbf{Bias}: The systematic error-how far, on average, is our estimator from the truth?
    \item \textbf{Variance}: The random error-how much does our estimator vary across different samples?
    \item \textbf{MSE}: The total expected squared error, which combines both sources
\end{itemize}

As model complexity increases:
\begin{itemize}
    \item \textbf{Bias decreases}: The function can ``express'' the data better-a more flexible model can capture the true underlying pattern
    \item \textbf{Variance increases}: The function becomes harder to estimate reliably-more parameters mean more sensitivity to the particular sample
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{figures/week_03_highdim/bias_variance_MSE.png}
    \caption{Bias-variance tradeoff: as model complexity increases, bias decreases but variance increases. The optimal complexity minimises total error (MSE). This U-shaped curve is fundamental to model selection.}
    \label{fig:bias-variance-MSE}
\end{figure}

\subsection{Evaluation Metrics: Defining ``Risk''}

Different metrics capture different aspects of model performance. The choice of metric defines what ``risk'' we're trying to minimise.

\begin{greybox}[Key Concepts]
\begin{enumerate}
    \item \textbf{Model}: How you predict $f(x)$ from $X$
    \item \textbf{Loss function}: Quantifies the discrepancy between actual outcome $y$ and predicted outcome $f(x)$:
    $$\ell(y, f(x))$$
    \item \textbf{Risk}: The expected loss of a model-the metric we minimise in ERM
\end{enumerate}
\end{greybox}

``Risk'' is a generalised concept referring to the \textbf{expected loss or error of a model with respect to its predictions on new data}. It quantifies how much, on average, the model's predictions deviate from actual values according to the chosen loss function.

\begin{greybox}[Common Evaluation Metrics]
\textbf{For Classification:}
\begin{itemize}
    \item \textbf{Accuracy}: Proportion of correct predictions. Use when false positives and false negatives have similar costs.
    \item \textbf{Precision}: $\frac{\text{TP}}{\text{TP} + \text{FP}}$. Use when the cost of false positives is high (e.g., spam filtering-you don't want legitimate emails marked as spam).
    \item \textbf{Recall}: $\frac{\text{TP}}{\text{TP} + \text{FN}}$. Use when the cost of false negatives is high (e.g., disease screening-you don't want to miss actual cases).
    \item \textbf{AUC-ROC}: Area under the ROC curve. Summarises performance across all classification thresholds; higher is better. Risk could be considered as $1 - \text{AUC}$.
\end{itemize}

\textbf{For Regression:}
\begin{itemize}
    \item \textbf{MSE}: Mean squared error. Penalises large errors heavily. Directly quantifies risk as expected squared error.
    \item \textbf{MAE}: Mean absolute error. More robust to outliers.
    \item \textbf{$R^2$}: Proportion of variance explained.
\end{itemize}
\end{greybox}

\textbf{Unpacking the classification metrics}:
\begin{itemize}
    \item TP (True Positives): Correctly predicted positive cases
    \item FP (False Positives): Incorrectly predicted positive (actually negative)-Type I error
    \item FN (False Negatives): Incorrectly predicted negative (actually positive)-Type II error
    \item TN (True Negatives): Correctly predicted negative cases
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.35\linewidth]{figures/week_03_highdim/image_2.png}
    \caption{Confusion matrix showing Type I errors (false positives) and Type II errors (false negatives). These frame the loss in terms of incorrect predictions.}
    \label{fig:confusion-matrix}
\end{figure}

\begin{redbox}[Accuracy Can Be Misleading]
In imbalanced datasets, accuracy can be deceptive. A classifier that always predicts the majority class achieves high accuracy but is useless.

\textbf{Example}: If 99\% of emails are not spam, a classifier that predicts ``not spam'' for everything achieves 99\% accuracy but catches zero spam. Precision and recall provide more insight in such cases.
\end{redbox}

\subsection{Population Risk vs Empirical Risk}

\begin{greybox}[Population Risk (True Risk)]
$$R(f) = R_{f,p^*} = \mathbb{E}_{(x,y) \sim p^*}[\ell(y, f(x))]$$

The expected loss over the \textbf{true data distribution} $p^*$. This is what we ultimately care about, but we cannot compute it-we don't know $p^*$.

\textbf{Unpacking the notation:}
\begin{itemize}
    \item $R_{f,p^*}$ or $R(f)$: Population risk for model $f$
    \item $\mathbb{E}_{p^*}[\cdot]$: Expectation over the true population distribution
    \item $\ell(y, f(x))$: Loss function measuring discrepancy between true $y$ and predicted $f(x)$
\end{itemize}

Population risk is the gold standard for model performance: it indicates how well the model would perform in general, beyond just the observed data. But since the true distribution $p^*$ is unknown, direct computation is infeasible.
\end{greybox}

\textbf{What this formula means in practice}: If we could somehow draw infinitely many samples from the true distribution and evaluate our model on each one, population risk is the average loss we would observe. It represents the model's ``true'' performance on the task.

\begin{greybox}[Empirical Risk]
$$\hat{R}(f) = \frac{1}{n} \sum_{i=1}^n \ell(y_i, f(x_i))$$

The average loss over our \textbf{training sample}. This we can compute, but it's only an estimate of population risk.

The empirical risk is based on the \textbf{empirical distribution} of the sample data, which approximates the true underlying distribution.
\end{greybox}

\textbf{The gap between these two}: Empirical risk uses only the $n$ samples we have; population risk averages over all possible samples. As $n \to \infty$, empirical risk converges to population risk (by the law of large numbers), but for finite $n$ there's always a gap-and this gap is precisely what causes overfitting.

\begin{bluebox}[Empirical Risk Minimisation (ERM)]
$$\hat{f}_{\text{ERM}} = \argmin_{f \in \mathcal{H}} \hat{R}(f) = \argmin_{f \in \mathcal{H}} \frac{1}{n} \sum_{i=1}^n \ell(y_i, f(x_i))$$

Find the function in hypothesis class $\mathcal{H}$ that minimises training loss. This is the foundation of most ML algorithms.

\textbf{Components:}
\begin{itemize}
    \item $\hat{f}_{\text{ERM}}$: The model we seek
    \item $\argmin_{f \in \mathcal{H}}$: Search over hypothesis space $\mathcal{H}$
    \item The sum: Empirical risk (average loss on training data)
\end{itemize}

ERM seeks to approximate optimal population risk by minimising loss on the observed dataset.
\end{bluebox}

\begin{redbox}[The Problem with Pure ERM]
If you choose a model based on ERM alone, you will overfit to the training data and end up with a high-order polynomial (or similarly complex model)-which is problematic!

Pure ERM drives training loss toward zero while test loss remains high. We need additional considerations...
\end{redbox}

\subsection{Three Levels of Optimality}

To understand what we can and cannot achieve, we distinguish three levels of model quality:

\begin{greybox}[Hierarchy of Functions]
\begin{enumerate}
    \item \textbf{Bayes optimal}: $f^{**} = \argmin_f R(f)$
    \begin{itemize}
        \item Best possible function over \textit{all} functions
        \item Theoretical ideal; typically unachievable
        \item Minimises true risk $R(f)$ without any constraints
        \item This is an ideal function-perhaps more complex than any polynomial, perhaps discontinuous
        \item Can never be observed
    \end{itemize}

    \item \textbf{Best in class}: $f^* = \argmin_{f \in \mathcal{H}} R(f)$
    \begin{itemize}
        \item Best function within our hypothesis class $\mathcal{H}$
        \item Still uses true risk (unknown in practice)
        \item Represents the best we could achieve given our modelling choice
        \item We cannot reach $f^{**}$ if the true function is not in $\mathcal{H}$
    \end{itemize}

    \item \textbf{Empirical best}: $\hat{f}_n = \argmin_{f \in \mathcal{H}} \hat{R}(f)$
    \begin{itemize}
        \item Best function based on training data (minimises empirical risk)
        \item What we actually compute
        \item Trained on $n$ samples-a subset of the full population
        \item Aims to approximate $f^*$ by minimising observed errors
    \end{itemize}
\end{enumerate}
\end{greybox}

\textbf{The progression}: We want $f^{**}$ (the ultimate goal), we settle for $f^*$ (the best our model class can do), and we actually compute $\hat{f}_n$ (what we can find from data). Each step introduces error.

\subsection{Approximation vs Estimation Error}

The gap between what we achieve and the theoretical best decomposes into two sources. This decomposition is fundamental to understanding model selection.

\begin{greybox}[Error Decomposition]
$$\underbrace{R(\hat{f}_n) - R(f^{**})}_{\text{Total excess risk}} = \underbrace{R(f^*) - R(f^{**})}_{\text{Approximation error}} + \underbrace{R(\hat{f}_n) - R(f^*)}_{\text{Estimation error}}$$

Or equivalently, thinking of this as $R_3 - R_1 = (R_2 - R_1) + (R_3 - R_2)$.
\end{greybox}

\textbf{Unpacking the decomposition}:
\begin{itemize}
    \item \textbf{Total excess risk}: How much worse is our learned model compared to the best possible?
    \item \textbf{Approximation error}: How much worse is the best model in our class compared to the best possible?
    \item \textbf{Estimation error}: How much worse is our learned model compared to the best in our class?
\end{itemize}

\subsubsection{Approximation Error}

\textbf{Approximation Error} (also called: bias, model misspecification):
\begin{itemize}
    \item Gap between Bayes optimal ($f^{**}$) and best-in-class ($f^*$)
    \item Due to \textbf{limitations of our hypothesis class}
    \item Does \textbf{not} decrease with more data
    \item Reduced by using more expressive model classes
    \item Quantifies how well the best theoretical model in our chosen hypothesis space can approximate the true best model
    \item This error is \textbf{theory-based}-inherent to our modelling choice
\end{itemize}

\begin{bluebox}[Approximation Error: The Cost of Our Modelling Choice]
Approximation error measures how much worse our chosen model class is compared to the best possible. We can never eradicate this entirely; we can only do a better job of selecting our function class. We will always pay some cost based on our modelling choice.

\textbf{Example}: If the true relationship is a sigmoid but we only consider linear functions, no amount of data will help-our best linear fit will always have positive approximation error.
\end{bluebox}

\subsubsection{Estimation Error}

\textbf{Estimation Error} (also called: variance, generalisation error):
\begin{itemize}
    \item Gap between best-in-class ($f^*$) and what we actually learn ($\hat{f}_n$)
    \item Due to \textbf{finite training data}
    \item \textbf{Decreases} with more data (typically $O(1/\sqrt{n})$)
    \item \textbf{Increases} with model complexity (overfitting)
    \item This error is \textbf{empirically-based}-from estimating from finite samples
\end{itemize}

\begin{redbox}[Estimation Error: What We Can Control]
The estimation error is influenced by:
\begin{enumerate}
    \item \textbf{Sample size}: Decreases as $n$ increases
    \item \textbf{Model complexity}: Increases if complexity is too high relative to available data (overfitting)
\end{enumerate}

This is where we \textit{can} do something-unlike approximation error, which is fixed by our model choice.
\end{redbox}

\subsubsection{The Fundamental Tradeoff}

\begin{bluebox}[Approximation vs Estimation: The Core Tradeoff]
\begin{center}
\begin{tabular}{l|cc}
& \textbf{Approximation Error} & \textbf{Estimation Error} \\
\hline
Simple model & High & Low \\
Complex model & Low & High \\
\end{tabular}
\end{center}

More expressive models reduce approximation error but increase estimation error. The optimal model balances these.

This is analogous to the bias-variance tradeoff: we could make $\mathcal{H}$ a huge class of functions, but this increases complexity and makes estimation harder. We want to choose a model that balances the tradeoff between approximation and estimation errors.
\end{bluebox}

\begin{greybox}[Balancing the Errors]
The total difference in risk between the theoretical best possible model and our empirically best model can be understood through two fundamental challenges:
\begin{enumerate}
    \item \textbf{Choosing the right model class} (approximation error)
    \item \textbf{Accurately estimating the best model within that class from limited data} (estimation error)
\end{enumerate}

Minimising total error involves balancing these two sources. Improving the model class to reduce approximation error might increase model complexity, potentially increasing estimation error if additional data is not available.
\end{greybox}

\subsection{Estimating Generalisation Error}

We cannot compute true risk, but we can \textbf{estimate} generalisation performance using held-out data.

\begin{greybox}[Train-Test Split]
Partition data into:
\begin{itemize}
    \item \textbf{Training data} $p_{\text{train}}(x,y)$: Do ERM-minimise loss on these points, learning the underlying pattern
    \item \textbf{Testing data} $p_{\text{test}}(x,y)$: Evaluate model performance-specifically, its ability to generalise to new, unseen data
\end{itemize}

By evaluating on testing data, we can estimate generalisation error:
$$\underbrace{\mathbb{E}_{p^*} R(\hat{f}_n) - R(f^*)}_{\text{Estimation/Generalisation Error}} \approx \underbrace{\mathbb{E}_{p_{\text{test}}} [\ell(y, \hat{f}_n)]}_{\text{Test Loss}} - \underbrace{\mathbb{E}_{p_{\text{train}}} [\ell(y, \hat{f}_n)]}_{\text{Training Loss}}$$
\end{greybox}

We are comparing:
\begin{itemize}
    \item \textbf{Training Loss}: How well we \textit{thought} we did-average loss on the training dataset
    \item \textbf{Test Loss}: How well we \textit{actually} did-average loss on unseen data
\end{itemize}

\textbf{Generalisation gap}:
$$\text{Gap} = \hat{R}_{\text{test}}(\hat{f}_n) - \hat{R}_{\text{train}}(\hat{f}_n)$$

A large gap indicates overfitting: the model performs much better on training data than on new data.

\begin{redbox}[The Optimism of Pure ERM]
Generalisation/estimation error quantifies how \textbf{overly optimistic} we were when using pure ERM.

In an overfitted model:
\begin{itemize}
    \item Approximation error is basically zero (the model can express the training data perfectly)
    \item But estimation/generalisation error is high (the gap between training loss approaching zero and test loss remaining high is large)
\end{itemize}

ERM makes us overly optimistic because we are overfitting to the data-it will reduce training loss to zero but this does not translate to good test performance.
\end{redbox}

\begin{bluebox}[Key Takeaways on Generalisation]
\begin{itemize}
    \item \textbf{Generalisation gap} = difference between test and training performance. Small gap indicates good generalisation; large gap suggests overfitting.
    \item Since we cannot directly observe true expected risk over the entire distribution $p^*$, we use train/test splits to estimate how well our model will perform in practice.
    \item The testing data provides an estimate-the model's true performance could vary in completely new contexts.
\end{itemize}
\end{bluebox}

%══════════════════════════════════════════════════════════════════════════════
\section{Regularisation}
\label{sec:regularisation}
%══════════════════════════════════════════════════════════════════════════════

\begin{bluebox}[Section Summary]
Regularisation penalises model complexity to prevent overfitting. \textbf{Ridge} (L2) shrinks coefficients toward zero with a closed-form solution and guaranteed invertibility. \textbf{Lasso} (L1) produces sparse solutions, setting some coefficients exactly to zero. \textbf{Elastic net} combines both. These can be understood through multiple lenses: necessity (invertibility), bias-variance tradeoff, Bayesian priors, and geometry.
\end{bluebox}

\textbf{Regularisation} adds a penalty for model complexity, trading off fit against simplicity. It prevents models from overfitting by introducing additional information or constraints to discourage overly complex models.

\subsection{The Mechanics of Regularisation}

Overfitting occurs when a model learns patterns specific to the training data, including noise, to the extent that it performs poorly on new data. Regularisation addresses this by adding a \textbf{penalty on the size of model parameters} to the loss function.

\begin{greybox}[Regularised Objective]
$$\mathcal{L}(\theta; \lambda) = \underbrace{\frac{1}{n} \sum_{i=1}^n \ell(y_i, f(x_i; \theta))}_{\text{Loss (data fit)}} + \lambda \underbrace{C(\theta)}_{\text{Complexity penalty}}$$

where $\lambda \geq 0$ controls the regularisation strength:
\begin{itemize}
    \item $\lambda = 0$: No regularisation (pure ERM)
    \item $\lambda \to \infty$: Ignore data, minimise complexity only
\end{itemize}

The parameter $\lambda$ manages the tradeoff between fitting the data and keeping the model simple.
\end{greybox}

\textbf{Unpacking the objective}:
\begin{itemize}
    \item The first term (loss) pulls parameters toward values that fit the data well
    \item The second term (penalty) pulls parameters toward ``simple'' values (often zero)
    \item $\lambda$ controls which pull is stronger
    \item The optimal $\theta$ balances these competing objectives
\end{itemize}

\subsection{Uses of Regularisation}

\begin{enumerate}
    \item \textbf{Prevent overfitting}: By penalising large coefficients, regularisation reduces model complexity, leading to lower variance and less overfitting
    \item \textbf{Improve generalisation}: A simpler model with smaller coefficients is less sensitive to noise in training data, making it better at predicting outcomes for unseen data
    \item \textbf{Feature selection} (L1): By driving some coefficients to zero, L1 regularisation helps identify the most important features
\end{enumerate}

\subsection{Ridge Regression (L2 Regularisation)}

\begin{greybox}[Ridge Regression]
$$\mathcal{L}_{\text{ridge}}(\beta; \lambda) = \frac{1}{n} \|y - X\beta\|_2^2 + \lambda \|\beta\|_2^2$$

where $\|\beta\|_2^2 = \beta^\top \beta = \sum_j \beta_j^2$ penalises the \textbf{squared magnitude} of coefficients.

\textbf{Closed-form solution}:
$$\hat{\beta}_{\text{ridge}} = (X^\top X + \lambda I_p)^{-1} X^\top y$$

Note: When $\lambda = 0$, this reduces to the standard OLS solution.
\end{greybox}

\textbf{Deriving the Ridge solution}:

Taking the gradient of the Ridge objective and setting it to zero:
\begin{align*}
\nabla_\beta \mathcal{L} &= -\frac{2}{n} X^\top(y - X\beta) + 2\lambda \beta = 0 \\
X^\top X \beta + n\lambda \beta &= X^\top y \\
(X^\top X + n\lambda I)\beta &= X^\top y
\end{align*}

The factor of $n$ is often absorbed into $\lambda$ (different conventions exist), giving the stated solution.

\begin{bluebox}[Why Ridge Works]
\begin{enumerate}
    \item \textbf{Shrinkage}: Coefficients are pulled toward zero, reducing variance
    \item \textbf{Guaranteed invertibility}: Adding $\lambda I$ ensures $X^\top X + \lambda I$ is always invertible
    \item \textbf{Stabilises conditioning}: Increases smallest eigenvalues, reducing $\kappa$
\end{enumerate}
\end{bluebox}

\subsubsection{Ridge as Rescaled OLS}

For orthonormal $X$ (i.e., $X^\top X = I$, where each column is independent and normalised):
$$\hat{\beta}_{\text{ridge}} = (I + \lambda I)^{-1} X^\top y = \frac{1}{1+\lambda} X^\top y = \frac{\hat{\beta}_{\text{OLS}}}{1 + \lambda}$$

Ridge uniformly shrinks all coefficients toward zero by a factor of $\frac{1}{1+\lambda}$. This introduces bias but reduces variance.

\textbf{Why this matters}: In the orthonormal case, Ridge is simply scaling down the OLS solution. For general $X$, the shrinkage is more complex-coefficients corresponding to directions with small eigenvalues are shrunk more than those with large eigenvalues.

\subsubsection{Geometric Interpretation of Ridge}

Ridge regression can equivalently be written as a \textbf{constrained optimisation}:
$$\min_\beta \|y - X\beta\|_2^2 \quad \text{subject to} \quad \|\beta\|_2^2 \leq t$$

for some $t$ that depends on $\lambda$ (larger $\lambda$ corresponds to smaller $t$).

\textbf{Geometric picture}: The OLS loss function defines elliptical contours centred at $\hat{\beta}_{\text{OLS}}$. The constraint $\|\beta\|_2^2 \leq t$ is a sphere centred at the origin. The ridge solution is where the smallest loss contour touches the sphere.

\begin{center}
\begin{verbatim}
                  |
           ____   |   ____
         /      \ | /      \
        |   OLS  \|/   L2   |
        |    *--+--     |  <-- Ridge solution on sphere
        |        /|\        |
         \      / | \      /
           --   |   --
                  |
\end{verbatim}
\end{center}

Since the sphere is smooth everywhere, the ridge solution is typically in the interior of any coordinate direction-all coefficients are shrunk but none are exactly zero.

\subsection{Lasso Regression (L1 Regularisation)}

\begin{greybox}[Lasso Regression]
$$\mathcal{L}_{\text{lasso}}(\beta; \lambda) = \frac{1}{n} \|y - X\beta\|_2^2 + \lambda \|\beta\|_1$$

where $\|\beta\|_1 = \sum_j |\beta_j|$ penalises the \textbf{sum of absolute values} of coefficients.

\textbf{No closed-form solution}-requires iterative optimisation (e.g., coordinate descent).
\end{greybox}

\textbf{Why no closed-form?} The L1 penalty $|\beta_j|$ is not differentiable at $\beta_j = 0$. This non-smoothness is precisely what enables sparsity (coefficients can be exactly zero), but it means we can't simply set the gradient to zero and solve.

\subsubsection{Lasso as Soft Thresholding}

For orthonormal $X$:
$$\hat{\beta}_{\text{lasso},j} = \text{sign}(\hat{\beta}_{\text{OLS},j}) \cdot (|\hat{\beta}_{\text{OLS},j}| - \lambda)_+$$

where $(z)_+ = \max(0, z)$ denotes the positive part.

\begin{greybox}[Understanding the Lasso Formula]
\begin{itemize}
    \item $\text{sign}(\hat{\beta}^{\text{OLS}}_j)$: Preserves the direction (positive or negative) of the coefficient
    \item $|\hat{\beta}^{\text{OLS}}_j| - \lambda$: Subtracts a constant $\lambda$ from the absolute value
    \item $(\cdot)_+$: Sets result to zero if negative
\end{itemize}

This is a \textbf{thresholding function}: take the magnitude of the OLS estimate, subtract $\lambda$, and take the positive part. Small coefficients (those with $|\hat{\beta}^{\text{OLS}}_j| < \lambda$) are set exactly to zero.
\end{greybox}

\textbf{Visualising the difference}:
\begin{itemize}
    \item \textbf{Ridge}: $\hat{\beta}_{\text{ridge}} = \frac{\hat{\beta}_{\text{OLS}}}{1+\lambda}$ - multiply by a shrinkage factor (never exactly zero)
    \item \textbf{Lasso}: $\hat{\beta}_{\text{lasso}} = \text{sign}(\hat{\beta}_{\text{OLS}}) \cdot (|\hat{\beta}_{\text{OLS}}| - \lambda)_+$ - subtract and threshold (can be exactly zero)
\end{itemize}

\subsubsection{Why Lasso Produces Sparsity: Geometric Intuition}

The Lasso constraint region $\|\beta\|_1 \leq t$ is a \textbf{diamond} (in 2D) or cross-polytope (in higher dimensions). Unlike the smooth L2 ball, this constraint region has \textbf{corners} at the coordinate axes.

\begin{center}
\begin{verbatim}
           beta_2
              |
          /\  |  /\
         /  \ | /  \
        /    \|/    \
   --+--*--+--  beta_1
        \    /|\    /
         \  / | \  /
          \/  |  \/
              |
\end{verbatim}
\end{center}

When the elliptical loss contours meet the diamond constraint, they are more likely to touch at a \textbf{corner} than at a smooth edge. Corners correspond to sparse solutions-coefficients on some axes are exactly zero.

\begin{greybox}[Why Corners Mean Sparsity]
At a corner of the L1 ball, one or more coordinates are exactly zero. The probability that an ellipse first touches a polytope at a corner (rather than a face) is positive-and in high dimensions, this probability increases. This geometric fact explains why Lasso automatically performs feature selection.

\textbf{Formal intuition}: The ellipse (loss contours) is smooth; the diamond (constraint) has corners. For the ellipse to touch the diamond at a non-corner point, it must be tangent to a flat face. But generically, the ellipse will hit a corner first because corners ``stick out.''
\end{greybox}

\begin{bluebox}[Ridge vs Lasso: A Comparison]
\begin{center}
\begin{tabular}{l|cc}
& \textbf{Ridge (L2)} & \textbf{Lasso (L1)} \\
\hline
Penalty & $\|\beta\|_2^2 = \sum \beta_j^2$ & $\|\beta\|_1 = \sum |\beta_j|$ \\
Effect & Shrinks all coefficients & Sets some coefficients to exactly 0 \\
Sparsity & No & Yes (automatic feature selection) \\
Solution & Closed-form & Iterative \\
Geometry & Circular constraint & Diamond constraint \\
Orthonormal $X$ & $\frac{\hat{\beta}_{\text{OLS}}}{1+\lambda}$ & Soft thresholding \\
\end{tabular}
\end{center}

\textbf{Ridge} gives small $\beta^\top \beta$; \textbf{Lasso} makes $\beta$ \textbf{sparse} (drives coefficients to zero).
\end{bluebox}

\subsection{Elastic Net}

\begin{greybox}[Elastic Net Regression]
Combines L1 and L2 penalties:
$$\mathcal{L}_{\text{elastic}}(\beta; \lambda, \alpha) = \frac{1}{n} \|y - X\beta\|_2^2 + \lambda \left[ \alpha \|\beta\|_1 + (1-\alpha) \|\beta\|_2^2 \right]$$

where $\alpha \in [0,1]$ controls the mix between L1 and L2:
\begin{itemize}
    \item $\alpha = 1$: Pure Lasso
    \item $\alpha = 0$: Pure Ridge
    \item $\alpha \in (0,1)$: Elastic Net (hybrid)
\end{itemize}

\textbf{Alternative parameterisation} (sometimes used):
$$\mathcal{L}_{\text{elastic}}(\beta; \lambda_1, \lambda_2) = \frac{1}{n} \|y - X\beta\|_2^2 + \lambda_1 \|\beta\|_1 + \lambda_2 \|\beta\|_2^2$$
\end{greybox}

\textbf{Why combine penalties?} Lasso has two limitations that Ridge addresses:

\begin{enumerate}
    \item \textbf{Correlated features}: When features are highly correlated, Lasso tends to select one arbitrarily and zero out the others. The L2 penalty encourages the coefficients of correlated features to be similar.

    \item \textbf{$n < p$ limitation}: Lasso can select at most $n$ features (when $n < p$). Adding L2 removes this constraint.

    \item \textbf{Stability}: Small changes in data can cause Lasso to select different features from a correlated group. The L2 component stabilises the selection.
\end{enumerate}

\begin{bluebox}[When to Use Each Method]
\begin{itemize}
    \item \textbf{Ridge}: When you believe all features contribute somewhat; prediction is the goal; features are correlated
    \item \textbf{Lasso}: When you want interpretable sparse models; feature selection is important; features are relatively independent
    \item \textbf{Elastic Net}: When you want sparsity but features are correlated; $p \gg n$; stability matters
\end{itemize}
\end{bluebox}

%══════════════════════════════════════════════════════════════════════════════
\section{Multiple Perspectives on Regularisation}
%══════════════════════════════════════════════════════════════════════════════

\begin{bluebox}[Section Summary]
Regularisation isn't just a trick-it has deep justifications from multiple perspectives: \textbf{necessity} (making ill-posed problems solvable), \textbf{bias-variance} (trading off estimation error), \textbf{Bayesian} (encoding prior beliefs), \textbf{geometric} (constraining the feasible region), and \textbf{measurement error} (accounting for noise in features).
\end{bluebox}

Regularisation can be understood from several complementary viewpoints, each providing different intuition.

\subsection{Perspective 1: Necessity (Invertibility)}

\begin{greybox}[When OLS Fails]
OLS requires $(X^\top X)^{-1}$ to exist. This fails when:
\begin{itemize}
    \item $n < p$ (more features than observations)
    \item Columns of $X$ are linearly dependent (multicollinearity)
    \item Near-collinearity (numerically unstable)
\end{itemize}

The requirements for invertibility:
\begin{itemize}
    \item Non-zero determinant
    \item Full rank: each column of $X$ is linearly independent
    \item Equivalent to an eigenvalue condition: all eigenvalues are positive
\end{itemize}

Ridge \textbf{guarantees invertibility}: $(X^\top X + \lambda I)$ is always positive definite for $\lambda > 0$.
\end{greybox}

\begin{greybox}[Diagonal Dominance]
The key insight is \textbf{diagonal dominance}: if $\sum_{j \neq i} |A_{ij}| < |A_{ii}|$ for all $i$, then $A$ is invertible.

Adding $\lambda I$ makes $(X^\top X)_{ii}$ larger-the ``ridge'' dominates the matrix, eventually making it invertible.

$$\hat{\beta}_{\text{ridge}} = (X^\top X + \lambda I_p)^{-1} X^\top y$$

Here the scaling factor $\lambda$ increases the diagonal terms, guaranteeing invertibility.
\end{greybox}

\textbf{Eigenvalue perspective}: If $X^\top X$ has eigenvalues $\lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_p \geq 0$, then $X^\top X + \lambda I$ has eigenvalues $\lambda_1 + \lambda \geq \lambda_2 + \lambda \geq \cdots \geq \lambda_p + \lambda > 0$. All eigenvalues are strictly positive, guaranteeing invertibility.

\subsection{Perspective 2: Bias-Variance Tradeoff}

\begin{redbox}[Core Intuition]
Regularisation \textbf{introduces bias} in order to \textbf{reduce variance}.
\end{redbox}

\begin{greybox}[Regularisation Trades Bias for Variance]
\begin{itemize}
    \item \textbf{OLS}: Unbiased but high variance (especially with many features)
    \item \textbf{Ridge}: Biased (shrinks toward zero) but lower variance
\end{itemize}

When variance dominates (high-dimensional settings), this tradeoff improves MSE:
$$\text{MSE} = \text{Bias}^2 + \text{Variance}$$

A small increase in bias can yield a large decrease in variance.
\end{greybox}

\textbf{Why variance can dominate}: In high dimensions with limited data, OLS estimates are highly sensitive to the particular sample drawn. Different samples give wildly different coefficient estimates. By shrinking toward zero, Ridge stabilises the estimates-they become more consistent across samples (lower variance) at the cost of being systematically too small (bias).

\begin{bluebox}[Why Social Scientists Often Avoid Regularisation]
In social science and econometrics, unbiased estimation is often prioritised:
\begin{enumerate}
    \item \textbf{Causal interpretation}: Biased estimates can lead to incorrect causal conclusions
    \item \textbf{Interpretability}: Shrinkage changes the meaning of coefficients
\end{enumerate}

The philosophy is: first find an unbiased estimator, then work to reduce its variance. Ridge introduces bias deliberately (scaling down)-useful for prediction but potentially problematic for inference.

This highlights a fundamental difference between \textbf{prediction} (where bias-variance tradeoff matters) and \textbf{inference} (where unbiasedness may be paramount).
\end{bluebox}

\subsection{Perspective 3: Bayesian Interpretation (MAP)}

\begin{redbox}[Key Connection]
The Bayesian MAP estimator \textit{is} ridge regression. There is a one-to-one correspondence between regularisation and prior distributions.
\end{redbox}

\begin{greybox}[Regularisation as Prior]
Ridge regression is equivalent to MAP estimation with a Gaussian prior:
$$\beta \sim \mathcal{N}(0, \tau^2 I)$$

The regularisation parameter relates to the prior: $\lambda = \sigma^2 / \tau^2$

\begin{itemize}
    \item $\tau \to \infty$ (weak prior, large variance): Ridge $\to$ OLS (indifferent to prior)
    \item $\tau \to 0$ (strong prior, small variance): $\hat{\beta} \to 0$ (ignores data, assumes $\beta = 0$)
\end{itemize}

Similarly, Lasso corresponds to a \textbf{Laplace prior} (double exponential):
$$p(\beta_j) \propto \exp(-|\beta_j|/b)$$

The Laplace distribution has heavier tails than Gaussian but concentrates more mass at zero, explaining why Lasso produces sparse solutions.
\end{greybox}

\textbf{Why the equivalence holds}: Bayes' theorem gives:
$$p(\beta | y, X) \propto p(y | X, \beta) \cdot p(\beta)$$

Taking the log:
$$\log p(\beta | y, X) = \log p(y | X, \beta) + \log p(\beta) + \text{const}$$

With Gaussian likelihood and Gaussian prior:
\begin{align*}
\log p(y | X, \beta) &\propto -\frac{1}{2\sigma^2}\|y - X\beta\|^2 \\
\log p(\beta) &\propto -\frac{1}{2\tau^2}\|\beta\|^2
\end{align*}

Maximising the posterior (MAP) is equivalent to minimising:
$$\frac{1}{\sigma^2}\|y - X\beta\|^2 + \frac{1}{\tau^2}\|\beta\|^2$$

which is exactly the Ridge objective with $\lambda = \sigma^2/\tau^2$.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{figures/week_03_highdim/image.png}
    \caption{Bayes' theorem: the posterior $p(\theta | \mathcal{D})$ is proportional to the prior $p(\theta)$ times the likelihood $p(\mathcal{D} | \theta)$. Regularisation enters through the prior.}
    \label{fig:bayes-theorem}
\end{figure}

\begin{bluebox}[Frequentist-Bayesian Connection]
\begin{center}
\begin{tabular}{l|l}
\textbf{Regularisation} & \textbf{Prior Distribution} \\
\hline
L2 (Ridge) & Gaussian $\mathcal{N}(0, \tau^2)$ \\
L1 (Lasso) & Laplace (double exponential) \\
Elastic Net & Mixture of Gaussian and Laplace \\
None (OLS) & Uniform (improper) \\
\end{tabular}
\end{center}

This relationship highlights a beautiful crossover between frequentist (regularisation) and Bayesian approaches. Choosing a specific form of regularisation implicitly makes assumptions akin to choosing a prior in Bayesian analysis.
\end{bluebox}

\begin{greybox}[Intuitive Understanding]
If you believe the true parameters should be small (to avoid overfitting), you can express this belief by:
\begin{itemize}
    \item \textbf{Frequentist}: Imposing a penalty on parameter size (regularisation)
    \item \textbf{Bayesian}: Choosing priors that favour smaller values
\end{itemize}

Both approaches add extra information to guide the optimisation toward certain properties. Regularisation does this via a penalty term; Bayesian inference does this through priors combined with the likelihood to form posteriors.
\end{greybox}

\subsection{Perspective 4: Geometric Interpretation}

The regularised objective can be written as a constrained optimisation:
$$\min_\beta \|y - X\beta\|_2^2 \quad \text{subject to} \quad \|\beta\|_p \leq t$$

\begin{itemize}
    \item \textbf{Ridge}: Constraint region is a \textbf{sphere} ($\ell_2$ ball)
    \item \textbf{Lasso}: Constraint region is a \textbf{diamond} ($\ell_1$ ball)
\end{itemize}

The Lasso's corners at the axes explain why it produces exact zeros: the elliptical contours of the loss function are more likely to first touch the constraint at a corner, corresponding to a coordinate being exactly zero.

\textbf{Lagrangian duality}: The penalised form $\min \|y - X\beta\|^2 + \lambda\|\beta\|^2$ and the constrained form $\min \|y - X\beta\|^2$ s.t. $\|\beta\|^2 \leq t$ are equivalent via Lagrangian duality. Each value of $\lambda$ corresponds to some value of $t$, and vice versa.

\subsection{Perspective 5: Measurement Error}

Adding Gaussian noise to features is equivalent to Ridge regression:

If we observe $\tilde{X} = X + \epsilon$ where $\epsilon_{ij} \sim \mathcal{N}(0, \sigma^2)$, then OLS on $\tilde{X}$ yields:
$$\hat{\beta} \approx (X^\top X + n\sigma^2 I)^{-1} X^\top y$$

This simplifies to the ridge regression objective:
$$R(\theta) = \frac{1}{n} \sum_{i=1}^n (X\beta - y_i)^2 + \sigma^2 \|\beta\|_2^2$$

\begin{bluebox}[Intuition: Noise Breaks Spurious Correlations]
Adding Gaussian noise to features effectively shrinks coefficients. Why?

If you add infinite Gaussian noise to each feature, all relationships become random-there will be no linear relationship between features and output. Less linear relationship means smaller coefficients.

Features that don't truly predict $y$ get shrunk because noisy versions of them show no relationship. Thus, adding noise is equivalent to regularisation.
\end{bluebox}

\textbf{Practical implication}: If you believe your features are measured with error (which is almost always true in practice), Ridge regression is implicitly accounting for this measurement error. This provides another justification for why regularisation often improves predictions on real data.

%══════════════════════════════════════════════════════════════════════════════
\section{Model Selection and Validation}
%══════════════════════════════════════════════════════════════════════════════

\begin{bluebox}[Section Summary]
Hyperparameters like $\lambda$ cannot be learned from training data alone-we need held-out data. \textbf{Validation sets} provide unbiased estimates of generalisation error; \textbf{cross-validation} maximises data efficiency when samples are scarce. The test set must remain untouched until final evaluation to avoid optimistic bias.
\end{bluebox}

How do we choose the regularisation strength $\lambda$? This is a \textbf{hyperparameter}-a parameter that controls the learning process rather than being learned from data.

We want to choose hyperparameter values to \textbf{minimise generalisation error}.

\subsection{Validation Sets}

\begin{greybox}[Three-Way Split]
\begin{enumerate}
    \item \textbf{Training set} ($p_{\text{train}}$): Fit model for each candidate $\lambda$
    \item \textbf{Validation set} ($p_{\text{validation}}$): Choose $\lambda$ that minimises validation loss (model selection)
    \item \textbf{Test set} ($p_{\text{test}}$): Estimate final generalisation error (used only once!)
\end{enumerate}

This prevents ``leaking'' test information into model selection.
\end{greybox}

\textbf{Why three sets?} If we use the test set to choose $\lambda$, we're effectively fitting to the test set-our reported ``test error'' will be optimistically biased. The validation set acts as a proxy for test data during model selection, preserving the test set's integrity for final evaluation.

\begin{redbox}[Never Use the Test Set for Model Selection!]
If you repeatedly evaluate on the test set and choose the best model, you're effectively fitting to the test set and will overestimate performance on truly new data.

The test set should be touched exactly once-at the very end, to report final performance.
\end{redbox}

\subsection{Cross-Validation}

When data is limited, cross-validation reuses data for both training and validation.

\begin{greybox}[$K$-Fold Cross-Validation]
\begin{enumerate}
    \item Split data into $K$ roughly equal folds
    \item For $k = 1, \ldots, K$:
    \begin{itemize}
        \item Train on all folds except $k$
        \item Evaluate on fold $k$
    \end{itemize}
    \item Average the $K$ validation scores
\end{enumerate}

Common choices: $K = 5$ or $K = 10$

\textbf{Leave-One-Out CV} (LOOCV): $K = n$. Uses maximum data for training but computationally expensive.
\end{greybox}

\textbf{Why cross-validation works}: Each data point serves as validation data exactly once and as training data $K-1$ times. This gives us a nearly unbiased estimate of generalisation error while using all data for both training and validation (though not simultaneously).

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{figures/week_03_highdim/kplot.pdf}
    \caption{Cross-validation error as a function of model complexity (indexed on x-axis). The U-shaped curve shows the bias-variance tradeoff: too simple models underfit (high error on left), too complex models overfit (high error on right). The optimal complexity minimises CV error.}
    \label{fig:cv-error}
\end{figure}

\begin{bluebox}[Choosing $\lambda$ via CV]
\begin{enumerate}
    \item Define a grid of $\lambda$ values (e.g., $10^{-4}, 10^{-3}, \ldots, 10^2$)
    \item For each $\lambda$, compute CV score
    \item Select $\lambda^* = \argmin_\lambda \text{CV}(\lambda)$
    \item Refit on full training data with $\lambda^*$
\end{enumerate}
\end{bluebox}

\textbf{Practical considerations}:
\begin{itemize}
    \item Use a logarithmic grid for $\lambda$ (it spans many orders of magnitude)
    \item $K = 5$ or $K = 10$ works well in practice; LOOCV can have high variance
    \item Stratified folds maintain class balance in classification problems
    \item Some practitioners use ``one standard error rule'': choose the simplest model within one standard error of the minimum CV error
\end{itemize}

%══════════════════════════════════════════════════════════════════════════════
\section{Regularised Polynomial Regression}
%══════════════════════════════════════════════════════════════════════════════

\begin{bluebox}[Section Summary]
The solution to high-dimensional instability: combine expressive feature expansions with regularisation. Use high-degree polynomials for flexibility, Ridge or Lasso to control complexity, and cross-validation to tune $\lambda$. This gives smooth, stable fits without the wild oscillations of unregularised high-degree polynomials.
\end{bluebox}

Combining polynomial features with regularisation gives the best of both worlds:
\begin{itemize}
    \item \textbf{High expressivity}: Can fit complex relationships (high-dimensional model)
    \item \textbf{Controlled complexity}: Regularisation prevents overfitting
\end{itemize}

\begin{bluebox}[Recipe for Flexible Regression]
\begin{enumerate}
    \item Expand features (polynomials, interactions, etc.)
    \item Apply Ridge or Lasso regularisation
    \item Choose $\lambda$ via cross-validation
\end{enumerate}

This allows fitting smooth, complex curves without the instability of high-degree unregularised polynomials.
\end{bluebox}

\textbf{Why this works}: The polynomial expansion provides the model with the \emph{capacity} to fit complex functions. Regularisation then controls \emph{how much} of this capacity is actually used, based on what the data supports. High $\lambda$ effectively ``turns off'' the higher-order terms; low $\lambda$ lets them contribute. Cross-validation finds the right balance.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{figures/week_03_highdim/regularised_polynomial.png}
    \caption{Regularised polynomial regression. The right panels show L2 regularisation producing smooth fits even with high-degree polynomials, avoiding the oscillatory behaviour of unregularised fits. Panel (c) is particularly notable: a smooth model that doesn't exhibit typical polynomial wiggles. Regularisation allows us to get the best of both worlds-expressivity without instability.}
    \label{fig:regularised-polynomial}
\end{figure}

\textbf{Connection to neural networks}: This same principle-high capacity plus regularisation-underlies modern deep learning. Neural networks have enormous capacity (millions of parameters), but techniques like weight decay (L2 regularisation), dropout, and early stopping prevent overfitting. The lesson from polynomial regression generalises: flexibility is useful, but must be controlled.

%══════════════════════════════════════════════════════════════════════════════
\section{Summary}
%══════════════════════════════════════════════════════════════════════════════

\begin{bluebox}[Key Concepts from Week 3]
\begin{enumerate}
    \item \textbf{Polynomial regression}: Nonlinear relationships via feature expansion; still linear in parameters
    \item \textbf{Numerical instability}: High-degree polynomials have ill-conditioned $X^\top X$; condition number measures error amplification
    \item \textbf{Runge's phenomenon}: Global approximators cause edge problems; more flexibility isn't always better
    \item \textbf{Curse of dimensionality}: Volume concentrates near boundaries; distances become uniform; local methods fail
    \item \textbf{Population vs empirical risk}: We minimise the latter (what we can compute) to approximate the former (what we care about)
    \item \textbf{Three levels of optimality}: Bayes optimal $f^{**}$, best-in-class $f^*$, empirical best $\hat{f}_n$
    \item \textbf{Approximation error}: Limitation of hypothesis class (doesn't decrease with $n$)
    \item \textbf{Estimation error}: Finite-sample error (decreases with $n$, increases with complexity)
    \item \textbf{Regularisation}: Penalty on complexity to reduce overfitting; trades bias for variance
    \item \textbf{Ridge (L2)}: Shrinks coefficients, guarantees invertibility, closed-form solution
    \item \textbf{Lasso (L1)}: Sparse solutions, automatic feature selection, no closed-form
    \item \textbf{Elastic Net}: Combines L1 and L2; sparsity with stability
    \item \textbf{Multiple perspectives}: Regularisation as necessity, bias-variance tradeoff, Bayesian prior, geometric constraint, measurement error
    \item \textbf{Bayesian view}: Ridge = Gaussian prior, Lasso = Laplace prior
    \item \textbf{Cross-validation}: Choose hyperparameters without overfitting to test data
\end{enumerate}
\end{bluebox}

\begin{bluebox}[Looking Ahead]
The techniques from this week-feature expansion controlled by regularisation-form the foundation for many advanced methods:
\begin{itemize}
    \item \textbf{Kernel methods} (Week 4): Implicit feature expansion to infinite dimensions
    \item \textbf{Neural networks}: Learned feature representations with massive capacity
    \item \textbf{Sparse models}: Lasso and extensions for interpretable high-dimensional inference
\end{itemize}

The core insight persists: model flexibility must be balanced against data availability, and regularisation provides a principled way to achieve this balance.
\end{bluebox}

