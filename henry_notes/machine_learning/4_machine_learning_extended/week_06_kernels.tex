% Week 6a: Kernel Methods

\section{Introduction to Kernel Methods}

\begin{bluebox}[Chapter Overview]
This chapter develops \textbf{kernel methods}-a family of algorithms that enable nonlinear learning through the elegant ``kernel trick.'' We cover:
\begin{itemize}
    \item The dual view of regression: weighting observations by similarity
    \item Feature maps and the kernel trick: implicit high-dimensional computation
    \item Common kernels and their properties
    \item Kernel ridge regression and the representer theorem
    \item Support vector machines for classification
    \item Reproducing kernel Hilbert spaces (intuition)
    \item Practical considerations and limitations
\end{itemize}

\textbf{Key insight}: We can perform nonlinear learning while retaining the computational benefits of linear methods by working implicitly in high-dimensional feature spaces.
\end{bluebox}

\subsection{A New Way to Think About Learning}

Kernel methods represent a fundamental shift in how we think about machine learning. Throughout this course, we have primarily asked: ``Which features matter?'' We assign coefficients to features and combine them linearly. But there is another equally valid question: ``Which training examples are similar to my test point?''

This reframing-from \textit{feature-centric} to \textit{observation-centric}-leads to remarkably flexible models that can capture complex nonlinear relationships while remaining computationally tractable. The key insight is that many machine learning algorithms depend on the data only through \textit{inner products} (dot products) between data points. If we can compute these inner products efficiently in a transformed space, we gain the benefits of that transformation without paying its computational cost.

\begin{bluebox}[Core Insight]
\textbf{Traditional regression asks}: ``Which features matter?''

\textbf{Kernel methods ask}: ``Which training examples are similar to my test point?''

This reframing enables us to work with infinite-dimensional feature spaces tractably.
\end{bluebox}

\subsection{The Problem: Linear Methods Meet Nonlinear Data}

Linear models are computationally efficient and well-understood, but real-world relationships are rarely linear. Consider classifying points in $\mathbb{R}^2$ where the decision boundary is circular-no hyperplane can separate the classes.

The traditional solution is \textbf{feature expansion}: transform inputs $x$ via a feature map $\phi(x)$ into a higher-dimensional space where linear separation becomes possible. But this creates a new problem: if $\phi(x)$ is high-dimensional (or infinite-dimensional), explicit computation becomes intractable.

\begin{redbox}
The feature expansion dilemma:
\begin{itemize}
    \item \textbf{Low-dimensional features}: Computationally cheap, but may not capture nonlinear structure
    \item \textbf{High-dimensional features}: Can capture complex patterns, but computationally expensive
\end{itemize}
Kernel methods resolve this tension elegantly.
\end{redbox}

\section{An Alternative View of Regression}

Traditional regression assigns coefficients to \textit{features}. An alternative perspective assigns weights to \textit{observations} based on their similarity to the test point. This dual view is the gateway to kernel methods.

\subsection{Two Equivalent Formulations of Ridge Regression}

Consider ridge regression with the familiar solution $\hat{\beta} = (X^\top X + \lambda I_p)^{-1}X^\top y$. We can derive two equivalent prediction formulas using the matrix identity:
\[
(A + \lambda I)^{-1}A = A(A + \lambda I)^{-1}
\]

This identity-sometimes called the ``push-through'' identity-allows us to switch between inverting different matrices.

\begin{greybox}[Two Views of Ridge Regression]
Starting from the ridge regression solution $\hat{\beta} = (X^\top X + \lambda I_p)^{-1}X^\top y$, we can derive two equivalent prediction formulas:

\textbf{Feature-space view} (primal):
$$\hat{y} = X\hat{\beta}_{\text{ridge}} = X\underbrace{(X^\top X + \lambda I_p)^{-1}}_{p \times p}X^\top y$$

\textbf{Observation-space view} (dual):
$$\hat{y} = \underbrace{XX^\top}_{n \times n}(XX^\top + \lambda I_n)^{-1}y$$

\textbf{Proof of equivalence}: Let $A = X^\top$ and apply the identity with $\lambda I$ added appropriately. Alternatively, use the Woodbury matrix identity.
\end{greybox}

Let us unpack what these two formulations mean:

\textbf{Feature-space view}: We compute a $p \times p$ matrix $X^\top X$, which measures how features relate to each other across all observations. Each entry $(X^\top X)_{jk} = \sum_{i=1}^n x_{ij} x_{ik}$ is the dot product between feature columns $j$ and $k$. We then invert this matrix and multiply by the design matrix and response.

\textbf{Observation-space view}: We compute an $n \times n$ matrix $XX^\top$, which measures how observations relate to each other across all features. Each entry $(XX^\top)_{ij} = \sum_{k=1}^p x_{ik} x_{jk} = x_i^\top x_j$ is the dot product between observation rows $i$ and $j$. This captures \textbf{similarity between observations}.

\begin{bluebox}[When to Use Each View]
\begin{itemize}
    \item \textbf{Feature-space} ($p \times p$ matrix): Use when $p \ll n$ (few features, many observations)
    \item \textbf{Observation-space} ($n \times n$ matrix): Use when $p \gg n$ (many features, few observations)
\end{itemize}

The key insight: $X^\top X$ measures similarity between \textbf{features}, while $XX^\top$ measures similarity between \textbf{observations}.

After feature expansion, $p$ can become very large (even infinite), making the observation-space view essential.
\end{bluebox}

The observation-space view is particularly powerful because it depends only on pairwise similarities between data points. This is the foundation of kernel methods: we will replace these dot products with more general similarity measures.

\subsection{Similarity as Dot Product}

The observation-space view reveals that predictions depend on similarities between data points. The dot product provides a natural measure of similarity, but why? Understanding this connection is crucial for kernel methods.

\begin{greybox}[Dot Product and Distance]
The squared Euclidean distance can be written in terms of dot products:
\begin{align*}
d^2(x, x') &= \|x - x'\|^2 = \sum_{i=1}^{d} (x_i - x'_i)^2 \\
&= \sum_{i=1}^{d} x_i^2 - 2\sum_{i=1}^{d} x_ix'_i + \sum_{i=1}^{d} x'^2_i \\
&= x^\top x - 2x^\top x' + x'^\top x'
\end{align*}

For normalised vectors ($\|x\| = \|x'\| = 1$):
$$d^2(x, x') = 2(1 - x^\top x')$$

Thus \textbf{dot product} $\propto$ \textbf{similarity} $\propto$ $1 -$ \textbf{distance}.
\end{greybox}

This derivation shows that for normalised vectors, the dot product directly measures similarity: as the dot product increases, the distance decreases. The relationship $d^2 = 2(1 - x^\top x')$ makes this explicit-maximising the dot product minimises the distance.

The dot product has a beautiful geometric interpretation through the angle between vectors:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{figures/week_06_kernels/dot product.png}
    \caption{The dot product $x \cdot y = \|x\|\|y\|\cos\theta$ captures both magnitude and direction. When $\theta$ is small (similar directions), $\cos\theta$ is large and the dot product is large.}
    \label{fig:dot-product}
\end{figure}

\begin{greybox}[Dot Product as Similarity Measure]
$$x \cdot y = \langle x, y \rangle = x^\top y = \|x\|\|y\|\cos\theta$$

This formula shows the dot product depends on:
\begin{enumerate}
    \item \textbf{Magnitudes} ($\|x\|$ and $\|y\|$): Longer vectors produce larger dot products
    \item \textbf{Direction} ($\cos\theta$): Vectors pointing similarly produce larger dot products
\end{enumerate}

The directional component $\cos\theta$ is called \textbf{cosine similarity}:
$$\text{cosine similarity} = \frac{x \cdot y}{\|x\|\|y\|} = \cos\theta$$

This ranges from $-1$ (opposite directions) through $0$ (orthogonal) to $+1$ (same direction).
\end{greybox}

\begin{bluebox}[Dot Product Interpretation]
$$x \cdot y = \|x\|\|y\|\cos\theta$$
\begin{itemize}
    \item $\theta = 0^\circ$: Parallel vectors $\Rightarrow$ maximum similarity
    \item $\theta = 90^\circ$: Orthogonal vectors $\Rightarrow$ no similarity
    \item $\theta = 180^\circ$: Anti-parallel vectors $\Rightarrow$ maximum dissimilarity
\end{itemize}

\textbf{Key intuition}: If similar, angle is small $\rightarrow$ cosine is large $\rightarrow$ dot product is large.

\textbf{Cosine similarity} normalises by magnitude: $\cos\theta = \frac{x \cdot y}{\|x\|\|y\|}$
\end{bluebox}

\subsection{Regression as Weighted Averaging}

In the observation-space view, prediction becomes a weighted average of training labels:

\begin{greybox}[Prediction as Similarity-Weighted Average]
For a test point $\tilde{x}$:
$$\hat{y}(\tilde{x}) = \sum_{i=1}^n w_i y_i$$

where the weights are:
$$w = \tilde{x}^\top X(XX^\top + \lambda I_n)^{-1}$$

Each weight $w_i$ reflects how similar training point $x_i$ is to the test point $\tilde{x}$.
\end{greybox}

Let us unpack this formula. The term $\tilde{x}^\top X$ computes the dot product between the test point $\tilde{x}$ and each column of $X^\top$-that is, each training observation. These raw similarities are then transformed by the matrix $(XX^\top + \lambda I_n)^{-1}$, which accounts for the relationships among training points and the regularisation.

The result is intuitive: observations more similar to $\tilde{x}$ receive higher weights in the prediction. We are ``taking a walk'' through the training data, finding observations similar to our test point, and using their labels to make predictions. The regularisation parameter $\lambda$ prevents overfitting by smoothing the weights.

\begin{redbox}
In linear regression, weights evolve \textbf{linearly} with the dot product $\tilde{x}^\top x_i$. This is inflexible-we may want nearby points to have much higher weight than distant ones, or we may want similarity to depend on nonlinear relationships.

Consider predicting at $\tilde{x} = 2$: with linear weighting, a training point at $x = 3$ might receive more weight than one at $x = 2.01$, simply due to the global structure of the linear model.

This motivates \textbf{nonlinear similarity measures}: kernels.
\end{redbox}

\subsection{The Importance of Defining Similarity Correctly}

Different notions of similarity lead to fundamentally different models. Our choice of similarity measure determines the kinds of functions we can learn-and the wrong choice can be disastrous.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{figures/week_06_kernels/covariance issues.png}
    \caption{All four datasets have the same correlation coefficient, but their structures differ dramatically. Linear correlation (based on Euclidean distance) cannot distinguish them. This is Anscombe's quartet.}
    \label{fig:covariance-issues}
\end{figure}

\begin{redbox}
\textbf{Critical insight}: Our measure of similarity determines the kinds of functions we can learn.

If we only measure linear correlation based on Euclidean distance, we will estimate the same covariance structure for all four datasets above. Different problems demand different notions of distance and similarity.

If you define distance differently, you get different measures of similarity. Two points that are ``close'' in Euclidean distance may not be ``close'' in a transformed feature space-and this flexibility is precisely what makes kernel methods powerful.
\end{redbox}

\section{Feature Maps and the Kernel Trick}

\begin{bluebox}[Section Summary: The Kernel Trick]
\textbf{The problem}: We want to work in high-dimensional feature spaces for expressive power, but computing $\phi(x)$ explicitly is expensive or impossible.

\textbf{The solution}: Many algorithms depend on data only through dot products $\langle x_i, x_j \rangle$. If we can compute $k(x, x') = \langle \phi(x), \phi(x') \rangle$ directly without computing $\phi$, we get the benefits of high-dimensional features at low cost.

\textbf{The insight}: A \textbf{kernel function} $k(x, x')$ computes inner products in feature space implicitly.
\end{bluebox}

\subsection{Feature Expansion}

To capture nonlinear relationships with linear models, we transform the input space. This is the idea of \textit{feature expansion} that we have used throughout this course-polynomial regression and Fourier basis expansions are examples.

\begin{greybox}[Feature Map]
A \textbf{feature map} $\phi: \mathcal{X} \to \mathcal{H}$ transforms inputs from the original space $\mathcal{X}$ into a (typically higher-dimensional) feature space $\mathcal{H}$.

\textbf{Examples:}
\begin{itemize}
    \item \textbf{Polynomial}: $\phi(x) = [1, x, x^2, \ldots, x^M]^\top$ for $x \in \mathbb{R}$
    \item \textbf{Trigonometric}: $\phi(x) = [1, \cos(x), \sin(x), \cos(2x), \sin(2x), \ldots]^\top$
    \item \textbf{Interaction terms}: $\phi([x_1, x_2]^\top) = [x_1, x_2, x_1 x_2, x_1^2, x_2^2]^\top$
\end{itemize}

In the expanded space, linear models can capture nonlinear relationships in the original space.
\end{greybox}

\textbf{Why this works}: A function that is nonlinear in $x$ may be linear in $\phi(x)$. For example, $y = ax^2 + bx + c$ is nonlinear in $x$ but linear in $\phi(x) = [1, x, x^2]^\top$ with coefficients $[c, b, a]^\top$. The key insight is that linear regression in a transformed feature space is equivalent to nonlinear regression in the original space.

\textbf{Concrete example}: For a 2D input $x = [x_1, x_2]^\top$, a polynomial expansion might be:
$$\phi(x) = [x_1, x_2, x_1^2, x_2^2, x_1 x_2]^\top$$
This maps 2 features to 5 features, enabling the model to capture quadratic relationships.

\textbf{The problem}: As feature dimension grows, computation becomes expensive:
\begin{itemize}
    \item Polynomial features of degree $M$ in $d$ dimensions: $\binom{d + M}{M}$ features
    \item For $d = 100$, $M = 5$: over 96 million features
    \item Some kernels correspond to \textit{infinite}-dimensional feature spaces
\end{itemize}

Computing $\phi(x)^\top\phi(x')$ explicitly becomes intractable when $\phi$ is very high- or infinite-dimensional.

\subsection{The Kernel Trick}

The kernel trick is one of the most elegant ideas in machine learning. It allows us to work in high-dimensional (even infinite-dimensional) feature spaces without ever explicitly computing the features.

\begin{greybox}[Kernel Definition]
A \textbf{kernel} is a function $k: \mathcal{X} \times \mathcal{X} \to \mathbb{R}$ that computes the inner product in some feature space:
$$k(x, x') = \langle \phi(x), \phi(x') \rangle_{\mathcal{H}}$$

for some feature map $\phi: \mathcal{X} \to \mathcal{H}$.

The \textbf{kernel trick}: In algorithms that depend on data only through inner products $\langle x_i, x_j \rangle$, replace these with kernel evaluations $k(x_i, x_j)$ to implicitly work in the feature space $\mathcal{H}$.
\end{greybox}

The beauty of kernel methods is their ability to \textit{implicitly} compute dot products in high-dimensional feature spaces without ever explicitly constructing the feature vectors $\phi(x)$ and $\phi(x')$.

To summarise the key objects:
\begin{itemize}
    \item $\phi(x)$ is a \textbf{feature expansion}: a function that transforms input $x$ into a higher-dimensional space
    \item $k(x, x') = \phi(x)^\top\phi(x')$ is a \textbf{kernel function}: computes similarity in the expanded space
    \item The kernel is a \textbf{similarity metric}-it measures how similar two inputs are in the transformed feature space
\end{itemize}

Let us see this in action with a concrete example.

\begin{greybox}[Worked Example: Polynomial Kernel]
Consider $x, z \in \mathbb{R}^2$ and the kernel $k(x, z) = (x^\top z)^2$.

\textbf{Step 1}: Expand the kernel algebraically.
\begin{align*}
k(x, z) &= (x^\top z)^2 = (x_1 z_1 + x_2 z_2)^2 \\
&= x_1^2 z_1^2 + 2x_1 x_2 z_1 z_2 + x_2^2 z_2^2
\end{align*}

\textbf{Step 2}: Identify this as an inner product.
$$k(x, z) = \begin{bmatrix} x_1^2 \\ \sqrt{2} x_1 x_2 \\ x_2^2 \end{bmatrix}^\top \begin{bmatrix} z_1^2 \\ \sqrt{2} z_1 z_2 \\ z_2^2 \end{bmatrix} = \phi(x)^\top \phi(z)$$

where $\phi(x) = (x_1^2, \sqrt{2} x_1 x_2, x_2^2)^\top$.

\textbf{Computational comparison}:
\begin{itemize}
    \item Direct kernel evaluation: 2 multiplications + 1 addition + 1 squaring = $O(d)$
    \item Explicit feature computation: Compute $\phi(x)$, $\phi(z)$, then inner product = $O(d^2)$
\end{itemize}

The kernel computes the same result more efficiently, and this gap widens dramatically for higher degrees.
\end{greybox}

Notice the $\sqrt{2}$ factor in the feature map. This is necessary to make the cross-term $2x_1 x_2 z_1 z_2$ emerge from the dot product. The kernel ``knows'' about this scaling implicitly-we never need to compute it.

\begin{greybox}[General Polynomial Kernel]
For the kernel $k(x, z) = (x^\top z + c)^M$ with $x, z \in \mathbb{R}^d$:
\begin{itemize}
    \item The implicit feature space has dimension $\binom{d + M}{M}$
    \item Direct kernel evaluation: $O(d)$ operations
    \item Explicit feature computation: $O\left(\binom{d+M}{M}\right)$ operations
\end{itemize}

For $d = 100$, $M = 3$: kernel takes $O(100)$ operations; explicit features would take $O(176,851)$ operations.

For $d = 100$, $M = 10$: kernel takes $O(100)$ operations; explicit features would require computing in a space of dimension $\binom{110}{10} \approx 10^{13}$!
\end{greybox}

\subsection{The Gram Matrix}

When applying kernel methods to a dataset, we need to compute all pairwise kernel evaluations. This information is collected in the \textit{Gram matrix}.

\begin{greybox}[Gram Matrix]
The \textbf{Gram matrix} (or kernel matrix) $K \in \mathbb{R}^{n \times n}$ contains all pairwise kernel evaluations:
$$K_{ij} = k(x_i, x_j) = \langle \phi(x_i), \phi(x_j) \rangle$$

In terms of the feature matrix $\Phi = [\phi(x_1), \ldots, \phi(x_n)]^\top$:
$$K = \Phi \Phi^\top$$

\textbf{Properties}:
\begin{itemize}
    \item $K$ is symmetric: $K_{ij} = K_{ji}$
    \item $K$ is positive semi-definite: $\alpha^\top K \alpha \geq 0$ for all $\alpha \in \mathbb{R}^n$
\end{itemize}
\end{greybox}

The Gram matrix replaces $XX^\top$ in kernelised algorithms. We store $K$ (which is $n \times n$) rather than $\Phi$ (which could be $n \times \infty$). This is the key computational insight: regardless of how high-dimensional the implicit feature space is, we only ever work with the $n \times n$ Gram matrix.

\textbf{Interpretation}: Entry $K_{ij}$ measures the similarity between observations $i$ and $j$ in the feature space. A valid kernel produces a positive semi-definite Gram matrix, meaning all eigenvalues are $\geq 0$. This ensures the kernel ``looks like'' a dot product-it must arise from some (possibly implicit) feature map.

\section{What Makes a Valid Kernel?}

Not every function $k(x, x')$ corresponds to an inner product in some feature space. Mercer's theorem characterises valid kernels.

\begin{bluebox}[Section Summary: Valid Kernels]
A function $k(x, x')$ is a valid kernel if and only if:
\begin{enumerate}
    \item It is \textbf{symmetric}: $k(x, x') = k(x', x)$
    \item It is \textbf{positive semi-definite}: The Gram matrix $K$ has all eigenvalues $\geq 0$ for any finite set of points
\end{enumerate}

This ensures $k$ corresponds to an inner product in \textit{some} Hilbert space.
\end{bluebox}

\begin{greybox}[Mercer's Theorem (Informal)]
Let $k: \mathcal{X} \times \mathcal{X} \to \mathbb{R}$ be a continuous symmetric function. Then $k$ is a valid kernel (i.e., corresponds to an inner product in some Hilbert space) if and only if it is \textbf{positive semi-definite}:

For any finite set $\{x_1, \ldots, x_n\} \subset \mathcal{X}$ and any $\alpha \in \mathbb{R}^n$:
$$\sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j k(x_i, x_j) \geq 0$$

Equivalently, the Gram matrix $K_{ij} = k(x_i, x_j)$ is positive semi-definite for all finite point sets.

\textbf{Interpretation}: PSD ensures that ``distances'' computed via $k$ behave sensibly-you cannot have negative squared distances in the implicit feature space.
\end{greybox}

The condition $\sum_{i,j} \alpha_i \alpha_j k(x_i, x_j) \geq 0$ can be written in matrix form as $\alpha^\top K \alpha \geq 0$. This is precisely the definition of a positive semi-definite matrix. Intuitively, this ensures that the ``geometry'' induced by the kernel is consistent-distances are non-negative, the triangle inequality holds, and so forth.

\textbf{Why does this matter?} If we try to use a function that is not a valid kernel, we lose the guarantees that make kernel methods work. The optimisation problems may become non-convex, solutions may not exist, and the interpretation as working in a feature space breaks down.

\subsection{Constructing Kernels}

One of the most powerful aspects of kernel methods is that kernels can be combined to express complex notions of similarity. The following closure properties allow us to build sophisticated kernels from simpler building blocks.

\begin{greybox}[Closure Properties of Kernels]
If $k_1$ and $k_2$ are valid kernels, then so are:
\begin{enumerate}
    \item $\alpha k_1$ for any $\alpha \geq 0$ (scaling)
    \item $k_1 + k_2$ (sum)
    \item $k_1 \cdot k_2$ (product)
    \item $f(x) k_1(x, x') f(x')$ for any function $f: \mathcal{X} \to \mathbb{R}$
    \item $g(k_1(x, x'))$ where $g$ is a polynomial with non-negative coefficients
    \item $\exp(k_1(x, x'))$ (exponential)
\end{enumerate}

These rules let us build complex kernels from simpler building blocks.
\end{greybox}

Let us understand why these rules work:

\textbf{Sum of kernels}: If $k_1$ corresponds to feature map $\phi_1$ and $k_2$ to $\phi_2$, then $k_1 + k_2$ corresponds to the concatenated feature map $[\phi_1; \phi_2]$. The resulting feature space is the direct sum of the individual spaces.

\textbf{Product of kernels}: The product $k_1 \cdot k_2$ corresponds to a feature space containing all pairwise products of features from $\phi_1$ and $\phi_2$. This creates an even higher-dimensional space capturing interactions.

\textbf{Exponential of kernels}: This follows from the Taylor expansion $e^x = \sum_{n=0}^\infty x^n/n!$ and the fact that powers of valid kernels are valid (via the product rule).

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{figures/week_06_kernels/manifpulating_kernels.png}
    \caption{Rules for constructing valid kernels from simpler ones. Any combination that preserves positive semi-definiteness yields a valid kernel.}
    \label{fig:kernel-manipulation}
\end{figure}

\begin{bluebox}[Kernel Combinations]
Custom kernels can encode domain knowledge about similarity:
$$k_{\text{custom}} = \alpha \cdot k_{\text{local}} + (1-\alpha) \cdot k_{\text{global}}$$

\textbf{Example applications}:
\begin{itemize}
    \item \textbf{Strings}: Count common substrings
    \item \textbf{Graphs}: Compare random walk distributions
    \item \textbf{Images}: Combine spatial and colour similarity
    \item \textbf{Time series}: Combine trend and seasonal components
\end{itemize}

The hyperparameter $\alpha$ balances the contributions of different similarity notions.
\end{bluebox}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{figures/week_06_kernels/combining kernels.png}
    \caption{Combining kernels: Gaussian (local), periodic (cyclical), and mixed. Adjusting $\sigma^2$ combines wide global structure with high-frequency local variation.}
    \label{fig:combining-kernels}
\end{figure}

The figure illustrates how different kernels capture different aspects of structure:
\begin{itemize}
    \item \textbf{Gaussian kernel}: Captures locality-nearby points are similar
    \item \textbf{Periodic kernel}: Captures cyclical patterns-e.g., days of the week have similarity to each other
    \item \textbf{Combined}: A low-frequency wide global model combined with a high-frequency local model
\end{itemize}

This flexibility allows you to express prior beliefs about your data: perhaps there is a cyclical time trend, or one part of the feature space benefits from local models while another requires global structure.

\section{Common Kernels}

\begin{bluebox}[Section Summary: Common Kernels]
\begin{center}
\begin{tabular}{lll}
\textbf{Kernel} & \textbf{Formula} & \textbf{Feature Dimension} \\
\hline
Linear & $k(x,z) = x^\top z$ & $d$ \\
Polynomial & $k(x,z) = (x^\top z + c)^M$ & $\binom{d+M}{M}$ \\
RBF/Gaussian & $k(x,z) = \exp\left(-\frac{\|x-z\|^2}{2\sigma^2}\right)$ & $\infty$ \\
\end{tabular}
\end{center}

\textbf{Rule of thumb}: Start with RBF for general problems; use polynomial when interactions up to a specific degree are meaningful; use linear as a baseline.
\end{bluebox}

\subsection{Linear Kernel}

The simplest kernel is the linear kernel, which corresponds to no feature transformation at all.

\begin{greybox}[Linear Kernel]
$$k(x, z) = x^\top z$$

\textbf{Properties}:
\begin{itemize}
    \item Feature map: $\phi(x) = x$ (identity)
    \item Feature dimension: $d$ (same as input)
    \item Corresponds to standard linear methods
\end{itemize}

\textbf{When to use}:
\begin{itemize}
    \item As a baseline before trying nonlinear kernels
    \item When $d \gg n$ (high-dimensional sparse data, e.g., text)
    \item When relationships are approximately linear
\end{itemize}

The linear kernel provides no additional expressiveness but serves as a useful baseline and is computationally cheapest.
\end{greybox}

Using the linear kernel in a kernel algorithm is equivalent to using the corresponding non-kernelised algorithm. For example, kernel ridge regression with a linear kernel is equivalent to standard ridge regression. This makes the linear kernel a useful sanity check: if a nonlinear kernel does not improve over linear, either the relationship is genuinely linear or the nonlinear kernel is poorly tuned.

\subsection{Polynomial Kernel}

The polynomial kernel implicitly computes dot products in a space containing all polynomial terms up to a specified degree.

\begin{greybox}[Polynomial Kernel]
$$k(x, z) = (x^\top z + c)^M$$

\textbf{Parameters}:
\begin{itemize}
    \item $M$: Degree (higher = more complex decision boundaries)
    \item $c \geq 0$: Trade-off between lower and higher-order terms
    \begin{itemize}
        \item $c = 0$: Homogeneous polynomial (only degree-$M$ terms)
        \item $c > 0$: Includes all terms from degree $0$ to $M$
    \end{itemize}
\end{itemize}

\textbf{Feature space}: Contains all monomials up to degree $M$:
$$\phi(x) \propto [\ldots, x_{i_1}^{a_1} x_{i_2}^{a_2} \cdots x_{i_k}^{a_k}, \ldots]$$
where $a_1 + a_2 + \cdots + a_k \leq M$.

\textbf{When to use}:
\begin{itemize}
    \item When interaction effects between features matter
    \item When domain knowledge suggests polynomial relationships
    \item Image processing (e.g., polynomial SVMs for digit recognition)
\end{itemize}
\end{greybox}

The constant $c$ controls which polynomial terms are emphasised. Let us see this with a detailed example.

\begin{greybox}[Detailed Example: Polynomial Kernel with $c > 0$]
Let $x, z \in \mathbb{R}^2$ and $k(x, z) = (x^\top z + 1)^2$.

Expanding:
\begin{align*}
k(x,z) &= (x_1 z_1 + x_2 z_2 + 1)^2 \\
&= x_1^2 z_1^2 + x_2^2 z_2^2 + 1 + 2x_1 x_2 z_1 z_2 + 2x_1 z_1 + 2x_2 z_2 \\
&= \phi(x)^\top \phi(z)
\end{align*}

where $\phi(x) = (x_1^2, x_2^2, 1, \sqrt{2}x_1 x_2, \sqrt{2}x_1, \sqrt{2}x_2)^\top$.

This feature space includes:
\begin{itemize}
    \item Constant term: $1$ (from $c = 1$)
    \item Linear terms: $x_1$, $x_2$
    \item Quadratic terms: $x_1^2$, $x_2^2$, $x_1 x_2$
\end{itemize}
\end{greybox}

Notice that with $c = 1$, the feature space includes terms of all degrees from 0 to 2. If we had used $c = 0$ (the homogeneous polynomial kernel), only the degree-2 terms would appear. The choice of $c$ thus determines whether lower-order terms contribute to similarity.

\subsection{Gaussian (RBF) Kernel}

The Gaussian kernel, also called the Radial Basis Function (RBF) kernel, is perhaps the most widely used kernel. It corresponds to an infinite-dimensional feature space and can approximate any continuous function.

\begin{greybox}[Gaussian/RBF Kernel]
$$k(x, z) = \exp\left(-\frac{\|x - z\|^2}{2\sigma^2}\right) = \exp\left(-\gamma \|x - z\|^2\right)$$

where $\gamma = \frac{1}{2\sigma^2}$.

\textbf{Properties}:
\begin{itemize}
    \item \textbf{Bounded}: $k(x, z) \in (0, 1]$, with $k(x, x) = 1$
    \item \textbf{Local}: Similarity decays exponentially with distance
    \item \textbf{Infinite-dimensional}: Corresponds to an infinite-dimensional feature space
\end{itemize}

\textbf{Parameters}:
\begin{itemize}
    \item Small $\sigma$ (large $\gamma$): Highly local; only very close points are similar
    \item Large $\sigma$ (small $\gamma$): More global; distant points retain similarity
\end{itemize}
\end{greybox}

The formula has an intuitive interpretation:
\begin{enumerate}
    \item Take the difference between $x$ and $z$
    \item Square the differences and sum them (squared Euclidean distance)
    \item Normalise by $2\sigma^2$
    \item Apply the exponential (which decays as distance increases)
\end{enumerate}

The result is a similarity measure that equals 1 when $x = z$ and decays smoothly towards 0 as points become distant. The bandwidth parameter $\sigma$ controls how quickly this decay happens.

\subsubsection{Why the RBF Kernel is Infinite-Dimensional}

The RBF kernel's power comes from its implicit infinite-dimensional feature space. We can see this by expanding the kernel using the Taylor series.

\begin{greybox}[Why RBF is Infinite-Dimensional]
Expanding the RBF kernel using Taylor series:
\begin{align*}
k(x, z) &= \exp\left(-\frac{\|x\|^2}{2\sigma^2}\right) \exp\left(\frac{x^\top z}{\sigma^2}\right) \exp\left(-\frac{\|z\|^2}{2\sigma^2}\right) \\
&= e^{-\|x\|^2/2\sigma^2} \cdot e^{-\|z\|^2/2\sigma^2} \cdot \sum_{k=0}^{\infty} \frac{(x^\top z)^k}{\sigma^{2k} k!}
\end{align*}

Each term $(x^\top z)^k$ corresponds to a polynomial kernel of degree $k$. Thus the RBF kernel implicitly uses an \textbf{infinite-dimensional} feature space containing all polynomial terms, with higher-degree terms down-weighted by $\frac{1}{\sigma^{2k} k!}$.

\textbf{Intuition}: The RBF kernel ``prefers'' smoother (lower-degree) functions through this weighting, while retaining the flexibility to capture arbitrary local variation when the data demands it.
\end{greybox}

The key insight from the Taylor expansion is that the RBF kernel contains \textit{all} polynomial terms of all degrees, but with weights that decrease rapidly as the degree increases. The $k$-th component of the implicit feature map has the form:
$$\phi(x)_k \propto \exp\left(-\frac{\|x\|^2}{2\sigma^2}\right) \frac{x^k}{\sigma^k\sqrt{k!}}$$

Because $k!$ grows extremely rapidly, \textbf{higher-degree terms are weighted down} exponentially. This means the RBF kernel ``prefers'' smoother, lower-degree functions while retaining flexibility for local variation.

This is analogous to regularisation in Fourier series, where we down-weight high-frequency components (e.g., $\cos(mx)/m$ for large $m$). The Gaussian kernel achieves similar smoothness implicitly through its infinite-dimensional feature space.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{figures/week_06_kernels/kernels.png}
    \caption{Left: Linear regression (rigid, global). Right: RBF kernel regression allows locally-weighted similarity, adapting flexibly to the data structure.}
    \label{fig:kernels-comparison}
\end{figure}

\begin{bluebox}[Choosing $\sigma$ for RBF]
The bandwidth $\sigma$ controls the bias-variance trade-off:
\begin{itemize}
    \item \textbf{Small $\sigma$}: High variance, low bias (can fit noise; decision boundaries follow individual points)
    \item \textbf{Large $\sigma$}: Low variance, high bias (overly smooth; approaches linear)
\end{itemize}

\textbf{Heuristics}:
\begin{itemize}
    \item Median heuristic: Set $\sigma$ to the median pairwise distance
    \item Cross-validation: Search over $\gamma \in \{10^{-3}, 10^{-2}, \ldots, 10^3\}$
\end{itemize}
\end{bluebox}

The bandwidth $\sigma$ is perhaps the most important hyperparameter in RBF kernel methods. Too small, and the model overfits-each training point becomes an isolated ``island'' with no influence on neighbours. Too large, and the model underfits-all points are considered similar, reducing to a constant prediction.

\subsection{Other Common Kernels}

Beyond polynomial and RBF kernels, many other kernels exist for specific applications:

\begin{itemize}
    \item \textbf{Periodic kernel}: $k(x, z) = \exp\left(-\frac{2\sin^2(\pi|x-z|/p)}{\sigma^2}\right)$ for cyclical/seasonal patterns
    \item \textbf{String kernels}: For text and sequence data, measuring similarity based on shared substrings
    \item \textbf{Graph kernels}: For structured/relational data, comparing random walk distributions or subgraph patterns
    \item \textbf{Mat\'ern kernels}: A family generalising RBF with controllable differentiability
\end{itemize}

The choice of kernel encodes your beliefs about the structure of similarity in your problem. Domain expertise can guide this choice-for time series with known periodicity, a periodic kernel is natural; for molecular data, graph kernels respect molecular structure.

\section{Kernel Ridge Regression}

\begin{bluebox}[Section Summary: Kernel Ridge Regression]
Kernel ridge regression performs ridge regression in the feature space defined by a kernel, without explicitly computing features:
$$\hat{y}(\tilde{x}) = k_{\tilde{x}}^\top (K + \lambda I)^{-1} y$$

Key insights:
\begin{itemize}
    \item Solves an $n \times n$ system (not $D \times D$ where $D$ may be infinite)
    \item The representer theorem guarantees the solution lies in the span of training kernel evaluations
    \item Computational cost: $O(n^3)$ for training, $O(n)$ for prediction
\end{itemize}
\end{bluebox}

\subsection{Derivation via the Dual}

We now derive kernel ridge regression by starting from ridge regression in the (implicit) feature space and showing that we never need to compute features explicitly.

\begin{greybox}[Primal Problem]
Ridge regression in feature space minimises:
$$\min_w \frac{1}{2}\|y - \Phi w\|^2 + \frac{\lambda}{2}\|w\|^2$$

where $\Phi = [\phi(x_1), \ldots, \phi(x_n)]^\top \in \mathbb{R}^{n \times D}$ is the feature matrix.

The primal solution is:
$$w^* = (\Phi^\top \Phi + \lambda I_D)^{-1} \Phi^\top y$$

\textbf{Problem}: If $D$ is large or infinite, this is intractable.
\end{greybox}

This is precisely the situation where the kernel trick saves us. The key insight comes from the representer theorem, which tells us that even though $w$ lives in a potentially infinite-dimensional space, we can express it using only $n$ coefficients.

\begin{greybox}[The Representer Theorem]
\textbf{Theorem}: For any regularised empirical risk minimisation problem of the form:
$$\min_w \sum_{i=1}^n L(y_i, \langle w, \phi(x_i) \rangle) + \Omega(\|w\|)$$

where $L$ is any loss function and $\Omega$ is a strictly increasing function, the optimal solution has the form:
$$w^* = \sum_{i=1}^n \alpha_i \phi(x_i) = \Phi^\top \alpha$$

for some $\alpha \in \mathbb{R}^n$.

\textbf{Interpretation}: Even though $w$ lives in a potentially infinite-dimensional space, it can be written as a linear combination of the (finite) training feature vectors. We need only find the $n$ coefficients $\alpha_i$.
\end{greybox}

The representer theorem is one of the most important results in kernel methods. It says that no matter how high-dimensional the feature space is, the optimal solution can always be written as a linear combination of the training feature vectors. This reduces an infinite-dimensional optimisation problem to a finite-dimensional one.

\textbf{Intuition}: Why should $w^*$ lie in the span of $\{\phi(x_1), \ldots, \phi(x_n)\}$? The regulariser $\|w\|^2$ penalises any component of $w$ that is orthogonal to this span. Since such components do not affect predictions on training data (they are orthogonal to all $\phi(x_i)$), they only add to the penalty without reducing the loss. Thus the optimal $w^*$ has no such components.

\begin{greybox}[Dual Derivation]
\textbf{Step 1}: Apply the representer theorem. Since $w^* = \Phi^\top \alpha$:
\begin{itemize}
    \item Predictions: $\Phi w^* = \Phi \Phi^\top \alpha = K\alpha$
    \item Regulariser: $\|w^*\|^2 = \alpha^\top \Phi \Phi^\top \alpha = \alpha^\top K \alpha$
\end{itemize}

\textbf{Step 2}: Rewrite the objective in terms of $\alpha$:
$$\min_\alpha \frac{1}{2}\|y - K\alpha\|^2 + \frac{\lambda}{2} \alpha^\top K \alpha$$

\textbf{Step 3}: Take derivative and set to zero:
\begin{align*}
\frac{\partial}{\partial \alpha} &= -K(y - K\alpha) + \lambda K\alpha = 0 \\
&\Rightarrow K(K + \lambda I)\alpha = Ky \\
&\Rightarrow \alpha^* = (K + \lambda I)^{-1}y
\end{align*}

\textbf{Step 4}: Prediction for new point $\tilde{x}$:
$$\hat{y}(\tilde{x}) = \langle w^*, \phi(\tilde{x}) \rangle = \sum_{i=1}^n \alpha_i^* k(x_i, \tilde{x}) = k_{\tilde{x}}^\top (K + \lambda I)^{-1} y$$

where $k_{\tilde{x}} = [k(\tilde{x}, x_1), \ldots, k(\tilde{x}, x_n)]^\top$.
\end{greybox}

Let us trace through the key steps:

\textbf{Step 1}: Using $w^* = \Phi^\top \alpha$, the predictions become $\Phi w^* = \Phi \Phi^\top \alpha = K\alpha$, where $K = \Phi \Phi^\top$ is the Gram matrix. Similarly, $\|w^*\|^2 = \alpha^\top \Phi \Phi^\top \alpha = \alpha^\top K \alpha$. Notice that everything now depends on $\Phi$ only through $K$.

\textbf{Step 2}: Substituting into the ridge regression objective gives us an optimisation problem in $\alpha$ that involves only the kernel matrix $K$.

\textbf{Step 3}: Taking derivatives and solving, we get $\alpha^* = (K + \lambda I)^{-1}y$. This involves inverting an $n \times n$ matrix, which is tractable even when the feature space is infinite-dimensional.

\textbf{Step 4}: For prediction, we compute kernel evaluations between the test point and all training points, then take a weighted combination.

\begin{greybox}[Kernel Ridge Regression: Final Form]
\textbf{Training}: Compute the Gram matrix $K_{ij} = k(x_i, x_j)$ and solve:
$$\alpha^* = (K + \lambda I_n)^{-1}y$$

\textbf{Prediction}: For new point $\tilde{x}$:
$$\hat{y}(\tilde{x}) = \sum_{i=1}^n \alpha_i^* k(\tilde{x}, x_i) = k_{\tilde{x}}^\top \alpha^*$$

\textbf{Complexity}:
\begin{itemize}
    \item Gram matrix: $O(n^2)$ kernel evaluations
    \item Matrix inversion: $O(n^3)$
    \item Prediction: $O(n)$ per test point
\end{itemize}
\end{greybox}

\textbf{Properties of Kernel Ridge Regression:}
\begin{itemize}
    \item \textbf{Global}: All training points contribute to each prediction (though distant points may contribute little with localised kernels)
    \item \textbf{Flexible}: Choice of kernel determines the notion of similarity
    \item \textbf{Closed-form}: Unlike many kernel methods, KRR has an analytical solution
\end{itemize}

\textbf{Hyperparameters to tune:}
\begin{itemize}
    \item Choice of kernel (and its parameters, e.g., $\sigma$ for RBF)
    \item Regularisation parameter $\lambda$
\end{itemize}

\begin{bluebox}[Computational Trade-off]
\begin{center}
\begin{tabular}{lcc}
& \textbf{Primal (explicit features)} & \textbf{Dual (kernel)} \\
\hline
Training & $O(nd^2 + d^3)$ & $O(n^2 d_k + n^3)$ \\
Prediction & $O(d)$ & $O(n)$ \\
\end{tabular}
\end{center}

Where $d_k$ is the cost of one kernel evaluation.

\textbf{Use primal when}: $d$ is moderate and $n$ is large.

\textbf{Use dual when}: $d$ is very large (or infinite) and $n$ is moderate.
\end{bluebox}

The prediction cost deserves attention: with kernel methods, prediction requires $O(n)$ kernel evaluations (one for each training point), whereas with explicit features, prediction is $O(d)$. This means kernel methods become expensive for prediction when $n$ is large, even though training remains tractable.

\section{Support Vector Machines}

\begin{bluebox}[Section Summary: Support Vector Machines]
SVMs are maximum-margin classifiers that:
\begin{itemize}
    \item Find the hyperplane that separates classes with the \textbf{largest margin}
    \item Use only a subset of training points (\textbf{support vectors}) for prediction
    \item Handle non-separable data via \textbf{soft margins} and \textbf{slack variables}
    \item Achieve nonlinear decision boundaries via the \textbf{kernel trick}
\end{itemize}

Key formula (dual): $\alpha^* = \argmax_\alpha \sum_i \alpha_i - \frac{1}{2}\sum_{i,j} \alpha_i \alpha_j y_i y_j k(x_i, x_j)$ subject to constraints.
\end{bluebox}

Support Vector Machines (SVMs) were one of the most successful machine learning algorithms before the deep learning era, and remain important today. They combine the kernel trick with a clever objective function that leads to sparse solutions.

\subsection{Maximum Margin Classification}

Consider binary classification with labels $y_i \in \{-1, +1\}$. A linear classifier predicts:
$$\hat{y}(x) = \text{sign}(w^\top x + b)$$

If the data is linearly separable, infinitely many hyperplanes achieve zero training error. Which should we choose?

The SVM answer is: choose the hyperplane that maximises the \textit{margin}-the distance from the hyperplane to the nearest training point.

\begin{greybox}[Geometric Margin]
For a hyperplane $w^\top x + b = 0$, the \textbf{geometric margin} of a point $(x_i, y_i)$ is:
$$\gamma_i = y_i \cdot \frac{w^\top x_i + b}{\|w\|}$$

This is the signed distance from $x_i$ to the hyperplane, positive if correctly classified.

The \textbf{margin} of the classifier is:
$$\gamma = \min_i \gamma_i$$

the distance from the hyperplane to the nearest point.
\end{greybox}

Let us unpack this formula. The quantity $(w^\top x_i + b)/\|w\|$ is the signed distance from point $x_i$ to the hyperplane (this follows from the geometry of hyperplanes). Multiplying by $y_i$ makes this positive when the point is correctly classified. The margin $\gamma = \min_i \gamma_i$ is thus the distance to the nearest point, which is also the point most likely to be misclassified under small perturbations.

\begin{greybox}[Why Maximise Margin?]
\textbf{Geometric intuition}: A larger margin means the classifier is more ``confident''-small perturbations to test points are less likely to change predictions.

\textbf{Statistical intuition}: Margin is inversely related to VC dimension. Larger margin $\Rightarrow$ lower complexity $\Rightarrow$ better generalisation bounds.

\textbf{Robustness}: Maximum margin classifiers are optimal under certain noise models (e.g., bounded perturbations).
\end{greybox}

The statistical argument is particularly important. The margin controls the ``effective complexity'' of the classifier: a large-margin classifier uses less of its capacity to fit the training data, leaving more capacity for generalisation. This connects to the bias-variance trade-off-larger margin means more bias (simpler decision boundary) but less variance (more stable under perturbations).

\subsection{Hard-Margin SVM}

The hard-margin SVM assumes the data is linearly separable and finds the maximum-margin hyperplane.

\begin{greybox}[Hard-Margin SVM: Primal Formulation]
\textbf{Objective}: Maximise the margin while correctly classifying all points.

Since $\gamma = 1/\|w\|$ when we normalise so that $\min_i |w^\top x_i + b| = 1$, maximising margin is equivalent to minimising $\|w\|$:

$$\min_{w, b} \frac{1}{2}\|w\|^2 \quad \text{subject to} \quad y_i(w^\top x_i + b) \geq 1 \quad \forall i$$

\textbf{Constraints}: Each training point must be on the correct side of the margin boundary.
\end{greybox}

The normalisation $\min_i |w^\top x_i + b| = 1$ fixes the scale of $w$ (since we can always rescale $w$ and $b$ together). With this normalisation, the margin is exactly $1/\|w\|$, so maximising margin is equivalent to minimising $\|w\|^2$ (the square is for mathematical convenience-it makes the problem quadratic).

\begin{greybox}[Hard-Margin SVM: Dual Formulation]
Using Lagrange multipliers $\alpha_i \geq 0$ for each constraint:

\textbf{Lagrangian}:
$$\mathcal{L}(w, b, \alpha) = \frac{1}{2}\|w\|^2 - \sum_{i=1}^n \alpha_i [y_i(w^\top x_i + b) - 1]$$

\textbf{KKT conditions} (setting derivatives to zero):
\begin{align*}
\frac{\partial \mathcal{L}}{\partial w} = 0 &\Rightarrow w = \sum_{i=1}^n \alpha_i y_i x_i \\
\frac{\partial \mathcal{L}}{\partial b} = 0 &\Rightarrow \sum_{i=1}^n \alpha_i y_i = 0
\end{align*}

\textbf{Dual problem} (substituting back):
$$\max_\alpha \sum_{i=1}^n \alpha_i - \frac{1}{2}\sum_{i,j} \alpha_i \alpha_j y_i y_j (x_i^\top x_j)$$
$$\text{subject to} \quad \alpha_i \geq 0, \quad \sum_i \alpha_i y_i = 0$$

\textbf{Key observation}: The objective depends on data only through inner products $x_i^\top x_j$!
\end{greybox}

The dual formulation is remarkable for two reasons:

1. \textbf{It depends only on inner products}: This means we can apply the kernel trick directly, replacing $x_i^\top x_j$ with $k(x_i, x_j)$.

2. \textbf{It leads to sparse solutions}: The KKT conditions imply that most $\alpha_i = 0$, so only a subset of training points (the support vectors) contribute to the solution.

\begin{greybox}[Support Vectors]
By the KKT complementary slackness conditions:
$$\alpha_i [y_i(w^\top x_i + b) - 1] = 0$$

Either $\alpha_i = 0$ (point is not used) or $y_i(w^\top x_i + b) = 1$ (point is exactly on the margin).

\textbf{Support vectors}: Points with $\alpha_i > 0$. These are the points lying exactly on the margin boundaries $w^\top x + b = \pm 1$.

\textbf{Prediction} depends only on support vectors:
$$\hat{y}(x) = \text{sign}\left(\sum_{i: \alpha_i > 0} \alpha_i y_i (x_i^\top x) + b\right)$$

\textbf{Sparsity}: Typically only a small fraction of training points are support vectors, making prediction efficient.
\end{greybox}

The complementary slackness condition is the key to understanding support vectors. It says that for each constraint, either the Lagrange multiplier is zero (constraint is not active) or the constraint is satisfied with equality (point is exactly on the margin). Points strictly inside the margin have $\alpha_i = 0$ and do not contribute to $w$.

This sparsity is a major advantage of SVMs over kernel ridge regression. In KRR, all training points contribute to predictions; in SVMs, only the support vectors matter. For large datasets, this can mean dramatic computational savings at prediction time.

\subsection{Soft-Margin SVM}

Real data is rarely linearly separable. Soft-margin SVMs allow some misclassification via \textbf{slack variables}.

\begin{greybox}[Soft-Margin SVM: Primal Formulation]
Introduce slack variables $\xi_i \geq 0$ to allow margin violations:

$$\min_{w, b, \xi} \frac{1}{2}\|w\|^2 + C\sum_{i=1}^n \xi_i$$
$$\text{subject to} \quad y_i(w^\top x_i + b) \geq 1 - \xi_i, \quad \xi_i \geq 0$$

\textbf{Interpretation}:
\begin{itemize}
    \item $\xi_i = 0$: Point correctly classified outside margin
    \item $0 < \xi_i < 1$: Point correctly classified but inside margin
    \item $\xi_i \geq 1$: Point misclassified
\end{itemize}

\textbf{Parameter $C$}: Trade-off between margin maximisation and error tolerance.
\begin{itemize}
    \item Large $C$: Penalise errors heavily; small margin, few violations
    \item Small $C$: Accept more errors for larger margin
\end{itemize}
\end{greybox}

The slack variables $\xi_i$ measure how much each point violates the margin constraint. The objective $C\sum_i \xi_i$ penalises these violations, with the parameter $C$ controlling the severity of the penalty.

\textbf{Geometric picture}: Without slack variables, all points must be outside the margin (on the correct side). With slack variables, points can ``pay a price'' (proportional to $\xi_i$) to violate this constraint. The parameter $C$ sets the exchange rate between margin size and violations.

\begin{greybox}[Soft-Margin SVM: Dual Formulation]
$$\max_\alpha \sum_{i=1}^n \alpha_i - \frac{1}{2}\sum_{i,j} \alpha_i \alpha_j y_i y_j (x_i^\top x_j)$$
$$\text{subject to} \quad 0 \leq \alpha_i \leq C, \quad \sum_i \alpha_i y_i = 0$$

The only change from hard-margin: upper bound $C$ on the dual variables.

\textbf{Support vector types}:
\begin{itemize}
    \item $\alpha_i = 0$: Point correctly classified, not on margin
    \item $0 < \alpha_i < C$: Point exactly on margin ($\xi_i = 0$)
    \item $\alpha_i = C$: Point inside margin or misclassified ($\xi_i > 0$)
\end{itemize}
\end{greybox}

The dual formulation of soft-margin SVM is nearly identical to hard-margin, with only the addition of the upper bound $\alpha_i \leq C$. This elegant result follows from the Lagrangian analysis.

The categorisation of support vectors into three types is useful for understanding the solution:
\begin{itemize}
    \item Points with $\alpha_i = 0$ are ``easy''-well inside the correct region
    \item Points with $0 < \alpha_i < C$ are ``on the fence''-exactly on the margin
    \item Points with $\alpha_i = C$ are ``difficult''-inside the margin or misclassified
\end{itemize}

\begin{redbox}
\textbf{Connection to regularisation}: The soft-margin SVM objective can be written as:
$$\min_w \frac{1}{n}\sum_{i=1}^n \max(0, 1 - y_i w^\top x_i) + \frac{1}{2C}\|w\|^2$$

This is regularised empirical risk with:
\begin{itemize}
    \item \textbf{Loss}: Hinge loss $\ell(y, f) = \max(0, 1 - yf)$
    \item \textbf{Regularisation}: L2 penalty with $\lambda = 1/C$
\end{itemize}

Large $C$ (small $\lambda$) means less regularisation.
\end{redbox}

This reformulation reveals that SVMs are simply regularised empirical risk minimisation with a particular loss function (the hinge loss). The hinge loss is zero when the point is correctly classified with margin $\geq 1$, and grows linearly with the margin violation. This contrasts with squared loss (which penalises even correct predictions that are ``too confident'') and logistic loss (which always has positive loss).

\subsection{Kernel SVMs}

Since the SVM dual depends on data only through inner products, we can apply the kernel trick.

\begin{greybox}[Kernel SVM]
Replace $x_i^\top x_j$ with $k(x_i, x_j)$:

\textbf{Dual problem}:
$$\max_\alpha \sum_{i=1}^n \alpha_i - \frac{1}{2}\sum_{i,j} \alpha_i \alpha_j y_i y_j k(x_i, x_j)$$
$$\text{subject to} \quad 0 \leq \alpha_i \leq C, \quad \sum_i \alpha_i y_i = 0$$

\textbf{Prediction}:
$$\hat{y}(x) = \text{sign}\left(\sum_{i \in SV} \alpha_i y_i k(x_i, x) + b\right)$$

where $SV = \{i : \alpha_i > 0\}$ is the set of support vectors.

\textbf{Computing $b$}: Use any support vector with $0 < \alpha_i < C$ (on the margin):
$$b = y_j - \sum_{i \in SV} \alpha_i y_i k(x_i, x_j)$$
\end{greybox}

The kernel SVM inherits all the benefits of the kernel trick: we can work in infinite-dimensional feature spaces (using e.g.\ the RBF kernel) while solving a finite-dimensional optimisation problem. Combined with the sparsity of the SVM solution, this makes kernel SVMs practical for many problems.

\begin{bluebox}[SVM vs Kernel Ridge Regression]
\begin{center}
\begin{tabular}{lcc}
& \textbf{Kernel SVM} & \textbf{Kernel Ridge} \\
\hline
Task & Classification & Regression \\
Loss & Hinge & Squared \\
Solution & Sparse (only SVs) & Dense (all points) \\
Optimisation & Quadratic program & Linear system \\
Prediction cost & $O(|SV|)$ & $O(n)$ \\
\end{tabular}
\end{center}

SVMs are preferred when sparsity (fast prediction) matters; kernel ridge is simpler to implement.
\end{bluebox}

The choice between SVM and kernel ridge regression depends on the application:
\begin{itemize}
    \item \textbf{Classification vs regression}: SVMs are designed for classification; kernel ridge regression handles both but is more natural for regression
    \item \textbf{Prediction speed}: SVMs can be much faster when few support vectors exist
    \item \textbf{Implementation complexity}: KRR just requires solving a linear system; SVMs require quadratic programming
    \item \textbf{Probability estimates}: SVMs give hard classifications; KRR gives continuous predictions that can be interpreted as probabilities (with appropriate post-processing)
\end{itemize}

\section{Reproducing Kernel Hilbert Spaces}

\begin{bluebox}[Section Summary: RKHS Intuition]
Every kernel defines a unique function space called an RKHS. Key ideas:
\begin{itemize}
    \item The kernel $k(x, \cdot)$ is a ``feature'' that measures similarity to $x$
    \item Functions in the RKHS are weighted combinations of these features
    \item The RKHS norm measures function ``complexity'' (smoothness)
    \item Regularisation in RKHS yields the representer theorem
\end{itemize}
\end{bluebox}

Reproducing Kernel Hilbert Spaces (RKHS) provide the theoretical foundation for kernel methods. While a full treatment requires functional analysis, we can develop useful intuition.

\begin{greybox}[RKHS Definition (Informal)]
A \textbf{Reproducing Kernel Hilbert Space} (RKHS) $\mathcal{H}_k$ associated with kernel $k$ is a Hilbert space of functions $f: \mathcal{X} \to \mathbb{R}$ with the \textbf{reproducing property}:
$$f(x) = \langle f, k(x, \cdot) \rangle_{\mathcal{H}_k}$$

\textbf{Interpretation}: Evaluating $f$ at $x$ is the same as taking the inner product with the kernel function centred at $x$.

\textbf{Key consequences}:
\begin{itemize}
    \item $k(x, x') = \langle k(x, \cdot), k(x', \cdot) \rangle$ - kernel is inner product of ``features''
    \item Functions in $\mathcal{H}_k$ are (infinite) linear combinations: $f = \sum_i \alpha_i k(x_i, \cdot)$
\end{itemize}
\end{greybox}

The reproducing property is the defining characteristic of an RKHS. It says that the kernel function $k(x, \cdot)$ (which maps any $x' \mapsto k(x, x')$) acts as a ``representer'' for point evaluation. To evaluate $f$ at $x$, we simply take the inner product with this representer.

\textbf{Intuition}: Think of $k(x, \cdot)$ as a ``basis function centred at $x$.'' Functions in the RKHS are weighted combinations of these basis functions. The reproducing property says that evaluating $f$ at $x$ extracts the ``$x$-component'' of $f$.

\begin{greybox}[RKHS Norm and Smoothness]
The RKHS norm $\|f\|_{\mathcal{H}_k}$ measures the ``complexity'' of $f$.

For $f = \sum_i \alpha_i k(x_i, \cdot)$:
$$\|f\|_{\mathcal{H}_k}^2 = \sum_{i,j} \alpha_i \alpha_j k(x_i, x_j) = \alpha^\top K \alpha$$

\textbf{Interpretation for RBF kernel}:
\begin{itemize}
    \item Large $\|f\|_{\mathcal{H}_k}$: Function varies rapidly
    \item Small $\|f\|_{\mathcal{H}_k}$: Function is smooth
\end{itemize}

\textbf{Connection to regularisation}: Penalising $\|f\|_{\mathcal{H}_k}^2$ encourages smooth functions, explaining why kernel methods generalise well.
\end{greybox}

The RKHS norm connects directly to the regularisation term in kernel methods. In kernel ridge regression, we minimise:
$$\sum_{i=1}^n (y_i - f(x_i))^2 + \lambda \|f\|_{\mathcal{H}_k}^2$$

The regulariser $\|f\|_{\mathcal{H}_k}^2$ penalises complex functions, and for the RBF kernel, this corresponds to penalising rapid variation. This explains why kernel ridge regression produces smooth predictions-the regulariser explicitly encourages smoothness.

\begin{bluebox}[Why RKHS Matters]
\begin{enumerate}
    \item \textbf{Representer theorem}: Solutions to regularised problems lie in finite-dimensional subspaces
    \item \textbf{Kernel choice = prior}: Different kernels encode different notions of smoothness
    \item \textbf{Infinite dimensions, finite computation}: The kernel trick works because of RKHS structure
    \item \textbf{Connections}: Links kernel methods to Gaussian processes, Bayesian inference, and functional analysis
\end{enumerate}
\end{bluebox}

The RKHS perspective illuminates several aspects of kernel methods:
\begin{itemize}
    \item \textbf{Why the representer theorem holds}: The regulariser penalises components orthogonal to the span of training kernel functions
    \item \textbf{What the kernel encodes}: Each kernel defines a different RKHS with a different notion of ``simple'' functions
    \item \textbf{Connection to Gaussian processes}: The RKHS is the ``support'' of a Gaussian process with covariance function $k$
\end{itemize}

\section{Related Methods}

\subsection{K-Nearest Neighbours}

While not a kernel method per se, KNN shares the similarity-based philosophy and provides a useful comparison.

\begin{greybox}[KNN Regression]
Predict using only the $K$ most similar training points:
$$\hat{y}(\tilde{x}) = \frac{1}{K}\sum_{i \in N_K(\tilde{x})} y_i$$

where $N_K(\tilde{x})$ is the set of $K$ nearest neighbours.
\end{greybox}

\textbf{Properties of KNN}:
\begin{itemize}
    \item \textbf{Exclusively local}: Only nearby points influence predictions (unlike kernel ridge regression)
    \item \textbf{Simple}: No training phase-just store the data
    \item \textbf{Non-parametric}: Makes no assumptions about functional form
    \item \textbf{Bias-variance tradeoff}:
    \begin{itemize}
        \item Small $K$: High variance (sensitive to noise)
        \item Large $K$: High bias (oversmoothing, missing local patterns)
    \end{itemize}
\end{itemize}

\textbf{Comparison with kernel methods}:
\begin{itemize}
    \item \textbf{KNN}: Hard weighting (equal weight to $K$ neighbours, zero to others)
    \item \textbf{Kernel methods}: Soft weighting (continuous decay with distance)
    \item \textbf{KNN}: No training phase; store data and compute at prediction time
    \item \textbf{Kernel methods}: Training phase (solve linear system or QP); prediction uses learned weights
\end{itemize}

\begin{redbox}
KNN uses simple averaging: all $K$ neighbours contribute equally. This ignores the possibility that some neighbours are much closer than others, or that relationships vary across the input space. Kernel methods offer more nuanced weighting.
\end{redbox}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{figures/week_06_kernels/kernel_1.png}
    \caption{Kernels can capture both global structure and local variation. The kernel induces low-rank global structure (capturing overall trends) while allowing for local adaptation.}
    \label{fig:kernel-structure-1}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{figures/week_06_kernels/kernel_2.png}
    \caption{Different features may demand different notions of similarity. By combining or learning kernel parameters, models can discover which features (or combinations) are most indicative of similarity.}
    \label{fig:kernel-structure-2}
\end{figure}

\begin{bluebox}[Balancing Structure and Flexibility]
Effective kernel methods combine:
\begin{enumerate}
    \item \textbf{Low-rank global structure}: Broad trends that apply across the dataset (like PCA)
    \item \textbf{Local variation}: Fine-grained patterns that differ across regions
    \item \textbf{Feature-specific similarity}: Different features may contribute differently to similarity
\end{enumerate}

There are many ways to construct custom kernels for your application. More structure (when correct) makes learning easier by reducing the hypothesis space.
\end{bluebox}

\section{Practical Considerations}

\begin{bluebox}[Section Summary: Practical Considerations]
\begin{itemize}
    \item \textbf{Kernel selection}: Start with RBF; tune via cross-validation
    \item \textbf{Hyperparameters}: $\sigma$ (RBF), $C$ (SVM), $\lambda$ (KRR) - critical for performance
    \item \textbf{Scaling}: $O(n^2)$ storage, $O(n^3)$ training - problematic for large $n$
    \item \textbf{Approximations}: Random features, Nystr\"om approximation for scalability
\end{itemize}
\end{bluebox}

\subsection{Kernel Selection}

\begin{greybox}[Choosing a Kernel]
\textbf{General guidelines}:
\begin{itemize}
    \item \textbf{RBF}: Default choice; works well in most situations
    \item \textbf{Linear}: When $d \gg n$ (text, genomics) or data is approximately linear
    \item \textbf{Polynomial}: When interaction effects are known to matter
    \item \textbf{Custom}: When domain knowledge suggests specific similarity structure
\end{itemize}

\textbf{Hyperparameter tuning}:
\begin{itemize}
    \item Grid search with cross-validation
    \item For RBF: $\gamma \in \{2^{-15}, 2^{-13}, \ldots, 2^3\}$
    \item For SVM: $C \in \{2^{-5}, 2^{-3}, \ldots, 2^{15}\}$
\end{itemize}

\textbf{Data preprocessing}:
\begin{itemize}
    \item \textbf{Standardise features}: Critical for RBF (distance-based)
    \item RBF is not scale-invariant; features on different scales will be weighted differently
\end{itemize}
\end{greybox}

The importance of preprocessing cannot be overstated. The RBF kernel computes distances, and if features are on different scales, the kernel will be dominated by the largest-scale feature. Standardising (zero mean, unit variance) puts all features on equal footing.

\subsection{Computational Scaling}

\begin{greybox}[Computational Complexity]
\begin{center}
\begin{tabular}{lcc}
\textbf{Operation} & \textbf{Time} & \textbf{Space} \\
\hline
Compute Gram matrix & $O(n^2 d)$ & $O(n^2)$ \\
Kernel ridge regression & $O(n^3)$ & $O(n^2)$ \\
SVM (typical) & $O(n^2)$ to $O(n^3)$ & $O(n^2)$ \\
Prediction (KRR) & $O(n)$ per point & - \\
Prediction (SVM) & $O(|SV|)$ per point & - \\
\end{tabular}
\end{center}

\textbf{Bottleneck}: For $n = 100,000$, storing $K$ requires 80GB (double precision). Matrix inversion is intractable.
\end{greybox}

The $O(n^2)$ storage and $O(n^3)$ computation are fundamental limitations of kernel methods. For large datasets, we need approximations.

\begin{greybox}[Scalability Approximations]
\textbf{Random Fourier Features} (Rahimi \& Recht, 2007):

For shift-invariant kernels (like RBF), approximate:
$$k(x, z) \approx \hat{\phi}(x)^\top \hat{\phi}(z)$$

where $\hat{\phi}(x) \in \mathbb{R}^D$ is a \textit{random} feature map with $D \ll n$.

\textbf{Procedure}:
\begin{enumerate}
    \item Sample $D$ frequencies $\omega_j$ from the kernel's Fourier transform
    \item Compute $\hat{\phi}(x) = \sqrt{\frac{2}{D}}[\cos(\omega_1^\top x + b_1), \ldots, \cos(\omega_D^\top x + b_D)]$
    \item Use standard linear methods on the random features
\end{enumerate}

\textbf{Complexity}: $O(nD)$ instead of $O(n^2)$.

\textbf{Nystr\"om approximation}: Approximate $K$ using a subset of $m \ll n$ landmark points.
\end{greybox}

Random features are a powerful technique for scaling kernel methods. The key insight is that for shift-invariant kernels, Bochner's theorem guarantees a Fourier representation. By sampling from this representation, we can construct a finite-dimensional approximation to the infinite-dimensional feature space.

\begin{redbox}
\textbf{When NOT to use kernel methods}:
\begin{itemize}
    \item \textbf{Large $n$}: When $n > 10,000$--$100,000$, consider random features or neural networks
    \item \textbf{Streaming data}: Kernel methods require storing all training data
    \item \textbf{Very high-dimensional features}: Distance becomes meaningless (curse of dimensionality)
\end{itemize}
\end{redbox}

\section{The Curse of Dimensionality}

Kernel methods rely on meaningful notions of distance. In high dimensions, distance becomes problematic.

\begin{redbox}
In high dimensions, \textbf{distance becomes meaningless}: all points become approximately equidistant. This fundamentally limits similarity-based methods.
\end{redbox}

\subsection{Why Distance Fails in High Dimensions}

Consider data uniformly distributed in a $d$-dimensional hypercube:

\begin{greybox}[Volume in High Dimensions]
For $X \sim \text{Uniform}(-1, 1)^d$:
\begin{itemize}
    \item Volume of hypercube: $2^d$
    \item Volume of $\epsilon$-ball: $V_d(\epsilon) = \frac{\pi^{d/2}}{\Gamma(d/2 + 1)}\epsilon^d$
\end{itemize}

The fraction of the hypercube within distance $\epsilon$ of any point shrinks exponentially with $d$.

\textbf{Consequence}: To capture a fixed fraction of nearby points, $\epsilon$ must grow with $d$. ``Local'' methods become global.
\end{greybox}

This is a fundamental geometric fact. As dimension increases, the volume of a sphere (relative to a cube) shrinks exponentially. Most of the volume of a high-dimensional cube is concentrated near its corners, far from the centre.

What does this mean practically? To capture a fixed fraction of data points as ``neighbours,'' we need $\epsilon$ to grow dramatically with dimension:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{figures/week_06_kernels/vol_dim_table.png}
    \caption{To capture 10\% of data as $d$ increases, $\epsilon$ must grow dramatically. In high dimensions, the neighbourhood radius approaches the boundary of the space.}
    \label{fig:vol-dim-table}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{figures/week_06_kernels/vol_dim_graph.png}
    \caption{Volume concentration in high dimensions: most volume lies near the boundary, and the concept of ``local neighbourhood'' becomes meaningless.}
    \label{fig:vol-dim-graph}
\end{figure}

\begin{greybox}[Distance Concentration]
For points uniformly distributed in high dimensions:
$$\frac{\max_{i,j} \|x_i - x_j\| - \min_{i,j} \|x_i - x_j\|}{\min_{i,j} \|x_i - x_j\|} \to 0 \quad \text{as } d \to \infty$$

All pairwise distances become nearly equal. The distinction between ``near'' and ``far'' vanishes.
\end{greybox}

This is the most devastating consequence of high dimensionality for similarity-based methods. When all points are approximately equidistant, there is no meaningful notion of ``nearest neighbour'' or ``most similar.'' The kernel matrix approaches a constant matrix, losing all discriminative power.

\subsection{Implications for Machine Learning}

\begin{bluebox}[Implications and Mitigations]
\begin{itemize}
    \item In high dimensions, local methods become global (everything is far)
    \item Prediction becomes \textbf{extrapolation} rather than interpolation
    \item \textbf{Dimensionality reduction} (PCA, autoencoders) can help
    \item Careful \textbf{feature selection} is essential
    \item For truly high-dimensional data, consider:
    \begin{itemize}
        \item Linear kernels (scale better)
        \item Random features with careful dimension choice
        \item Deep learning (learns relevant features)
    \end{itemize}
\end{itemize}
\end{bluebox}

\textbf{Remedies}:
\begin{itemize}
    \item \textbf{Dimensionality reduction}: PCA, t-SNE, autoencoders can project data to lower dimensions where distance is meaningful
    \item \textbf{Careful feature selection}: Only include features relevant to the prediction task
    \item \textbf{Structured models}: Use domain knowledge to constrain the model (e.g., convolutional structure for images)
    \item \textbf{Deep learning}: Neural networks can learn low-dimensional representations where distance is meaningful
\end{itemize}

\begin{redbox}
\textbf{When NOT to use kernel methods (summary)}:
\begin{itemize}
    \item \textbf{High-dimensional features}: Distance becomes meaningless; everything is far apart
    \item \textbf{Large $n$}: The $n \times n$ kernel matrix becomes prohibitively large
    \item \textbf{Streaming/online settings}: Must store all training data
\end{itemize}

Kernel methods shine with moderate-sized datasets in low-to-moderate dimensions, where domain knowledge can inform kernel choice.
\end{redbox}

\section{Summary}

\begin{bluebox}[Key Concepts from Kernel Methods]
\begin{enumerate}
    \item \textbf{Dual view}: Regression can weight observations by similarity, not just features by coefficients

    \item \textbf{Kernels}: Functions $k(x, x') = \langle \phi(x), \phi(x') \rangle$ that compute inner products in high-dimensional spaces implicitly

    \item \textbf{Kernel trick}: Replace $x^\top x'$ with $k(x, x')$ to work in (potentially infinite) feature spaces tractably

    \item \textbf{Mercer's theorem}: Valid kernels are symmetric and positive semi-definite; they can be combined to build custom similarity measures

    \item \textbf{Kernel ridge regression}: Solves ridge regression in feature space via an $n \times n$ system; representer theorem guarantees finite representation

    \item \textbf{Support vector machines}: Maximum-margin classifiers that use only support vectors for prediction; kernel trick enables nonlinear boundaries

    \item \textbf{RKHS}: Each kernel defines a function space; the RKHS norm measures complexity and connects to regularisation

    \item \textbf{Common kernels}: Linear (baseline), polynomial (interactions), RBF (infinite-dimensional, local similarity)

    \item \textbf{Curse of dimensionality}: Distance loses meaning in high dimensions; kernel methods struggle when $d$ is large
\end{enumerate}
\end{bluebox}

\begin{bluebox}[Quick Reference: Key Formulas]
\textbf{Kernel definition}: $k(x, z) = \langle \phi(x), \phi(z) \rangle$

\textbf{Common kernels}:
\begin{align*}
\text{Linear}: && k(x,z) &= x^\top z \\
\text{Polynomial}: && k(x,z) &= (x^\top z + c)^M \\
\text{RBF}: && k(x,z) &= \exp\left(-\frac{\|x-z\|^2}{2\sigma^2}\right)
\end{align*}

\textbf{Kernel ridge regression}: $\hat{y}(\tilde{x}) = k_{\tilde{x}}^\top (K + \lambda I)^{-1} y$

\textbf{SVM prediction}: $\hat{y}(x) = \text{sign}\left(\sum_{i \in SV} \alpha_i y_i k(x_i, x) + b\right)$

\textbf{Representer theorem}: $w^* = \sum_{i=1}^n \alpha_i \phi(x_i)$
\end{bluebox}

\begin{redbox}
\textbf{When NOT to use kernel methods}:
\begin{itemize}
    \item \textbf{Large $n$}: $O(n^2)$ storage and $O(n^3)$ training become prohibitive
    \item \textbf{High-dimensional features}: Distance becomes meaningless; prefer linear methods or deep learning
    \item \textbf{Streaming/online settings}: Must store all training data
\end{itemize}

\textbf{Consider instead}: Random features, deep learning, or linear methods with careful feature engineering.
\end{redbox}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.4\linewidth]{figures/week_06_kernels/meme.png}
    \label{fig:kernel-meme}
\end{figure}
