% Week 10: Neural Networks I (Foundations)

\section{Overview}

\begin{bluebox}[Chapter Summary]
This chapter introduces the foundations of neural networks-from biological inspiration to practical training. We begin with the perceptron, prove its convergence for linearly separable data, and demonstrate its fundamental limitations. Multi-layer perceptrons (MLPs) overcome these limitations through learned feature representations. Key concepts include:
\begin{itemize}
    \item \textbf{Perceptrons}: Linear classifiers with provable convergence guarantees
    \item \textbf{Linear separability}: The boundary of what single-layer networks can learn
    \item \textbf{MLPs}: Composing simple functions to build universal approximators
    \item \textbf{Backpropagation}: Efficient gradient computation via the chain rule
    \item \textbf{Training}: Loss functions, optimisers, initialisation, and regularisation
\end{itemize}
\end{bluebox}

Neural networks represent a fundamental paradigm shift in machine learning. In the models we have studied previously-linear regression, logistic regression, kernel methods-we either work with features directly or apply hand-crafted transformations (polynomial features, radial basis functions). The key limitation is that these transformations are \textit{fixed}: we choose them based on domain knowledge before seeing any data, then learn only the weights that combine these features.

Neural networks take a radically different approach: they learn the feature transformations themselves. This means the model can discover which patterns in the data are relevant, rather than relying on our ability to anticipate them.

\begin{bluebox}[Neural Networks in Context]
Neural networks extend linear models by:
\begin{enumerate}
    \item \textbf{Learning feature representations} $\phi(x)$ rather than hand-crafting them
    \item \textbf{Composing simple functions} to build complex mappings
    \item \textbf{Using non-linear activations} to escape the limitations of linear models
\end{enumerate}
The transition from $f(x) = w^\top x$ to $f(x) = w^\top \phi(x; \theta)$ where $\phi$ is \emph{learned} represents a fundamental shift in machine learning.
\end{bluebox}

To understand what this means concretely, consider image classification. A linear model applied to raw pixels would need to learn that ``cat-ness'' is some weighted combination of individual pixel values. This is hopelessly naive-the same cat in a different position would have completely different pixel values. What we need are features like ``presence of pointed ears,'' ``whisker-like patterns,'' or ``feline body shape.'' Hand-crafting such features is impractical for complex tasks.

A neural network learns a hierarchy of features: early layers might detect edges, middle layers combine edges into textures and shapes, and later layers recognise high-level concepts like ``cat'' or ``dog.'' Crucially, \textit{all of these features emerge from training on data}-we do not specify them in advance.

%═══════════════════════════════════════════════════════════════════════════════
\section{Biological Motivation}
%═══════════════════════════════════════════════════════════════════════════════

\begin{bluebox}[Section Summary]
Artificial neural networks draw inspiration from biological neurons, though the analogy is loose. Understanding the biological motivation provides context for design choices, while recognising the differences prevents over-interpreting ``brain-like'' claims.
\end{bluebox}

The term ``neural network'' comes from early attempts to model computation in biological brains. While modern deep learning has diverged substantially from neuroscience, the historical connection explains much of the terminology and provides useful intuition.

\subsection{Neurons and Synapses}

The brain contains approximately $10^{11}$ neurons, each connected to thousands of others via synapses. A biological neuron operates through a cycle of signal reception, integration, and transmission:
\begin{enumerate}
    \item \textbf{Receives signals} from other neurons via dendrites
    \item \textbf{Integrates} these signals in the cell body (soma)
    \item \textbf{Fires} an action potential down the axon if the integrated signal exceeds a threshold
    \item \textbf{Transmits} the signal to downstream neurons via synaptic connections
\end{enumerate}

The strength of synaptic connections varies-some are excitatory (encouraging the downstream neuron to fire), others inhibitory (discouraging firing)-and these strengths change with experience. This \textbf{synaptic plasticity} is believed to underlie learning and memory: connections that contribute to successful outcomes are strengthened, while others weaken.

\subsection{The McCulloch-Pitts Neuron (1943)}

The first computational model of a neuron was proposed by McCulloch and Pitts in 1943 (see Week 1). Their model abstracted the biological neuron as a binary threshold unit, capturing the essential computation of ``weighted summation followed by thresholding.''

\begin{greybox}[McCulloch-Pitts Neuron]
A neuron $j$ with $n$ binary inputs $x_1, \ldots, x_n$, weights $w_1, \ldots, w_n$, and threshold $\theta$ computes:
\[
y_j = \begin{cases} 1 & \text{if } \sum_{i=1}^{n} w_i x_i \geq \theta \\ 0 & \text{otherwise} \end{cases}
\]

McCulloch and Pitts showed that networks of such neurons could compute any Boolean function, establishing a connection between neural activity and logic.
\end{greybox}

\textbf{What this model captures}: The neuron receives inputs $x_i$ (analogous to signals from other neurons), each weighted by $w_i$ (analogous to synaptic strength). It sums these contributions and compares to a threshold $\theta$. If the weighted sum is large enough, the neuron ``fires'' (outputs 1); otherwise it remains silent (outputs 0).

\textbf{What this model lacks}: The weights in McCulloch-Pitts neurons were \textit{fixed}, not learned. The model described what neurons could compute, but not how brains could learn to compute it.

\subsection{From Biology to Artificial Networks}

\begin{redbox}
Modern artificial neural networks differ substantially from biological neurons:
\begin{itemize}
    \item \textbf{Continuous activations}: Real neurons communicate via spike trains (sequences of discrete pulses); artificial neurons use continuous values
    \item \textbf{Synchronous updates}: Biological neural activity is asynchronous and parallel; artificial networks update in discrete layers
    \item \textbf{Backpropagation}: There is no known biological mechanism for propagating error gradients backwards through synapses
    \item \textbf{Architecture}: Biological connectivity is sparse and irregular; artificial networks often use dense, structured connections
\end{itemize}
The ``neural'' in neural networks is more historical inspiration than literal description.
\end{redbox}

Despite these differences, the biological analogy remains useful for intuition:
\begin{itemize}
    \item Weights $\approx$ synaptic strengths (learned from experience)
    \item Activation functions $\approx$ firing rate as a function of input current
    \item Layers $\approx$ hierarchical processing in sensory cortex
\end{itemize}

But modern deep learning succeeds through mathematics and engineering, not biological fidelity. The field has developed its own principles that work well empirically, even where they diverge from neuroscience.

%═══════════════════════════════════════════════════════════════════════════════
\section{The Perceptron}
%═══════════════════════════════════════════════════════════════════════════════

\begin{bluebox}[Section Summary]
The perceptron (Rosenblatt, 1957) was the first neural network that could \emph{learn} from data. It implements a linear classifier with a simple update rule that provably converges for linearly separable data. Understanding the perceptron-its power and limitations-provides the foundation for understanding deeper networks.
\end{bluebox}

The perceptron, introduced by Frank Rosenblatt in 1957, addressed the key limitation of McCulloch-Pitts neurons: it could \textit{learn} its weights from examples. This was a revolutionary idea-a machine that could automatically adjust its behaviour based on experience.

The perceptron is a linear classifier: a non-probabilistic counterpart to logistic regression. Where logistic regression outputs probabilities via the sigmoid function, the perceptron outputs hard decisions via a step function.

\subsection{Architecture}

\begin{greybox}[Perceptron Model]
For input $x \in \mathbb{R}^d$ and parameters $w \in \mathbb{R}^d$, $b \in \mathbb{R}$:

\textbf{Pre-activation (weighted sum):}
\[
z = w^\top x + b = \sum_{i=1}^{d} w_i x_i + b
\]

\textbf{Activation (threshold):}
\[
\hat{y} = \text{sign}(z) = \begin{cases} +1 & \text{if } z \geq 0 \\ -1 & \text{if } z < 0 \end{cases}
\]

\textbf{Compact notation}: By augmenting $x$ with a 1 (i.e., $\tilde{x} = [x; 1]$) and $w$ with $b$ (i.e., $\tilde{w} = [w; b]$), we can write:
\[
\hat{y} = \text{sign}(\tilde{w}^\top \tilde{x})
\]
\end{greybox}

Let us unpack this notation carefully:
\begin{itemize}
    \item \textbf{Input $x \in \mathbb{R}^d$}: A vector of $d$ features describing one example (e.g., pixel values, measurements, etc.)
    \item \textbf{Weight vector $w \in \mathbb{R}^d$}: One weight per feature, learned during training. Large positive $w_i$ means feature $i$ is strong evidence for class $+1$; large negative $w_i$ means it is evidence for class $-1$.
    \item \textbf{Bias $b \in \mathbb{R}$}: Shifts the decision boundary. Without $b$, the decision boundary would always pass through the origin.
    \item \textbf{Pre-activation $z$}: The ``score'' for the positive class. Positive $z$ predicts $+1$; negative $z$ predicts $-1$.
    \item \textbf{Sign function}: Converts the continuous score into a discrete prediction.
\end{itemize}

The compact notation $\tilde{w}^\top \tilde{x}$ absorbs the bias into the weights by treating it as the weight for a constant ``feature'' that always equals 1. This simplifies the mathematics without changing the model.

The perceptron computes whether the input lies on the positive or negative side of a hyperplane defined by $w^\top x + b = 0$.

\subsection{Geometric Interpretation}

The perceptron implements a geometric concept: it partitions space using a hyperplane.

\begin{greybox}[Hyperplane Classifier]
The weight vector $w$ defines a hyperplane in $\mathbb{R}^d$:
\[
H = \{x \in \mathbb{R}^d : w^\top x + b = 0\}
\]

\textbf{Properties:}
\begin{itemize}
    \item $w$ is the normal vector to the hyperplane (perpendicular to $H$)
    \item $b / \|w\|$ is the signed distance from the origin to $H$
    \item Points with $w^\top x + b > 0$ lie on the positive side; $w^\top x + b < 0$ on the negative side
    \item The signed distance from point $x$ to $H$ is $\frac{w^\top x + b}{\|w\|}$
\end{itemize}
\end{greybox}

\textbf{Unpacking the geometry}:

In two dimensions ($d = 2$), the hyperplane $H$ is a line. Points on one side of the line are classified as $+1$; points on the other side as $-1$. The weight vector $w$ points ``into'' the positive region-it is perpendicular to the line and points toward class $+1$.

In three dimensions ($d = 3$), $H$ is a plane. In higher dimensions, $H$ is a $(d-1)$-dimensional subspace called a hyperplane.

The quantity $w^\top x + b$ measures how ``deep'' point $x$ is into the positive region. The normalised version $(w^\top x + b) / \|w\|$ gives the actual geometric distance from $x$ to the decision boundary.

\begin{center}
\begin{verbatim}
                    w (normal vector)
                    ^
                    |
        Positive    |    Negative
        class (+1)  |    class (-1)
                    |
    ------+------ Hyperplane H
                    |
                    |
\end{verbatim}
\end{center}

The perceptron's task is to find a hyperplane that separates positive from negative examples.

\subsection{The Perceptron Learning Algorithm}

Rosenblatt's key contribution was not just the perceptron model, but a learning algorithm that could automatically find the weights.

\begin{greybox}[Perceptron Learning Algorithm]
\textbf{Input:} Training set $\{(x_1, y_1), \ldots, (x_n, y_n)\}$ with $x_i \in \mathbb{R}^d$, $y_i \in \{-1, +1\}$

\textbf{Initialisation:} $w \leftarrow 0$, $b \leftarrow 0$

\textbf{Repeat} until convergence:
\begin{enumerate}
    \item For each training example $(x_i, y_i)$:
    \item \quad Compute prediction: $\hat{y}_i = \text{sign}(w^\top x_i + b)$
    \item \quad If $\hat{y}_i \neq y_i$ (misclassification):
    \item \quad\quad $w \leftarrow w + \eta \, y_i \, x_i$
    \item \quad\quad $b \leftarrow b + \eta \, y_i$
\end{enumerate}

\textbf{Output:} Classifier $f(x) = \text{sign}(w^\top x + b)$

Here $\eta > 0$ is the learning rate (often set to 1 for the basic algorithm).
\end{greybox}

\textbf{Reading the algorithm}:
\begin{itemize}
    \item We initialise all weights to zero (the hyperplane is undefined, but the algorithm will quickly give it shape).
    \item We cycle through training examples. For each correctly classified example, we do nothing-no news is good news.
    \item When we misclassify an example $(x_i, y_i)$, we adjust the weights to make the correct answer more likely next time.
    \item The learning rate $\eta$ controls how large each adjustment is. With $\eta = 1$, we get the classical perceptron algorithm.
\end{itemize}

\textbf{Pseudocode:}
\begin{verbatim}
def perceptron_train(X, y, max_iterations=1000, eta=1.0):
    n, d = X.shape
    w = np.zeros(d)
    b = 0.0

    for _ in range(max_iterations):
        errors = 0
        for i in range(n):
            y_hat = np.sign(np.dot(w, X[i]) + b)
            if y_hat != y[i]:
                w = w + eta * y[i] * X[i]
                b = b + eta * y[i]
                errors += 1
        if errors == 0:
            break  # Converged

    return w, b
\end{verbatim}

\subsection{Understanding the Update Rule}

Why does the update $w \leftarrow w + \eta \, y_i \, x_i$ make sense? Let us trace through the logic carefully.

\begin{greybox}[Update Rule Intuition]
Consider a misclassified point $(x_i, y_i)$ where $y_i = +1$ but $w^\top x_i + b < 0$.

After the update $w' = w + \eta x_i$:
\[
(w')^\top x_i = w^\top x_i + \eta \|x_i\|^2 > w^\top x_i
\]

The new weight vector increases the score for $x_i$, pushing it toward correct classification.

Similarly, if $y_i = -1$ but $w^\top x_i + b > 0$, the update $w' = w - \eta x_i$ decreases the score for $x_i$.

\textbf{Geometric view:} The update rotates the hyperplane toward correctly classifying the misclassified point.
\end{greybox}

\textbf{Detailed explanation}: When we misclassify a positive example ($y_i = +1$), the current weights give it a negative score ($w^\top x_i < 0$). We want to increase this score. Adding $\eta x_i$ to $w$ does exactly that: the new score is $(w + \eta x_i)^\top x_i = w^\top x_i + \eta \|x_i\|^2$. Since $\|x_i\|^2 > 0$, the score increases.

Similarly, when we misclassify a negative example ($y_i = -1$), we subtract $\eta x_i$ from $w$, which decreases the score.

The factor $y_i$ in the update rule $w \leftarrow w + \eta y_i x_i$ elegantly handles both cases: it is $+1$ for positive examples (add $x_i$) and $-1$ for negative examples (subtract $x_i$).

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{figures/week_10_neural_nets/peceptron.png}
    \caption{Perceptron learning: the decision boundary (dashed line) adjusts iteratively as misclassifications are corrected.}
    \label{fig:perceptron}
\end{figure}

\textbf{Connection to gradient descent}: The perceptron update resembles gradient descent on logistic regression's negative log-likelihood, but operates on single observations rather than batches and uses hard thresholds rather than probabilities. In fact, the perceptron update can be derived as a subgradient of the \textit{hinge loss} (used in SVMs), though Rosenblatt discovered it through different reasoning.

\subsection{The Perceptron Convergence Theorem}

The perceptron algorithm is guaranteed to converge if the data is linearly separable. This was one of the first theoretical results about learning algorithms.

\begin{greybox}[Perceptron Convergence Theorem]
\textbf{Assumptions:}
\begin{enumerate}
    \item The data is linearly separable: there exists $w^* \in \mathbb{R}^d$ with $\|w^*\| = 1$ and margin $\gamma > 0$ such that $y_i (w^{*\top} x_i) \geq \gamma$ for all $i$
    \item All data points satisfy $\|x_i\| \leq R$
\end{enumerate}

\textbf{Conclusion:} The perceptron algorithm makes at most $\left(\frac{R}{\gamma}\right)^2$ updates before finding a separating hyperplane.
\end{greybox}

\textbf{Unpacking the assumptions}:
\begin{itemize}
    \item \textbf{$w^*$ with $\|w^*\| = 1$}: A unit-length weight vector that correctly classifies all points. We normalise to unit length to make the margin well-defined.
    \item \textbf{Margin $\gamma$}: The minimum ``depth'' of any point into its correct region. All positive points have $w^{*\top} x_i \geq \gamma$; all negative points have $w^{*\top} x_i \leq -\gamma$. This measures how ``easy'' the separation is.
    \item \textbf{Radius $R$}: The maximum distance of any point from the origin. This bounds how ``spread out'' the data is.
\end{itemize}

The bound $(R/\gamma)^2$ tells us that convergence is fast when the margin is large relative to the data spread.

\begin{greybox}[Proof Sketch]
Let $w^*$ be the optimal separator with $\|w^*\| = 1$ and margin $\gamma$. We track two quantities through the updates.

\textbf{Progress toward $w^*$:} After $k$ updates (on examples $(x_{i_1}, y_{i_1}), \ldots, (x_{i_k}, y_{i_k})$), with $\eta = 1$:
\[
w^{(k)} = \sum_{j=1}^{k} y_{i_j} x_{i_j}
\]
\[
w^{(k)} \cdot w^* = \sum_{j=1}^{k} y_{i_j} (w^{*\top} x_{i_j}) \geq k\gamma
\]
Each update increases $w^{(k)} \cdot w^*$ by at least $\gamma$.

\textbf{Bounded growth:} Since updates only occur on misclassified points (where $y_i (w^\top x_i) \leq 0$):
\[
\|w^{(k)}\|^2 = \|w^{(k-1)}\|^2 + 2y_{i_k}(w^{(k-1)\top} x_{i_k}) + \|x_{i_k}\|^2 \leq \|w^{(k-1)}\|^2 + R^2
\]
By induction: $\|w^{(k)}\|^2 \leq kR^2$.

\textbf{Combining:} By Cauchy-Schwarz:
\[
k\gamma \leq w^{(k)} \cdot w^* \leq \|w^{(k)}\| \cdot \|w^*\| = \|w^{(k)}\| \leq \sqrt{k} R
\]
Therefore $k \leq (R/\gamma)^2$. \hfill $\square$
\end{greybox}

\textbf{Walking through the proof}:

The proof bounds $k$ (the number of updates) by tracking two quantities:

1. \textbf{Alignment with $w^*$}: Each update on a misclassified point adds $y_{i_j} x_{i_j}$ to our weight vector. When we dot this with the optimal $w^*$, we get $y_{i_j} (w^{*\top} x_{i_j})$, which by assumption is at least $\gamma$. So after $k$ updates, $w^{(k)} \cdot w^* \geq k\gamma$.

2. \textbf{Magnitude of $w^{(k)}$}: We cannot have $w^{(k)}$ growing too fast, because updates only happen on misclassified points. The key insight is that $y_{i_k}(w^{(k-1)\top} x_{i_k}) \leq 0$ for misclassified points, so the cross-term in $\|w^{(k)}\|^2$ is non-positive. This gives $\|w^{(k)}\|^2 \leq kR^2$.

3. \textbf{The contradiction}: Cauchy-Schwarz says $w^{(k)} \cdot w^* \leq \|w^{(k)}\|$. Combining with step 1: $k\gamma \leq \|w^{(k)}\| \leq \sqrt{k}R$. Squaring: $k^2 \gamma^2 \leq kR^2$, so $k \leq (R/\gamma)^2$.

\begin{bluebox}[Convergence Bound Interpretation]
The bound $k \leq (R/\gamma)^2$ reveals:
\begin{itemize}
    \item \textbf{Larger margin $\gamma$}: Faster convergence-well-separated data is easier to learn
    \item \textbf{Larger radius $R$}: Slower convergence-spread-out data requires more adjustments
    \item \textbf{Dimension-independent}: The bound depends only on $R/\gamma$, not on $d$
\end{itemize}
This is an early example of a margin-based generalisation bound-ideas that later became central to SVMs and statistical learning theory.
\end{bluebox}

%═══════════════════════════════════════════════════════════════════════════════
\section{Limitations of the Perceptron}
%═══════════════════════════════════════════════════════════════════════════════

\begin{bluebox}[Section Summary]
The perceptron can only learn linearly separable functions. The XOR problem demonstrates this limitation geometrically. More generally, single-layer networks cannot represent functions requiring nonlinear decision boundaries. These limitations, highlighted by Minsky \& Papert (1969), shaped the field's subsequent development.
\end{bluebox}

The perceptron convergence theorem has a critical assumption: linear separability. When this assumption fails, the perceptron algorithm has no guarantee of success-in fact, it will cycle indefinitely without converging.

\subsection{Linear Separability Requirement}

\begin{greybox}[Linear Separability]
A dataset $\{(x_i, y_i)\}$ with $y_i \in \{-1, +1\}$ is \textbf{linearly separable} if there exists a hyperplane $w^\top x + b = 0$ such that:
\[
y_i (w^\top x_i + b) > 0 \quad \text{for all } i
\]

Equivalently: all positive examples lie strictly on one side of the hyperplane, and all negative examples on the other.
\end{greybox}

This is a strong assumption. In two dimensions, it means the classes can be separated by a straight line. In three dimensions, by a plane. Many real-world problems are not linearly separable in their original feature space.

The perceptron convergence theorem guarantees finding a separator \emph{only if one exists}. If the data is not linearly separable, the algorithm will cycle indefinitely without converging-it will keep finding misclassified points and updating, but never reach a state where all points are correctly classified.

\subsection{The XOR Problem}

The XOR (exclusive or) function is the canonical example of a non-linearly-separable problem.

\begin{greybox}[XOR Function]
\begin{center}
\begin{tabular}{cc|c}
$x_1$ & $x_2$ & $x_1 \oplus x_2$ \\
\hline
0 & 0 & 0 (negative) \\
0 & 1 & 1 (positive) \\
1 & 0 & 1 (positive) \\
1 & 1 & 0 (negative) \\
\end{tabular}
\end{center}

\textbf{Geometric view:} In $\mathbb{R}^2$, the positive examples $(0,1)$ and $(1,0)$ occupy opposite corners of the unit square from the negative examples $(0,0)$ and $(1,1)$.
\end{greybox}

XOR is ``exclusive or'': it returns 1 when exactly one input is 1, and 0 when both inputs are the same.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{figures/week_10_neural_nets/XOR.png}
    \caption{XOR is not linearly separable-no single line can separate the classes.}
    \label{fig:xor}
\end{figure}

\textbf{Why no hyperplane works}: Consider trying to draw a line separating the positive examples from the negative ones:
\begin{itemize}
    \item Any line separating $(0,0)$ from $(0,1)$ must pass between them (vertically or with positive slope on the left).
    \item Any line separating $(1,1)$ from $(1,0)$ must pass between them (vertically or with positive slope on the right).
    \item But a single straight line cannot simultaneously separate $(0,0)$ from $(0,1)$ \textit{and} $(1,1)$ from $(1,0)$ while putting $(0,1)$ and $(1,0)$ on the same side.
\end{itemize}

Another way to see this: the positive examples are ``diagonal'' from each other, as are the negative examples. No single line can separate two diagonally opposite corners from the other two.

\begin{greybox}[What the Perceptron Cannot Compute]
A single perceptron can compute:
\begin{itemize}
    \item AND: $x_1 \land x_2$ (threshold at 1.5 with weights $w_1 = w_2 = 1$)
    \item OR: $x_1 \lor x_2$ (threshold at 0.5 with weights $w_1 = w_2 = 1$)
    \item NOT: $\neg x_1$ (threshold at 0.5 with weight $w_1 = -1$)
    \item Any linearly separable Boolean function
\end{itemize}

A single perceptron \textbf{cannot} compute:
\begin{itemize}
    \item XOR: $x_1 \oplus x_2$
    \item XNOR: $\neg(x_1 \oplus x_2)$
    \item Any non-linearly-separable function
\end{itemize}
\end{greybox}

\textbf{Verifying AND is linearly separable}: With $w_1 = w_2 = 1$ and threshold (bias) $b = -1.5$, the perceptron outputs 1 when $x_1 + x_2 - 1.5 \geq 0$, i.e., when $x_1 + x_2 \geq 1.5$. This is only true when both $x_1 = 1$ and $x_2 = 1$.

\textbf{Why XOR is different}: XOR would require outputting 1 when $x_1 + x_2 = 1$ exactly, but 0 when $x_1 + x_2 = 0$ or $x_1 + x_2 = 2$. No single threshold on a weighted sum can achieve this.

\subsection{No Margin Maximisation}

Even when data is linearly separable, infinitely many hyperplanes correctly classify all points.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{figures/week_10_neural_nets/infinite hyperplane.png}
    \caption{Multiple valid decision boundaries exist for linearly separable data.}
    \label{fig:infinite-hyperplane}
\end{figure}

The perceptron has no mechanism to prefer one over another-it simply finds \emph{some} separator, which depends on the order of training examples and initialisation. This contrasts with Support Vector Machines, which explicitly maximise the margin (see Week 6 on kernels).

From a generalisation perspective, some hyperplanes are better than others. A hyperplane that barely separates the classes (small margin) is more likely to misclassify new points than one with a large margin. The perceptron provides no such guarantee.

\begin{redbox}
These limitations, publicised by Minsky \& Papert (1969) in their book \emph{Perceptrons}, triggered the first ``AI winter.'' Their mathematical analysis showed that single-layer networks could not learn many important functions. Interest revived in the 1980s with multi-layer networks and backpropagation-architectures that \emph{were} known in 1969, but for which no efficient training algorithm was widely available.
\end{redbox}

\subsection{Solutions to the XOR Problem}

The XOR problem admits several solutions, each historically significant:

\begin{greybox}[Approaches to XOR]
\textbf{1. Feature engineering:} Map inputs to a space where they become linearly separable. Adding $x_1 x_2$ as a feature:
\begin{center}
\begin{tabular}{ccc|c}
$x_1$ & $x_2$ & $x_1 x_2$ & $y$ \\
\hline
0 & 0 & 0 & 0 \\
0 & 1 & 0 & 1 \\
1 & 0 & 0 & 1 \\
1 & 1 & 1 & 0 \\
\end{tabular}
\end{center}
Now the hyperplane $x_1 + x_2 - 2x_1 x_2 = 0.5$ separates the classes.

\textbf{2. Multi-layer networks:} Add a hidden layer that learns intermediate representations. XOR $= (x_1 \land \neg x_2) \lor (\neg x_1 \land x_2)$ decomposes into linearly separable sub-problems.

\textbf{3. Kernel methods:} Implicitly map to high-dimensional feature spaces (see Week 6).
\end{greybox}

\textbf{Why feature engineering works}: In the augmented space $(x_1, x_2, x_1 x_2)$:
\begin{itemize}
    \item $(0, 0) \mapsto (0, 0, 0)$ - class 0
    \item $(0, 1) \mapsto (0, 1, 0)$ - class 1
    \item $(1, 0) \mapsto (1, 0, 0)$ - class 1
    \item $(1, 1) \mapsto (1, 1, 1)$ - class 0
\end{itemize}
The class 1 points have $x_1 x_2 = 0$ and $x_1 + x_2 = 1$; the class 0 points have either $x_1 + x_2 = 0$ or $x_1 x_2 = 1$. A linear separator exists in this 3D space.

Multi-layer networks automate the feature engineering-the hidden layers learn representations that make the problem linearly separable in the final layer. This is the key insight that revived neural networks.

%═══════════════════════════════════════════════════════════════════════════════
\section{Multi-Layer Perceptrons (MLPs)}
%═══════════════════════════════════════════════════════════════════════════════

\begin{bluebox}[Section Summary]
Multi-layer perceptrons overcome single-layer limitations by stacking layers of neurons with nonlinear activations. Each layer transforms its input, learning increasingly abstract representations. With sufficient width, a single hidden layer can approximate any continuous function (universal approximation), though deeper networks often learn more efficiently.
\end{bluebox}

The solution to the perceptron's limitations is deceptively simple: add more layers. A multi-layer perceptron (MLP) consists of an input layer, one or more hidden layers, and an output layer. Each hidden layer transforms its input through a linear operation followed by a nonlinear activation.

\subsection{From Fixed to Learned Features}

\begin{greybox}[From Fixed to Learned Features]
\textbf{Traditional approach (linear models):}
\[
f(x; w) = w^\top \phi(x)
\]
where $\phi(x)$ is a fixed, hand-crafted feature transformation.

\textbf{Neural network approach:}
\[
f(x; W_2, W_1) = W_2^\top \phi(W_1^\top x)
\]
where $\phi$ is a nonlinear activation applied element-wise, and both $W_1$ and $W_2$ are \textbf{learned}.

\textbf{Deep networks} compose multiple such transformations:
\[
f(x; \theta) = f_L \circ f_{L-1} \circ \cdots \circ f_1(x)
\]
where $f_l(z) = \sigma(W_l z + b_l)$ applies an affine transformation followed by nonlinearity $\sigma$.
\end{greybox}

\textbf{What this means}: Instead of choosing features like ``degree-2 polynomial'' or ``RBF with bandwidth $\sigma$,'' we let the network discover useful features. The first layer might learn to detect certain patterns; the second layer combines these patterns into more complex features; and so on.

The key insight: instead of designing features by hand, let the network learn them from data.

\subsection{Architecture and Notation}

\begin{greybox}[MLP Notation]
Consider an MLP with $L$ layers:

\textbf{Layer $l$ parameters:}
\begin{itemize}
    \item Weight matrix: $W^{(l)} \in \mathbb{R}^{n_l \times n_{l-1}}$
    \item Bias vector: $b^{(l)} \in \mathbb{R}^{n_l}$
    \item $n_l$ = number of neurons in layer $l$
\end{itemize}

\textbf{Forward computation:}
\begin{align*}
z^{(l)} &= W^{(l)} a^{(l-1)} + b^{(l)} \quad \text{(pre-activation)} \\
a^{(l)} &= \sigma(z^{(l)}) \quad \text{(activation)}
\end{align*}
where $a^{(0)} = x$ (input) and $\sigma$ is the activation function.

\textbf{Network output:} $\hat{y} = a^{(L)}$ (possibly with a different final activation, e.g., softmax for classification).
\end{greybox}

\textbf{Reading the notation}:
\begin{itemize}
    \item $a^{(l)}$ is the activation (output) of layer $l$. The superscript $(l)$ indicates the layer number.
    \item $z^{(l)}$ is the pre-activation: the result after the linear transformation but before the nonlinearity.
    \item $W^{(l)}$ maps from $n_{l-1}$ inputs to $n_l$ outputs. Row $i$ of $W^{(l)}$ contains the weights for neuron $i$ in layer $l$.
    \item $b^{(l)}$ contains one bias term per neuron in layer $l$.
\end{itemize}

\textbf{Example: Two-layer network for XOR}

\begin{center}
\begin{verbatim}
     x1 --\
             [h1]-\
     x2 --/        \
                      [y]
     x1 --\        /
             [h2]-/
     x2 --/

Layer 0 (input):  a^(0) = [x1, x2]
Layer 1 (hidden): z^(1) = W^(1) a^(0) + b^(1),  a^(1) = sigma(z^(1))
Layer 2 (output): z^(2) = W^(2) a^(1) + b^(2),  y = sigma(z^(2))
\end{verbatim}
\end{center}

Here, the hidden layer has 2 neurons ($n_1 = 2$), the input has 2 features ($n_0 = 2$), and the output has 1 neuron ($n_2 = 1$). The weight matrix $W^{(1)}$ is $2 \times 2$; $W^{(2)}$ is $1 \times 2$.

\subsection{Why Non-Linearity is Essential}

\begin{redbox}
Without non-linear activations, composing linear layers produces a linear function:
\[
f(x) = W_L W_{L-1} \cdots W_1 x = Mx
\]
where $M = W_L W_{L-1} \cdots W_1$ is just another matrix.

Depth provides \textbf{no additional representational power} without non-linearity.
\end{redbox}

\textbf{Why this matters}: If $W_1$ is $n_1 \times d$, $W_2$ is $n_2 \times n_1$, etc., the product $M$ is $n_L \times d$-equivalent to a single linear layer. All the intermediate structure collapses.

Consider an extreme example: stacking 100 linear layers with 1000 neurons each. Without nonlinearities, this is equivalent to a single $n_L \times d$ matrix. We have 100 times the parameters but no more representational power.

The nonlinearity $\sigma$ breaks this collapse, enabling the network to represent nonlinear functions. Each layer can now contribute new nonlinear structure, and depth becomes meaningful.

\subsection{What MLPs Learn: A Geometric View}

Each layer of an MLP performs a geometric transformation:
\begin{enumerate}
    \item \textbf{Affine transformation} ($z = Wx + b$): Rotates, scales, shears, and translates
    \item \textbf{Nonlinearity} ($a = \sigma(z)$): ``Folds'' or ``bends'' the space, creating nonlinear boundaries
\end{enumerate}

For the XOR problem, the hidden layer learns to ``unfold'' the input space so that the two classes become linearly separable:

\begin{center}
\begin{verbatim}
Input space:                Hidden space:

(0,1) +        + (1,0)     After hidden layer transformation,
                           the + and - points become
      (0,0)   (1,1)        linearly separable
        -       -
\end{verbatim}
\end{center}

\textbf{Intuition}: The hidden layer implements a nonlinear ``warping'' of space. Points that were not separable by a line in the original space become separable after the transformation. The output layer then simply draws a line in this transformed space.

\subsection{Universal Approximation Theorem}

\begin{greybox}[Universal Approximation Theorem (Informal)]
A feedforward network with a single hidden layer containing a finite number of neurons can approximate any continuous function on compact subsets of $\mathbb{R}^n$, to arbitrary precision, given:
\begin{itemize}
    \item A suitable non-linear activation function (e.g., sigmoid, ReLU)
    \item Sufficiently many hidden neurons
\end{itemize}
\end{greybox}

\textbf{What this says}: Given any continuous function $g: \mathbb{R}^d \to \mathbb{R}$ and any desired accuracy $\epsilon > 0$, there exists an MLP with one hidden layer such that $|f(x) - g(x)| < \epsilon$ for all $x$ in a bounded region.

\textbf{What this does not say}: It does not tell us how many hidden neurons we need (could be astronomically large), how to find the right weights (the optimisation problem could be hard), or whether the resulting network will generalise well.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/week_10_neural_nets/univerasal expression.png}
    \caption{ReLU networks learn piecewise linear functions, partitioning input space into regions.}
    \label{fig:universal}
\end{figure}

\textbf{Intuition for ReLU networks:} Each ReLU unit contributes a ``hinge''-a point where the function changes slope. With enough hinges, any continuous function can be approximated by a piecewise linear function. More ReLU units mean finer approximation.

\begin{bluebox}[Universal Approximation: Theory vs Practice]
The theorem says: ``one hidden layer suffices in principle.''

\textbf{But in practice:}
\begin{itemize}
    \item The required width may be exponentially large
    \item Deep networks often achieve the same approximation with fewer parameters
    \item Depth enables hierarchical representations that match data structure
    \item Optimisation is often easier with depth (when done right)
\end{itemize}
Universal approximation is an \emph{existence} result, not a prescription for architecture.
\end{bluebox}

\subsection{Why Depth Matters}

\begin{greybox}[Representational Efficiency of Depth]
Some functions can be represented exponentially more efficiently with deep networks:

\textbf{Example:} The function $f(x) = x_1 \cdot x_2 \cdot x_3 \cdots x_n$ (product of $n$ inputs)
\begin{itemize}
    \item \textbf{Shallow network:} Requires $\Omega(2^n)$ neurons to approximate
    \item \textbf{Deep network:} $O(n)$ neurons arranged in $O(\log n)$ layers suffice
\end{itemize}

The intuition: depth enables \textbf{reuse of intermediate computations}. Computing $((x_1 \cdot x_2) \cdot (x_3 \cdot x_4)) \cdot \ldots$ shares subcomputations; a shallow network must enumerate all combinations.
\end{greybox}

\textbf{A concrete example}: To compute $x_1 \cdot x_2 \cdot x_3 \cdot x_4$:
\begin{itemize}
    \item \textbf{Deep approach}: First compute $a = x_1 \cdot x_2$ and $b = x_3 \cdot x_4$, then compute $a \cdot b$. Total: 3 multiplications, depth 2.
    \item \textbf{Shallow approach}: Would need to somehow encode all $2^4$ possible combinations of inputs and their contributions to the output.
\end{itemize}

More practically, deep networks learn \textbf{hierarchical representations}:
\begin{itemize}
    \item Early layers: Low-level features (edges, textures)
    \item Middle layers: Parts and patterns
    \item Deep layers: Objects and concepts
\end{itemize}

This hierarchy matches the compositional structure of real-world data. Images are made of objects, which are made of parts, which are made of edges. Natural language has similar hierarchical structure.

%═══════════════════════════════════════════════════════════════════════════════
\section{Activation Functions}
%═══════════════════════════════════════════════════════════════════════════════

\begin{bluebox}[Section Summary]
Activation functions introduce nonlinearity between layers. The choice of activation affects gradient flow, training dynamics, and what functions the network can represent. Modern practice favours ReLU and its variants for hidden layers, with task-specific activations (sigmoid, softmax) for outputs.
\end{bluebox}

Activation functions are the nonlinear ``glue'' that makes neural networks powerful. Without them, as we have seen, depth provides no benefit. The choice of activation function has profound effects on training dynamics.

\begin{greybox}[Common Activation Functions]
\textbf{Step function} (original perceptron):
\[
f(x) = \mathbb{I}(x \geq 0) = \begin{cases} 1 & x \geq 0 \\ 0 & x < 0 \end{cases}
\]
Binary output; no gradient for optimisation (derivative is 0 almost everywhere).

\textbf{Sigmoid (logistic):}
\[
\sigma(x) = \frac{1}{1 + e^{-x}}, \quad \sigma'(x) = \sigma(x)(1 - \sigma(x))
\]
Squashes output to $(0, 1)$; interpretable as probability. \textbf{Problem:} Saturates for large $|x|$, causing vanishing gradients.

\textbf{Tanh (hyperbolic tangent):}
\[
\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}, \quad \tanh'(x) = 1 - \tanh^2(x)
\]
Squashes output to $(-1, 1)$; zero-centred (unlike sigmoid). Still saturates.

\textbf{ReLU} (Rectified Linear Unit):
\[
\text{ReLU}(x) = \max(0, x), \quad \text{ReLU}'(x) = \mathbb{I}(x > 0)
\]
Simple, efficient, non-saturating for positive inputs. \textbf{Problem:} ``Dead neurons'' when $x < 0$ forever.

\textbf{Leaky ReLU:}
\[
\text{LeakyReLU}(x) = \max(\alpha x, x), \quad \text{where } \alpha \approx 0.01
\]
Non-zero gradient for negative inputs prevents dead neurons.

\textbf{ELU} (Exponential Linear Unit):
\[
\text{ELU}(x) = \begin{cases} x & x > 0 \\ \alpha(e^x - 1) & x \leq 0 \end{cases}
\]
Smooth at zero; negative values help push mean activations toward zero.

\textbf{GELU} (Gaussian Error Linear Unit):
\[
\text{GELU}(x) = x \cdot \Phi(x) \approx x \cdot \sigma(1.702 x)
\]
where $\Phi$ is the standard Gaussian CDF. Smooth, non-monotonic; used in Transformers.

\textbf{Swish:}
\[
\text{Swish}(x) = x \cdot \sigma(x)
\]
Smooth, non-monotonic; sometimes outperforms ReLU in deep networks.
\end{greybox}

\textbf{Understanding each activation}:

\begin{itemize}
    \item \textbf{Sigmoid}: Historically important; outputs are bounded in $(0,1)$, which is useful for probabilities. But it saturates: when $|x|$ is large, $\sigma'(x) \approx 0$, so gradients vanish.

    \item \textbf{Tanh}: Like sigmoid but centred at zero. Outputs in $(-1, 1)$. Same saturation problem.

    \item \textbf{ReLU}: The breakthrough activation for deep learning. For $x > 0$, ReLU is simply the identity, so gradients flow unchanged. For $x \leq 0$, ReLU outputs 0, which can cause ``dead neurons'' that never recover.

    \item \textbf{Leaky ReLU}: Fixes ReLU's dead neuron problem by allowing a small gradient ($\alpha$, typically 0.01) for negative inputs.

    \item \textbf{GELU/Swish}: Modern smooth activations that blend the benefits of ReLU (non-saturation for positive inputs) with smoothness at zero. Widely used in Transformers.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{figures/week_10_neural_nets/activation functions1.png}
    \caption{Comparison of common activation functions.}
    \label{fig:activations1}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{figures/week_10_neural_nets/activation function 2.png}
    \caption{Activation function derivatives-crucial for gradient flow.}
    \label{fig:activations2}
\end{figure}

\subsection{The Saturation Problem}

\begin{greybox}[Sigmoid Saturation]
For sigmoid $\sigma(x)$:
\begin{itemize}
    \item When $x \gg 0$: $\sigma(x) \approx 1$, $\sigma'(x) \approx 0$
    \item When $x \ll 0$: $\sigma(x) \approx 0$, $\sigma'(x) \approx 0$
\end{itemize}

In the \textbf{saturated regime}, gradients vanish:
\[
\frac{\partial \mathcal{L}}{\partial z} = \frac{\partial \mathcal{L}}{\partial a} \cdot \sigma'(z) \approx 0
\]

Learning stops because the gradient signal disappears.
\end{greybox}

\textbf{Why saturation is problematic}: During backpropagation, gradients multiply through layers. If $\sigma'(z) \approx 0$ at some layer, the gradient signal to earlier layers is near zero. Those layers cannot learn.

The maximum derivative of sigmoid is $\sigma'(0) = 0.25$. Even in the best case, each sigmoid layer reduces gradient magnitude by at least 75\%. After several layers, gradients become negligibly small.

ReLU avoids saturation for positive inputs: $\text{ReLU}'(x) = 1$ for all $x > 0$. This enables training of much deeper networks.

\begin{redbox}
\textbf{ReLU's dead neuron problem:} If a neuron's input is always negative, its gradient is always zero, and it can never recover. This typically happens when:
\begin{itemize}
    \item Learning rate is too high, pushing weights to extreme values
    \item Poor initialisation places neurons in the dead zone
\end{itemize}
Leaky ReLU, ELU, and similar variants address this by maintaining a small gradient for negative inputs.
\end{redbox}

\subsection{Output Layer Activations}

The output layer activation depends on the task:

\begin{greybox}[Output Activations by Task]
\textbf{Regression}: No activation (or linear/identity). Output can be any real number.

\textbf{Binary classification}: Sigmoid. Output in $(0,1)$ interpreted as $P(y=1|x)$.

\textbf{Multi-class classification}: Softmax:
\[
\text{softmax}(z)_k = \frac{e^{z_k}}{\sum_{j=1}^{K} e^{z_j}}
\]
Outputs $K$ positive values summing to 1, interpretable as class probabilities.
\end{greybox}

\begin{bluebox}[Activation Function Guidelines]
\begin{itemize}
    \item \textbf{Hidden layers:} ReLU is the default; Leaky ReLU or GELU if dead neurons are a concern
    \item \textbf{Output layer (regression):} Linear (no activation)
    \item \textbf{Output layer (binary classification):} Sigmoid
    \item \textbf{Output layer (multi-class):} Softmax
    \item \textbf{Avoid sigmoid/tanh in hidden layers} of deep networks (saturation)
\end{itemize}
\end{bluebox}

%═══════════════════════════════════════════════════════════════════════════════
\section{Loss Functions}
%═══════════════════════════════════════════════════════════════════════════════

\begin{bluebox}[Section Summary]
The loss function defines what the network should learn by measuring the discrepancy between predictions and targets. Mean squared error is standard for regression; cross-entropy for classification. The choice of loss affects gradients and training dynamics.
\end{bluebox}

The loss function $\mathcal{L}(\hat{y}, y)$ measures how bad our predictions are. Training minimises this loss. The choice of loss function encodes our assumptions about the problem and affects how the network learns.

\subsection{Mean Squared Error (Regression)}

\begin{greybox}[Mean Squared Error]
For regression with predictions $\hat{y}_i$ and targets $y_i$:
\[
\mathcal{L}_{\text{MSE}} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
\]

\textbf{Gradient:}
\[
\frac{\partial \mathcal{L}}{\partial \hat{y}_i} = \frac{2}{n}(\hat{y}_i - y_i)
\]

\textbf{Interpretation:} MSE corresponds to maximum likelihood under Gaussian noise: if $y = f(x) + \epsilon$ with $\epsilon \sim \mathcal{N}(0, \sigma^2)$, minimising MSE is equivalent to maximising the likelihood.
\end{greybox}

\textbf{Why MSE for regression?}

1. \textbf{Probabilistic interpretation}: If we assume the true relationship is $y = f(x) + \epsilon$ where $\epsilon$ is Gaussian noise, the log-likelihood of observing $y$ given prediction $f(x)$ is:
\[
\log p(y | f(x)) = -\frac{(y - f(x))^2}{2\sigma^2} + \text{const}
\]
Maximising this is equivalent to minimising $(y - f(x))^2$.

2. \textbf{Smooth gradients}: The gradient is proportional to the error. Large errors give large gradients, driving fast correction. Small errors give small gradients, enabling fine-tuning.

\subsection{Cross-Entropy (Classification)}

\begin{greybox}[Binary Cross-Entropy]
For binary classification with predicted probability $\hat{p}_i = \sigma(z_i)$ and label $y_i \in \{0, 1\}$:
\[
\mathcal{L}_{\text{BCE}} = -\frac{1}{n} \sum_{i=1}^{n} \left[ y_i \log(\hat{p}_i) + (1 - y_i) \log(1 - \hat{p}_i) \right]
\]

\textbf{Gradient with respect to logit $z_i$:}
\[
\frac{\partial \mathcal{L}}{\partial z_i} = \frac{1}{n}(\hat{p}_i - y_i)
\]

This elegant form results from combining the sigmoid and log in the loss. Gradients do not vanish even when the sigmoid saturates, because the log ``unsquashes'' the output.
\end{greybox}

\textbf{Unpacking binary cross-entropy}:
\begin{itemize}
    \item When $y_i = 1$: Loss is $-\log(\hat{p}_i)$. This is small when $\hat{p}_i \approx 1$ (correct confident prediction) and large when $\hat{p}_i \approx 0$ (wrong confident prediction).
    \item When $y_i = 0$: Loss is $-\log(1 - \hat{p}_i)$. This is small when $\hat{p}_i \approx 0$ and large when $\hat{p}_i \approx 1$.
    \item The loss penalises confident wrong predictions heavily (the $-\log$ goes to infinity as the probability goes to zero).
\end{itemize}

\textbf{Why the gradient is so clean}: The derivative $\frac{\partial \mathcal{L}}{\partial z_i} = \hat{p}_i - y_i$ is remarkably simple. For a positive example ($y_i = 1$), we push $z_i$ up if $\hat{p}_i < 1$. For a negative example ($y_i = 0$), we push $z_i$ down if $\hat{p}_i > 0$. The magnitude of the push is proportional to the error.

\begin{greybox}[Multi-class Cross-Entropy]
For $K$ classes with predicted probabilities $\hat{p}_{i,k} = \text{softmax}(z_i)_k$ and one-hot label $y_{i,k}$:
\[
\mathcal{L}_{\text{CE}} = -\frac{1}{n} \sum_{i=1}^{n} \sum_{k=1}^{K} y_{i,k} \log(\hat{p}_{i,k})
\]

Since $y$ is one-hot (only one class is 1, rest are 0), this simplifies to:
\[
\mathcal{L}_{\text{CE}} = -\frac{1}{n} \sum_{i=1}^{n} \log(\hat{p}_{i,y_i})
\]

\textbf{Interpretation:} Cross-entropy measures the ``surprise'' of the true labels under the predicted distribution. Minimising cross-entropy is equivalent to maximising the likelihood of the data under a categorical distribution.
\end{greybox}

\textbf{Reading the formula}: For each example $i$, we look at the probability assigned to the \textit{correct} class $y_i$, take its log, and negate. If we are confident and correct ($\hat{p}_{i,y_i} \approx 1$), loss is near 0. If we are confident and wrong ($\hat{p}_{i,y_i} \approx 0$), loss is very large.

\begin{bluebox}[{Why Cross-Entropy, Not MSE, for Classification?}]
\begin{itemize}
    \item \textbf{Gradient magnitude:} With MSE and sigmoid output, gradients vanish when predictions are confident but wrong. Cross-entropy maintains strong gradients.
    \item \textbf{Probabilistic interpretation:} Cross-entropy corresponds to maximum likelihood for categorical outcomes.
    \item \textbf{Numerical stability:} Log-softmax can be computed stably; MSE with softmax is less stable.
\end{itemize}
\end{bluebox}

\textbf{The gradient problem with MSE for classification}: With MSE loss and sigmoid output, the gradient is:
\[
\frac{\partial \mathcal{L}}{\partial z} = (\hat{p} - y) \cdot \sigma'(z)
\]
When $z$ is large and wrong (e.g., $z = 10$ but $y = 0$), $\sigma(z) \approx 1$ and $\sigma'(z) \approx 0$. The gradient is tiny even though the prediction is badly wrong. Cross-entropy fixes this by using $\log$, which cancels the squashing effect of sigmoid.

%═══════════════════════════════════════════════════════════════════════════════
\section{Backpropagation}
%═══════════════════════════════════════════════════════════════════════════════

\begin{bluebox}[Section Summary]
Backpropagation is the algorithm for computing gradients in neural networks. It applies the chain rule systematically, computing gradients layer-by-layer from output to input. The key insight: reuse intermediate computations to achieve $O(W)$ complexity where $W$ is the number of weights.
\end{bluebox}

Backpropagation is the workhorse algorithm of deep learning. It efficiently computes how the loss changes with respect to every parameter in the network, enabling gradient-based optimisation.

\subsection{The Credit Assignment Problem}

Training a neural network requires computing $\nabla_\theta \mathcal{L}$: how does the loss change with respect to each parameter?

In deep networks, each parameter contributes to the loss through a long chain of transformations. The \textbf{credit assignment problem} asks: which parameters are ``responsible'' for the error?

Consider a 10-layer network with a prediction error. The error could be due to problems in any layer. Parameters in layer 1 affect the loss through 9 subsequent transformations. How do we attribute ``blame'' correctly?

Backpropagation answers this by propagating error signals backward through the network, attributing ``credit'' or ``blame'' to each parameter proportional to its gradient.

\subsection{Chain Rule Review}

\begin{greybox}[Chain Rule for Composition]
For scalar functions $f: \mathbb{R} \to \mathbb{R}$ and $g: \mathbb{R} \to \mathbb{R}$:
\[
\frac{d}{dx} f(g(x)) = f'(g(x)) \cdot g'(x)
\]

For vector functions $f: \mathbb{R}^m \to \mathbb{R}$ and $g: \mathbb{R}^n \to \mathbb{R}^m$:
\[
\frac{\partial f}{\partial x_j} = \sum_{i=1}^{m} \frac{\partial f}{\partial g_i} \cdot \frac{\partial g_i}{\partial x_j}
\]

In matrix form with Jacobians:
\[
\nabla_x f(g(x)) = J_g^\top \nabla_g f
\]
where $J_g \in \mathbb{R}^{m \times n}$ has entries $(J_g)_{ij} = \frac{\partial g_i}{\partial x_j}$.
\end{greybox}

\textbf{The key insight}: The chain rule tells us how to decompose the derivative of a composition. If $h = f \circ g$, then $h'(x) = f'(g(x)) \cdot g'(x)$. We can compute $h'$ by first computing $g'$ and $f'$ separately, then multiplying.

For neural networks, we have many composed functions. The chain rule lets us decompose the gradient computation into local computations at each layer.

\subsection{Forward and Backward Passes}

\begin{greybox}[Backpropagation via Chain Rule]
For a network $\hat{y} = f_L \circ f_{L-1} \circ \cdots \circ f_1(x)$:

\textbf{Forward pass:} Compute and store activations:
\begin{align*}
a^{(0)} &= x \\
z^{(l)} &= W^{(l)} a^{(l-1)} + b^{(l)} \\
a^{(l)} &= \sigma(z^{(l)})
\end{align*}

\textbf{Backward pass:} Compute gradients from output to input:
\[
\delta^{(L)} = \frac{\partial \mathcal{L}}{\partial z^{(L)}} \quad \text{(output layer gradient)}
\]
For $l = L-1, \ldots, 1$:
\[
\delta^{(l)} = (W^{(l+1)})^\top \delta^{(l+1)} \odot \sigma'(z^{(l)})
\]
where $\odot$ denotes element-wise multiplication.

\textbf{Parameter gradients:}
\[
\frac{\partial \mathcal{L}}{\partial W^{(l)}} = \delta^{(l)} (a^{(l-1)})^\top, \quad \frac{\partial \mathcal{L}}{\partial b^{(l)}} = \delta^{(l)}
\]
\end{greybox}

\textbf{Understanding the backward pass}:

The quantity $\delta^{(l)} = \frac{\partial \mathcal{L}}{\partial z^{(l)}}$ is the ``error signal'' at layer $l$-how sensitive the loss is to changes in the pre-activations.

\begin{itemize}
    \item \textbf{$\delta^{(L)}$}: Computed directly from the loss function. For cross-entropy with softmax, this is simply $\hat{p} - y$.

    \item \textbf{$\delta^{(l)} = (W^{(l+1)})^\top \delta^{(l+1)} \odot \sigma'(z^{(l)})$}: The error from layer $l+1$ is propagated back through the weights $W^{(l+1)}$ and scaled by the activation derivative $\sigma'(z^{(l)})$.

    \item \textbf{Why $(W^{(l+1)})^\top$}: During the forward pass, $z^{(l+1)} = W^{(l+1)} a^{(l)}$. The Jacobian of this with respect to $a^{(l)}$ is $W^{(l+1)}$. In the backward pass, we multiply by the transpose.

    \item \textbf{Why $\odot \sigma'(z^{(l)})$}: The activation $a^{(l)} = \sigma(z^{(l)})$ introduces element-wise nonlinearity. Its Jacobian is diagonal with entries $\sigma'(z^{(l)}_i)$.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{figures/week_10_neural_nets/Back propagation.png}
    \caption{Backpropagation computes gradients layer by layer using the chain rule.}
    \label{fig:backprop}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{figures/week_10_neural_nets/MLP.png}
    \caption{MLP structure with alternating linear and non-linear layers.}
    \label{fig:mlp}
\end{figure}

The gradient for each layer reuses computations from later layers. This is the key efficiency: we do not recompute the chain from scratch for each parameter.

\begin{align*}
\frac{\partial \mathcal{L}}{\partial \theta_3} &= \frac{\partial \mathcal{L}}{\partial x_4} \cdot \frac{\partial x_4}{\partial \theta_3} \\
\frac{\partial \mathcal{L}}{\partial \theta_2} &= \frac{\partial \mathcal{L}}{\partial x_4} \cdot \frac{\partial x_4}{\partial x_3} \cdot \frac{\partial x_3}{\partial \theta_2} \\
\frac{\partial \mathcal{L}}{\partial \theta_1} &= \frac{\partial \mathcal{L}}{\partial x_4} \cdot \frac{\partial x_4}{\partial x_3} \cdot \frac{\partial x_3}{\partial x_2} \cdot \frac{\partial x_2}{\partial \theta_1}
\end{align*}

Notice how $\frac{\partial \mathcal{L}}{\partial x_4}$ and $\frac{\partial \mathcal{L}}{\partial x_4} \cdot \frac{\partial x_4}{\partial x_3}$ are reused across multiple parameter gradients.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{figures/week_10_neural_nets/backpror algo.png}
    \caption{Backpropagation algorithm: gradients are computed iteratively from output to input.}
    \label{fig:backprop-algo}
\end{figure}

\subsection{Worked Example: Two-Layer Network}

\begin{greybox}[Backpropagation Example]
Consider a network with:
\begin{itemize}
    \item Input: $x \in \mathbb{R}^2$
    \item Hidden layer: 2 neurons, ReLU activation
    \item Output: 1 neuron, sigmoid activation, binary cross-entropy loss
\end{itemize}

\textbf{Forward pass:}
\begin{align*}
z^{(1)} &= W^{(1)} x + b^{(1)} \in \mathbb{R}^2 \\
a^{(1)} &= \text{ReLU}(z^{(1)}) \in \mathbb{R}^2 \\
z^{(2)} &= W^{(2)} a^{(1)} + b^{(2)} \in \mathbb{R} \\
\hat{y} &= \sigma(z^{(2)}) \in (0, 1) \\
\mathcal{L} &= -[y \log \hat{y} + (1-y) \log(1-\hat{y})]
\end{align*}

\textbf{Backward pass:}

\textit{Step 1:} Output layer gradient (combining sigmoid and cross-entropy):
\[
\delta^{(2)} = \frac{\partial \mathcal{L}}{\partial z^{(2)}} = \hat{y} - y
\]

\textit{Step 2:} Gradients for $W^{(2)}$, $b^{(2)}$:
\[
\frac{\partial \mathcal{L}}{\partial W^{(2)}} = \delta^{(2)} (a^{(1)})^\top, \quad \frac{\partial \mathcal{L}}{\partial b^{(2)}} = \delta^{(2)}
\]

\textit{Step 3:} Propagate to hidden layer:
\[
\delta^{(1)} = (W^{(2)})^\top \delta^{(2)} \odot \mathbb{I}(z^{(1)} > 0)
\]

\textit{Step 4:} Gradients for $W^{(1)}$, $b^{(1)}$:
\[
\frac{\partial \mathcal{L}}{\partial W^{(1)}} = \delta^{(1)} x^\top, \quad \frac{\partial \mathcal{L}}{\partial b^{(1)}} = \delta^{(1)}
\]
\end{greybox}

\textbf{Numerical example:} Let $x = [1, 0.5]^\top$, $y = 1$, and suppose:
\[
W^{(1)} = \begin{bmatrix} 0.5 & 0.3 \\ -0.2 & 0.8 \end{bmatrix}, \quad b^{(1)} = \begin{bmatrix} 0.1 \\ -0.1 \end{bmatrix}
\]
\[
W^{(2)} = [0.6, 0.4], \quad b^{(2)} = 0.2
\]

\textbf{Forward pass}:
\begin{align*}
z^{(1)} &= \begin{bmatrix} 0.5 \cdot 1 + 0.3 \cdot 0.5 + 0.1 \\ -0.2 \cdot 1 + 0.8 \cdot 0.5 - 0.1 \end{bmatrix} = \begin{bmatrix} 0.75 \\ 0.1 \end{bmatrix} \\
a^{(1)} &= \text{ReLU}(z^{(1)}) = \begin{bmatrix} 0.75 \\ 0.1 \end{bmatrix} \quad \text{(both positive, so unchanged)} \\
z^{(2)} &= 0.6 \cdot 0.75 + 0.4 \cdot 0.1 + 0.2 = 0.69 \\
\hat{y} &= \sigma(0.69) \approx 0.666
\end{align*}

\textbf{Backward pass}:
\begin{align*}
\delta^{(2)} &= 0.666 - 1 = -0.334 \quad \text{(we under-predicted; need to increase $z^{(2)}$)} \\
\frac{\partial \mathcal{L}}{\partial W^{(2)}} &= -0.334 \cdot [0.75, 0.1] = [-0.251, -0.033] \\
\delta^{(1)} &= \begin{bmatrix} 0.6 \\ 0.4 \end{bmatrix} \cdot (-0.334) \odot \begin{bmatrix} 1 \\ 1 \end{bmatrix} = \begin{bmatrix} -0.200 \\ -0.134 \end{bmatrix} \\
\frac{\partial \mathcal{L}}{\partial W^{(1)}} &= \begin{bmatrix} -0.200 \\ -0.134 \end{bmatrix} [1, 0.5] = \begin{bmatrix} -0.200 & -0.100 \\ -0.134 & -0.067 \end{bmatrix}
\end{align*}

Note that all gradients are negative, meaning we should increase all weights to increase the output toward the correct label $y = 1$.

\subsection{Computational Graph Perspective}

Modern frameworks represent computations as directed acyclic graphs (DAGs):
\begin{itemize}
    \item \textbf{Nodes:} Operations (add, multiply, activation)
    \item \textbf{Edges:} Data flow (tensors)
\end{itemize}

Backpropagation traverses this graph in reverse topological order, applying the chain rule at each node. This generalises naturally to arbitrary architectures beyond simple feedforward networks.

\subsection{Automatic Differentiation}

Modern frameworks (PyTorch, TensorFlow, JAX) use \textbf{automatic differentiation}: decomposing functions into atomic operations with known derivatives, then applying chain rule automatically.

\begin{bluebox}[Differentiation Rules for Composition]
\begin{itemize}
    \item \textbf{Addition}: $\nabla_x(f + g) = \nabla_x f + \nabla_x g$
    \item \textbf{Multiplication}: $\nabla_x(f \cdot g) = f \cdot \nabla_x g + g \cdot \nabla_x f$
    \item \textbf{Composition}: $\nabla_x f(g(x)) = \nabla_u f(u)|_{u=g(x)} \cdot \nabla_x g(x)$
\end{itemize}
\end{bluebox}

The key distinction from symbolic differentiation: autodiff operates on \emph{evaluated} computation graphs, not symbolic expressions. It computes numerical gradients without forming explicit derivative expressions.

\textbf{Forward vs reverse mode}: Autodiff can be implemented in two modes:
\begin{itemize}
    \item \textbf{Forward mode}: Computes $\frac{\partial \text{output}}{\partial \text{one input}}$ efficiently. Good when there are few inputs and many outputs.
    \item \textbf{Reverse mode}: Computes $\frac{\partial \text{one output}}{\partial \text{all inputs}}$ efficiently. Good when there is one output (the loss) and many inputs (parameters). This is backpropagation.
\end{itemize}

Neural networks have one loss and millions of parameters, so reverse mode (backprop) is the right choice.

%═══════════════════════════════════════════════════════════════════════════════
\section{Training Neural Networks}
%═══════════════════════════════════════════════════════════════════════════════

\begin{bluebox}[Section Summary]
Training neural networks requires choosing an optimiser, learning rate, and batch size. Mini-batch stochastic gradient descent balances computational efficiency with gradient variance. Adam adapts learning rates per-parameter and is the default choice for most applications.
\end{bluebox}

Training a neural network means finding parameters $\theta$ that minimise the loss. This is done via gradient descent (or variants), using gradients computed by backpropagation.

\subsection{Gradient Descent}

\begin{greybox}[Gradient Descent Variants]
\textbf{Batch gradient descent:} Use all $n$ examples per update:
\[
\theta \leftarrow \theta - \eta \cdot \frac{1}{n} \sum_{i=1}^{n} \nabla_\theta \ell(y_i, f(x_i; \theta))
\]
Low variance but expensive per update.

\textbf{Stochastic gradient descent (SGD):} Use one example per update:
\[
\theta \leftarrow \theta - \eta \cdot \nabla_\theta \ell(y_i, f(x_i; \theta))
\]
Cheap per update but high variance.

\textbf{Mini-batch SGD:} Use $B$ examples per update:
\[
\theta \leftarrow \theta - \frac{\eta}{B} \sum_{b=1}^{B} \nabla_\theta \ell(y_b, f(x_b; \theta))
\]
The practical default. Typical batch sizes: 32--512.
\end{greybox}

\textbf{Why mini-batches?}
\begin{itemize}
    \item \textbf{Computational efficiency}: Modern GPUs are optimised for batch operations. Processing 64 examples is often only marginally slower than processing 1.
    \item \textbf{Reduced variance}: Averaging over $B$ examples reduces gradient variance by a factor of $\sqrt{B}$.
    \item \textbf{Regularisation effect}: The noise from using different mini-batches can help escape local minima and improve generalisation.
\end{itemize}

\begin{greybox}[SGD Update Rule]
\[
\theta(t+1) = \theta(t) - \frac{\eta_t}{B} \sum_{b=1}^{B} \nabla_\theta \ell(y_b, f(x_b; \theta))
\]

where:
\begin{itemize}
    \item $\eta_t$ = learning rate (possibly time-varying)
    \item $B$ = mini-batch size
    \item $\nabla_\theta \ell$ = gradient of loss with respect to parameters
\end{itemize}
\end{greybox}

\subsection{Learning Rate}

\begin{redbox}
The learning rate $\eta$ is often the most important hyperparameter:
\begin{itemize}
    \item \textbf{Too large:} Training diverges; loss oscillates or explodes
    \item \textbf{Too small:} Training is slow; may get stuck in poor local minima
    \item \textbf{Just right:} Rapid progress toward good solutions
\end{itemize}

Common strategies:
\begin{itemize}
    \item Start with $\eta \in \{0.1, 0.01, 0.001\}$ and adjust based on loss curves
    \item Use learning rate schedules (decay over time)
    \item Use adaptive methods (Adam) that adjust rates automatically
\end{itemize}
\end{redbox}

\textbf{Diagnosing learning rate problems}:
\begin{itemize}
    \item \textbf{Loss increases or oscillates wildly}: Learning rate too high. Try reducing by 3--10x.
    \item \textbf{Loss decreases very slowly}: Learning rate too low. Try increasing by 3--10x.
    \item \textbf{Loss decreases then plateaus}: May need learning rate decay, or the model has converged.
\end{itemize}

\subsection{Optimisers}

\begin{greybox}[SGD with Momentum]
Momentum accumulates gradients over time, smoothing updates:
\[
v_t = \beta v_{t-1} + \nabla_\theta \mathcal{L}
\]
\[
\theta_t = \theta_{t-1} - \eta v_t
\]
where $\beta \approx 0.9$ is the momentum coefficient.

\textbf{Intuition:} Momentum helps escape local minima and damps oscillations in narrow valleys.
\end{greybox}

\textbf{Why momentum helps}: Imagine rolling a ball down a loss landscape. Without momentum, the ball moves directly downhill at each point. With momentum, the ball builds up speed in consistent directions and can roll through small bumps.

In narrow valleys (where the loss is steep in one direction but shallow in another), vanilla SGD oscillates back and forth across the valley while making slow progress along it. Momentum accumulates the consistent ``down the valley'' component while averaging out the oscillating ``across the valley'' component.

\begin{greybox}[Adam Optimiser]
Adam combines momentum with adaptive learning rates:

\textbf{Momentum} (exponential average of gradients):
\[
m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t
\]

\textbf{Adaptive scaling} (exponential average of squared gradients):
\[
v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2
\]

\textbf{Bias correction} (important for early iterations):
\[
\hat{m}_t = \frac{m_t}{1 - \beta_1^t}, \quad \hat{v}_t = \frac{v_t}{1 - \beta_2^t}
\]

\textbf{Update:}
\[
\theta_t = \theta_{t-1} - \frac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t
\]

\textbf{Default values}: $\beta_1 = 0.9$, $\beta_2 = 0.999$, $\epsilon = 10^{-8}$, $\eta = 0.001$
\end{greybox}

\textbf{Understanding Adam}:
\begin{itemize}
    \item \textbf{$m_t$}: Like momentum, tracks the direction of recent gradients.
    \item \textbf{$v_t$}: Tracks the magnitude of recent gradients per parameter.
    \item \textbf{Division by $\sqrt{\hat{v}_t}$}: Parameters with large gradients get smaller effective learning rates; parameters with small gradients get larger effective rates. This is ``adaptive.''
    \item \textbf{Bias correction}: At the start, $m_t$ and $v_t$ are biased toward zero (they are initialised to zero and slowly accumulate). The correction factors compensate for this.
\end{itemize}

\textbf{Choosing an optimiser:}
\begin{itemize}
    \item \textbf{Adam}: Default for most deep learning; adaptive and robust
    \item \textbf{SGD with momentum}: Sometimes better final performance; requires more tuning
    \item \textbf{L-BFGS}: Small problems with smooth objectives; converges quickly
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/week_10_neural_nets/MLP Recipe.png}
    \caption{MLP design recipe: architecture, loss function, and optimiser.}
    \label{fig:mlp-recipe}
\end{figure}

\begin{bluebox}[MLP Design Choices]
\textbf{Architecture:}
\begin{itemize}
    \item Number of layers (depth)
    \item Units per layer (width)
    \item Activation functions
\end{itemize}

\textbf{Loss function:}
\begin{itemize}
    \item Regression: MSE $\ell = (y - \hat{y})^2$
    \item Classification: Cross-entropy $\ell = -y\log\hat{y} - (1-y)\log(1-\hat{y})$
\end{itemize}

\textbf{Optimiser:}
\begin{itemize}
    \item Adam (adaptive learning rates)-default choice
    \item SGD with momentum (often better final performance)
    \item L-BFGS (second-order; small problems only)
\end{itemize}
\end{bluebox}

%═══════════════════════════════════════════════════════════════════════════════
\section{Initialisation}
%═══════════════════════════════════════════════════════════════════════════════

\begin{bluebox}[Section Summary]
Proper weight initialisation is essential for training deep networks. Poor initialisation causes vanishing/exploding activations before training even begins. Xavier and He initialisations calibrate weight variance to maintain stable activation magnitudes across layers.
\end{bluebox}

How we initialise the weights before training begins has a surprisingly large effect on whether training succeeds at all.

\subsection{Why Initialisation Matters}

\begin{greybox}[The Problem with Naive Initialisation]
Consider initialising all weights to the same value (e.g., zero or a constant):
\begin{itemize}
    \item \textbf{All zeros:} All hidden neurons compute the same output. By symmetry, they receive the same gradient and remain identical forever. The network has effectively one neuron per layer.
    \item \textbf{All same constant:} Same problem-symmetry is not broken.
    \item \textbf{Too large:} Activations explode; sigmoid/tanh saturate immediately.
    \item \textbf{Too small:} Activations shrink toward zero through layers.
\end{itemize}
\end{greybox}

\textbf{The symmetry problem}: If all weights start identical, all neurons in a layer compute identical outputs. During backprop, they receive identical gradients. After the update, they remain identical. The network cannot learn different features in different neurons-it has collapsed to effectively one neuron per layer.

\textbf{Solution}: Random initialisation breaks symmetry. Each neuron starts slightly different, learns slightly different gradients, and evolves into a unique feature detector.

We need initialisations that:
\begin{enumerate}
    \item Break symmetry (different neurons learn different features)
    \item Maintain activation variance across layers
    \item Maintain gradient variance across layers (for backprop)
\end{enumerate}

\subsection{Xavier/Glorot Initialisation}

\begin{greybox}[Xavier Initialisation]
For a layer with $n_{\text{in}}$ input neurons and $n_{\text{out}}$ output neurons, initialise weights as:
\[
W_{ij} \sim \mathcal{N}\left(0, \frac{2}{n_{\text{in}} + n_{\text{out}}}\right)
\]
or uniformly:
\[
W_{ij} \sim \text{Uniform}\left(-\sqrt{\frac{6}{n_{\text{in}} + n_{\text{out}}}}, \sqrt{\frac{6}{n_{\text{in}} + n_{\text{out}}}}\right)
\]

\textbf{Derivation intuition:} If $z = \sum_{i=1}^{n} w_i x_i$ and we want $\text{Var}(z) = \text{Var}(x)$, then with $\text{Var}(w_i) = \sigma^2_w$:
\[
\text{Var}(z) = n \cdot \sigma^2_w \cdot \text{Var}(x)
\]
Setting $\sigma^2_w = 1/n$ preserves variance in the forward pass. Xavier averages the requirements of forward and backward passes.
\end{greybox}

\textbf{Deriving the variance}: Assume inputs $x_i$ are i.i.d. with mean 0 and variance $\text{Var}(x)$, and weights $w_i$ are i.i.d. with mean 0 and variance $\sigma^2_w$. Then:
\[
\text{Var}(z) = \text{Var}\left(\sum_i w_i x_i\right) = \sum_i \text{Var}(w_i x_i) = \sum_i E[w_i^2] E[x_i^2] = n \sigma^2_w \text{Var}(x)
\]

To keep variance stable ($\text{Var}(z) = \text{Var}(x)$), we need $n \sigma^2_w = 1$, so $\sigma^2_w = 1/n$.

Xavier initialisation averages the forward and backward requirements: $\sigma^2_w = 2/(n_{\text{in}} + n_{\text{out}})$.

Xavier initialisation was designed for sigmoid and tanh activations, which are roughly linear near zero.

\subsection{He Initialisation}

\begin{greybox}[He Initialisation (for ReLU)]
For ReLU activations, use:
\[
W_{ij} \sim \mathcal{N}\left(0, \frac{2}{n_{\text{in}}}\right)
\]

\textbf{Why different from Xavier?} ReLU zeros out half the activations (those with negative pre-activations). This halves the variance, so we double the weight variance to compensate:
\[
\text{Var}(z) = \frac{1}{2} \cdot n \cdot \sigma^2_w \cdot \text{Var}(x)
\]
Setting $\sigma^2_w = 2/n$ maintains unit variance through ReLU.
\end{greybox}

\textbf{Why ReLU needs different initialisation}: After ReLU, roughly half the activations are zero (those with negative inputs). The variance of the output is therefore half the variance of the input. To compensate, we need twice the weight variance.

\begin{bluebox}[Initialisation Guidelines]
\begin{itemize}
    \item \textbf{Sigmoid/Tanh:} Xavier initialisation
    \item \textbf{ReLU/Leaky ReLU:} He initialisation
    \item \textbf{Biases:} Usually initialised to zero
    \item Modern frameworks (PyTorch, TensorFlow) use sensible defaults for common layer types
\end{itemize}
\end{bluebox}

%═══════════════════════════════════════════════════════════════════════════════
\section{Vanishing and Exploding Gradients}
%═══════════════════════════════════════════════════════════════════════════════

\begin{bluebox}[Section Summary]
In deep networks, gradients can vanish (approach zero) or explode (grow unboundedly) as they propagate through layers. This makes training unstable or ineffective. Solutions include careful initialisation, non-saturating activations (ReLU), gradient clipping, and normalisation techniques.
\end{bluebox}

As networks become deeper, a fundamental problem emerges: gradients can become either extremely small or extremely large as they propagate backward through many layers.

\begin{greybox}[Gradient Flow Through Depth]
By the chain rule:
\[
\frac{\partial \mathcal{L}}{\partial z^{(l)}} = \frac{\partial \mathcal{L}}{\partial z^{(L)}} \prod_{k=l}^{L-1} \frac{\partial z^{(k+1)}}{\partial z^{(k)}}
\]

If derivatives between layers are approximately constant $\approx J$:
\[
\frac{\partial \mathcal{L}}{\partial z^{(l)}} \approx J^{L-l} \frac{\partial \mathcal{L}}{\partial z^{(L)}}
\]

\textbf{Eigenvalue behaviour:}
\begin{itemize}
    \item $|\lambda_{\max}| < 1$: Gradients vanish exponentially with depth
    \item $|\lambda_{\max}| > 1$: Gradients explode exponentially with depth
\end{itemize}
\end{greybox}

\textbf{The mathematics}: Consider a simplified model where each layer multiplies by a constant factor $\lambda$. After $L - l$ layers, the gradient is scaled by $\lambda^{L-l}$.
\begin{itemize}
    \item If $\lambda = 0.9$ and $L - l = 50$, the factor is $0.9^{50} \approx 0.005$-gradients effectively vanish.
    \item If $\lambda = 1.1$ and $L - l = 50$, the factor is $1.1^{50} \approx 117$-gradients explode.
\end{itemize}

Only when $\lambda \approx 1$ can we train deep networks reliably.

\subsection{Solutions}

\subsubsection{Non-Saturating Activations (for vanishing gradients)}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{figures/week_10_neural_nets/nonsaturating activation functions.png}
    \caption{ReLU and Leaky ReLU maintain gradient flow better than sigmoid.}
    \label{fig:nonsaturating}
\end{figure}

\begin{bluebox}[Activation Function Gradients]
\textbf{Sigmoid}: Gradient $\to 0$ when $\sigma(z) \approx 0$ or $\sigma(z) \approx 1$ (saturation)

\textbf{ReLU}: Gradient is 1 for $z > 0$, 0 for $z < 0$ (dead neurons possible)

\textbf{Leaky ReLU}: Gradient is 1 for $z > 0$, $\alpha$ for $z < 0$ (no saturation, no dead neurons)
\end{bluebox}

The sigmoid derivative $\sigma'(z) = \sigma(z)(1-\sigma(z))$ has maximum value 0.25 (at $z=0$). Even in the best case, gradients shrink by 75\% per layer. After 10 layers: $0.25^{10} \approx 10^{-6}$.

ReLU has derivative 1 for positive inputs, allowing gradients to flow unchanged. This was a key enabler of deep learning.

\subsubsection{Gradient Clipping (for exploding gradients)}

\begin{greybox}[Gradient Clipping]
\[
g \leftarrow \min\left(1, \frac{c}{\|g\|}\right) g
\]

Scales gradients to have maximum norm $c$, preserving direction while preventing instability.
\end{greybox}

\textbf{How it works}: If the gradient norm exceeds threshold $c$, we scale it down to exactly $c$. If the norm is below $c$, we leave it unchanged. The direction is always preserved.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{figures/week_10_neural_nets/gradient clipping.png}
    \caption{Gradient clipping constrains step size while preserving direction.}
    \label{fig:gradient-clipping}
\end{figure}

\subsubsection{Batch Normalisation}

Addresses \textbf{internal covariate shift}: the distribution of layer inputs changes during training as earlier layers update.

\begin{greybox}[Batch Normalisation]
\[
y = \frac{x - \bar{x}_b}{\sqrt{\sigma_b^2 + \epsilon}} \cdot \gamma + \beta
\]

where:
\begin{itemize}
    \item $\bar{x}_b$, $\sigma_b^2$ = batch mean and variance
    \item $\gamma$, $\beta$ = learned scale and shift parameters
    \item $\epsilon$ = small constant for numerical stability
\end{itemize}

\textbf{At test time}: Use exponential moving averages of training statistics.
\end{greybox}

\textbf{What batch normalisation does}: For each mini-batch, it normalises activations to have mean 0 and variance 1. Then it applies learnable scale ($\gamma$) and shift ($\beta$) parameters.

\textbf{Why it helps}:
\begin{itemize}
    \item \textbf{Stable activations}: Keeps activations in a well-behaved range, preventing saturation.
    \item \textbf{Higher learning rates}: With normalised inputs, we can use larger learning rates.
    \item \textbf{Regularisation}: The noise from batch statistics provides implicit regularisation.
\end{itemize}

\textbf{Benefits:}
\begin{itemize}
    \item Allows higher learning rates
    \item Provides regularisation effect
    \item Stabilises gradient flow
\end{itemize}

%═══════════════════════════════════════════════════════════════════════════════
\section{Regularisation}
%═══════════════════════════════════════════════════════════════════════════════

\begin{bluebox}[Section Summary]
Neural networks with many parameters are prone to overfitting. Regularisation techniques constrain model complexity to improve generalisation. Weight decay penalises large weights; dropout forces redundant representations.
\end{bluebox}

Neural networks often have more parameters than training examples. Without regularisation, they can memorise the training data perfectly while failing to generalise.

\subsection{Weight Decay (L2 Regularisation)}

\begin{greybox}[Weight Decay]
\[
\mathcal{L}_{\text{reg}} = \mathcal{L} + \frac{\lambda}{2}\|\mathbf{w}\|^2
\]

Penalises large weights, encouraging simpler models. Often implemented directly in the optimiser rather than as an explicit loss term.
\end{greybox}

\textbf{Why weight decay works}:
\begin{itemize}
    \item \textbf{Prevents extreme weights}: Without regularisation, weights can grow arbitrarily large if it helps fit the training data, even slightly.
    \item \textbf{Smooth functions}: Large weights create sharp, wiggly decision boundaries. Small weights create smoother boundaries that generalise better.
    \item \textbf{Bayesian interpretation}: Weight decay corresponds to a Gaussian prior on weights-we prefer weights near zero a priori.
\end{itemize}

\subsection{Dropout}

\begin{greybox}[Dropout]
During training, randomly set a fraction $p$ of activations to zero:
\[
\tilde{h}_i = \frac{1}{1-p} \cdot h_i \cdot \text{Bernoulli}(1-p)
\]

At test time, use all neurons without dropout (but scale by $1-p$, or equivalently, scale training activations by $1/(1-p)$).

\textbf{Effect}: Forces the network to learn redundant representations; acts as ensemble averaging over $2^n$ subnetworks.
\end{greybox}

\textbf{Understanding dropout}:
\begin{itemize}
    \item During training, each forward pass uses a different random subset of neurons.
    \item The scaling factor $1/(1-p)$ ensures the expected output is unchanged.
    \item At test time, we use all neurons, which approximates averaging over all possible dropout masks.
\end{itemize}

\textbf{Why dropout works}:
\begin{itemize}
    \item \textbf{Prevents co-adaptation}: Neurons cannot rely on specific partners being present. Each neuron must be independently useful.
    \item \textbf{Ensemble effect}: Dropout trains an exponential number of ``thinned'' networks. The test-time network approximates the average of all these networks.
    \item \textbf{Redundant representations}: Forces the network to spread information across many neurons, making it robust to noise and missing features.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{figures/week_10_neural_nets/regularization.png}
    \caption{Dropout randomly removes connections during training.}
    \label{fig:dropout}
\end{figure}

Weight decay and dropout are complementary: weight decay constrains magnitudes; dropout encourages distributed representations.

%═══════════════════════════════════════════════════════════════════════════════
\section{Neural Networks as Gaussian Processes}
%═══════════════════════════════════════════════════════════════════════════════

\begin{bluebox}[Infinite-Width Limit]
As the number of hidden units $\to \infty$, an MLP's output distribution converges to a Gaussian Process. The \textbf{Neural Tangent Kernel} (NTK) characterises this:

\[
k(x, y) = \nabla_\theta f(x; \theta)^\top \nabla_\theta f(y; \theta)
\]

In this limit, the NTK remains approximately constant during training, and the network behaves like kernel regression.
\end{bluebox}

This is a remarkable theoretical connection that helps us understand neural networks through the lens of well-understood Gaussian processes.

\textbf{The intuition}: With randomly initialised weights, a neural network's output for any fixed input is a sum of many random terms (the contributions from each neuron). By the central limit theorem, this sum becomes Gaussian as the number of neurons grows. Moreover, the covariance between outputs at different inputs defines a kernel function.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{figures/week_10_neural_nets/MLP is a GP.png}
    \caption{MLPs (red) as samples from a GP (blue) defined by the NTK.}
    \label{fig:mlp-gp}
\end{figure}

\textbf{The Neural Tangent Kernel}: The NTK measures how similar the gradients are at two different inputs:
\[
k(x, y) = \nabla_\theta f(x; \theta)^\top \nabla_\theta f(y; \theta)
\]

If the gradients at $x$ and $y$ point in similar directions, these inputs are ``similar'' from the network's perspective-updating the network to fit $x$ will also affect its prediction at $y$.

This connection provides theoretical tools for understanding deep learning and bridges parametric (NNs) and non-parametric (GPs) approaches. It explains some phenomena like the ability of overparameterised networks to generalise despite having more parameters than data points.

\textbf{Practical implications}: While infinite-width limits are idealised, they suggest:
\begin{itemize}
    \item Wider networks may be easier to analyse theoretically.
    \item Architecture choices (depth, activation functions) affect the implicit kernel.
    \item Overparameterised networks can generalise well, contrary to classical intuition.
\end{itemize}

%═══════════════════════════════════════════════════════════════════════════════
\section{Looking Ahead: Advanced Architectures}
%═══════════════════════════════════════════════════════════════════════════════

\begin{bluebox}[Connection to Week 11]
The foundational concepts from this week-layers, activations, backpropagation, and training-extend to specialised architectures:

\begin{itemize}
    \item \textbf{Convolutional Neural Networks (CNNs)}: Exploit spatial structure in images through local connectivity and weight sharing. Convolution replaces full matrix multiplication, encoding the inductive bias that nearby pixels are related.

    \item \textbf{Recurrent Neural Networks (RNNs)}: Process sequential data by maintaining hidden state across time steps. Backpropagation through time (BPTT) applies the same principles across the unrolled computation graph.

    \item \textbf{Attention Mechanisms}: Enable flexible, learned weighting of inputs. The Transformer architecture builds entirely on attention, achieving state-of-the-art results in NLP and increasingly in other domains.
\end{itemize}

Week 11 will develop these architectures in detail, showing how domain-specific inductive biases enable learning from images, sequences, and structured data.
\end{bluebox}

%═══════════════════════════════════════════════════════════════════════════════
\section{Summary}
%═══════════════════════════════════════════════════════════════════════════════

\begin{bluebox}[Key Concepts from Week 10]
\begin{enumerate}
    \item \textbf{Biological inspiration}: Neurons inspire artificial networks, but the analogy is loose
    \item \textbf{Perceptrons}: Linear classifiers with provable convergence for separable data
    \item \textbf{Convergence theorem}: At most $(R/\gamma)^2$ updates for margin $\gamma$, radius $R$
    \item \textbf{Linear separability}: Single-layer networks can only learn linearly separable functions
    \item \textbf{XOR problem}: Demonstrates perceptron limitations; motivates multi-layer networks
    \item \textbf{MLPs}: Learn feature representations through composed layers
    \item \textbf{Non-linearity}: Essential-without it, depth provides no benefit
    \item \textbf{Activation functions}: ReLU for hidden layers; sigmoid/softmax for outputs
    \item \textbf{Loss functions}: MSE for regression; cross-entropy for classification
    \item \textbf{Backpropagation}: Chain rule applied layer-by-layer; $O(W)$ complexity
    \item \textbf{Training}: Mini-batch SGD; Adam as default optimiser
    \item \textbf{Initialisation}: Xavier for sigmoid/tanh; He for ReLU
    \item \textbf{Gradient problems}: Vanishing (use ReLU, batch norm); Exploding (use clipping)
    \item \textbf{Regularisation}: Weight decay + dropout prevent overfitting
    \item \textbf{Universal approximation}: One hidden layer suffices theoretically; depth helps empirically
\end{enumerate}
\end{bluebox}

\begin{bluebox}[Practical Checklist]
When building an MLP:
\begin{enumerate}
    \item \textbf{Architecture}: Start with 2--3 hidden layers, width 128--512
    \item \textbf{Activation}: ReLU for hidden layers
    \item \textbf{Output}: Linear (regression), sigmoid (binary), softmax (multi-class)
    \item \textbf{Loss}: MSE (regression), cross-entropy (classification)
    \item \textbf{Initialisation}: He for ReLU networks
    \item \textbf{Optimiser}: Adam with default parameters
    \item \textbf{Learning rate}: Start with $10^{-3}$; adjust based on training curves
    \item \textbf{Regularisation}: Dropout (0.2--0.5) and/or weight decay ($10^{-4}$)
    \item \textbf{Batch size}: 32--256 typically
    \item \textbf{Monitor}: Training and validation loss; watch for overfitting
\end{enumerate}
\end{bluebox}
