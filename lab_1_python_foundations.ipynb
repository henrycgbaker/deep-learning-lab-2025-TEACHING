{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: Python Foundations for Deep Learning\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "1. Write type-annotated Python functions using modern syntax\n",
    "2. Apply Pythonic patterns (comprehensions, context managers, enumerate/zip)\n",
    "3. Perform NumPy array operations including broadcasting and reshaping\n",
    "4. Load, preprocess, and visualise data with Pandas and Matplotlib\n",
    "5. Understand OOP conventions used in PyTorch (classes, `__call__`, etc.)\n",
    "6. Implement linear regression using sklearn and from scratch\n",
    "7. Write generators for memory-efficient data processing\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Basic Python programming (variables, loops, functions, classes)\n",
    "- Familiarity with importing packages\n",
    "\n",
    "## Why This Lab?\n",
    "\n",
    "This lab covers **all Python/ML foundations** needed before diving into deep learning:\n",
    "- **Type hints** make code self-documenting\n",
    "- **NumPy broadcasting** is essential for tensor operations\n",
    "- **Pandas** for data loading and preprocessing\n",
    "- **sklearn basics** bridge to neural network training\n",
    "- **OOP patterns** like `__call__` are central to `nn.Module`\n",
    "- **Generators** power PyTorch DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Environment Setup ====\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Detect environment\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"Running on Google Colab\")\n",
    "else:\n",
    "    print(\"Running locally\")\n",
    "\n",
    "def download_file(url: str, filename: str) -> str:\n",
    "    \"\"\"Download file if it doesn't exist. Works on both Colab and local.\"\"\"\n",
    "    if os.path.exists(filename):\n",
    "        print(f\"'{filename}' already exists\")\n",
    "        return filename\n",
    "    \n",
    "    print(f\"Downloading {filename}...\")\n",
    "    if IN_COLAB:\n",
    "        import subprocess\n",
    "        subprocess.run(['wget', '-q', url, '-O', filename], check=True)\n",
    "    else:\n",
    "        import urllib.request\n",
    "        urllib.request.urlretrieve(url, filename)\n",
    "    print(f\"Downloaded {filename}\")\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Device Setup ====\n",
    "import torch\n",
    "\n",
    "def get_device():\n",
    "    \"\"\"Get best available device: CUDA > MPS > CPU.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "        print(f\"Using CUDA GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "        device = torch.device('mps')\n",
    "        print(\"Using Apple MPS (Metal)\")\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        print(\"Using CPU\")\n",
    "    return device\n",
    "\n",
    "DEVICE = get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Python Foundations for Deep Learning\n",
    "\n",
    "---\n",
    "\n",
    "## 2.1 Type Hints\n",
    "\n",
    "Type hints make code self-documenting and enable better IDE support.\n",
    "\n",
    "### Basic Syntax (Python 3.10+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic type hints\n",
    "def greet(name: str) -> str:\n",
    "    return f\"Hello, {name}!\"\n",
    "\n",
    "# Collections - use lowercase (Python 3.10+)\n",
    "def average(values: list[float]) -> float:\n",
    "    return sum(values) / len(values)\n",
    "\n",
    "# Dictionaries\n",
    "def word_count(text: str) -> dict[str, int]:\n",
    "    words = text.lower().split()\n",
    "    return {word: words.count(word) for word in set(words)}\n",
    "\n",
    "# Optional values (can be None)\n",
    "def find_index(items: list[str], target: str) -> int | None:\n",
    "    try:\n",
    "        return items.index(target)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "# Test\n",
    "print(f\"greet('World'): {greet('World')}\")\n",
    "print(f\"average([1, 2, 3, 4, 5]): {average([1, 2, 3, 4, 5])}\")\n",
    "print(f\"word_count('the cat and the dog'): {word_count('the cat and the dog')}\")\n",
    "print(f\"find_index(['a', 'b', 'c'], 'b'): {find_index(['a', 'b', 'c'], 'b')}\")\n",
    "print(f\"find_index(['a', 'b', 'c'], 'x'): {find_index(['a', 'b', 'c'], 'x')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Q: Why use `int | None` instead of `Optional[int]`?</b></summary>\n",
    "\n",
    "**A:** `int | None` is the modern Python 3.10+ syntax. It's more readable and doesn't require importing from `typing`. The older `Optional[int]` still works but is more verbose.\n",
    "\n",
    "```python\n",
    "# Old style (pre-3.10)\n",
    "from typing import Optional, List\n",
    "def f(x: Optional[int]) -> List[str]: ...\n",
    "\n",
    "# Modern style (3.10+)\n",
    "def f(x: int | None) -> list[str]: ...\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Add Type Hints\n",
    "\n",
    "Add type hints to the following functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Add type hints to these functions\n",
    "\n",
    "def calculate_loss(predictions, targets):\n",
    "    \"\"\"Calculate mean squared error loss.\"\"\"\n",
    "    return sum((p - t) ** 2 for p, t in zip(predictions, targets)) / len(predictions)\n",
    "\n",
    "def get_batch(data, batch_idx, batch_size):\n",
    "    \"\"\"Get a batch from data. Returns None if batch_idx out of range.\"\"\"\n",
    "    start = batch_idx * batch_size\n",
    "    if start >= len(data):\n",
    "        return None\n",
    "    return data[start:start + batch_size]\n",
    "\n",
    "def create_optimiser_config(lr, momentum, weight_decay):\n",
    "    \"\"\"Create optimiser configuration dictionary.\"\"\"\n",
    "    return {\"lr\": lr, \"momentum\": momentum, \"weight_decay\": weight_decay}\n",
    "\n",
    "# Test (uncomment after adding hints):\n",
    "# print(calculate_loss([1.0, 2.0], [1.1, 2.2]))\n",
    "# print(get_batch([1,2,3,4,5], 0, 2))\n",
    "# print(create_optimiser_config(0.01, 0.9, 1e-4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solution: Type Hints</b></summary>\n",
    "\n",
    "```python\n",
    "def calculate_loss(predictions: list[float], targets: list[float]) -> float:\n",
    "    \"\"\"Calculate mean squared error loss.\"\"\"\n",
    "    return sum((p - t) ** 2 for p, t in zip(predictions, targets)) / len(predictions)\n",
    "\n",
    "def get_batch(data: list, batch_idx: int, batch_size: int) -> list | None:\n",
    "    \"\"\"Get a batch from data. Returns None if batch_idx out of range.\"\"\"\n",
    "    start = batch_idx * batch_size\n",
    "    if start >= len(data):\n",
    "        return None\n",
    "    return data[start:start + batch_size]\n",
    "\n",
    "def create_optimiser_config(lr: float, momentum: float, weight_decay: float) -> dict[str, float]:\n",
    "    \"\"\"Create optimiser configuration dictionary.\"\"\"\n",
    "    return {\"lr\": lr, \"momentum\": momentum, \"weight_decay\": weight_decay}\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Docstrings\n",
    "\n",
    "Use Google-style docstrings for complex functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    epochs: int = 10,\n",
    "    learning_rate: float = 0.001,\n",
    "    verbose: bool = True\n",
    ") -> dict[str, list[float]]:\n",
    "    \"\"\"\n",
    "    Train a PyTorch model.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model to train (nn.Module)\n",
    "        train_loader: DataLoader with training data\n",
    "        epochs: Number of training epochs\n",
    "        learning_rate: Learning rate for optimiser\n",
    "        verbose: Whether to print progress\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with 'train_loss' history\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If epochs < 1\n",
    "    \n",
    "    Example:\n",
    "        >>> history = train_model(model, loader, epochs=5)\n",
    "        >>> plt.plot(history['train_loss'])\n",
    "    \"\"\"\n",
    "    if epochs < 1:\n",
    "        raise ValueError(\"epochs must be >= 1\")\n",
    "    # ... training code ...\n",
    "    return {\"train_loss\": []}\n",
    "\n",
    "# For simple/obvious functions, a one-liner is fine:\n",
    "def relu(x: float) -> float:\n",
    "    \"\"\"Return max(0, x).\"\"\"\n",
    "    return max(0, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Pythonic Patterns\n",
    "\n",
    "### List Comprehensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of:\n",
    "squares_loop = []\n",
    "for i in range(10):\n",
    "    squares_loop.append(i ** 2)\n",
    "\n",
    "# Use:\n",
    "squares = [i ** 2 for i in range(10)]\n",
    "print(f\"Squares: {squares}\")\n",
    "\n",
    "# With condition\n",
    "evens = [i for i in range(20) if i % 2 == 0]\n",
    "print(f\"Evens: {evens}\")\n",
    "\n",
    "# Dict comprehension\n",
    "word_lengths = {word: len(word) for word in [\"cat\", \"elephant\", \"dog\"]}\n",
    "print(f\"Word lengths: {word_lengths}\")\n",
    "\n",
    "# Set comprehension (removes duplicates)\n",
    "unique_lengths = {len(word) for word in [\"cat\", \"bat\", \"elephant\", \"ant\"]}\n",
    "print(f\"Unique lengths: {unique_lengths}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### enumerate, zip, sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enumerate - get index and value\n",
    "fruits = [\"apple\", \"banana\", \"cherry\"]\n",
    "for i, fruit in enumerate(fruits):\n",
    "    print(f\"{i}: {fruit}\")\n",
    "\n",
    "# zip - iterate multiple sequences together\n",
    "names = [\"Alice\", \"Bob\", \"Charlie\"]\n",
    "scores = [85, 92, 78]\n",
    "for name, score in zip(names, scores):\n",
    "    print(f\"{name}: {score}\")\n",
    "\n",
    "# sorted with key function\n",
    "students = [(\"Alice\", 85), (\"Bob\", 92), (\"Charlie\", 78)]\n",
    "by_score = sorted(students, key=lambda x: x[1], reverse=True)\n",
    "print(f\"By score (desc): {by_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Q: When should you use a list comprehension vs a regular loop?</b></summary>\n",
    "\n",
    "**A:** Use comprehensions when:\n",
    "- Building a new list/dict/set from an iterable\n",
    "- The logic fits on one readable line\n",
    "\n",
    "Use regular loops when:\n",
    "- You need complex logic or multiple statements\n",
    "- You're modifying in place rather than creating new\n",
    "- Readability suffers from one-liner\n",
    "\n",
    "**Rule of thumb:** If you can't understand it in 5 seconds, use a loop.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Pythonic Refactoring\n",
    "\n",
    "Refactor this verbose code to use Pythonic patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERBOSE VERSION - refactor this to be Pythonic!\n",
    "\n",
    "# Task 1: Create list of (name, score) tuples where score > 80\n",
    "names = [\"Alice\", \"Bob\", \"Charlie\", \"Diana\"]\n",
    "scores = [95, 72, 88, 65]\n",
    "high_scorers = []\n",
    "for i in range(len(names)):\n",
    "    if scores[i] > 80:\n",
    "        high_scorers.append((names[i], scores[i]))\n",
    "\n",
    "# Task 2: Create dict mapping filename -> extension\n",
    "files = [\"data.csv\", \"model.pt\", \"config.json\", \"README.md\"]\n",
    "extensions = {}\n",
    "for f in files:\n",
    "    parts = f.split(\".\")\n",
    "    name = parts[0]\n",
    "    ext = parts[1]\n",
    "    extensions[name] = ext\n",
    "\n",
    "# Task 3: Read file, count non-empty lines (use context manager!)\n",
    "f = open(\"test_file.txt\", \"w\")\n",
    "f.write(\"line1\\n\\nline2\\nline3\\n\")\n",
    "f.close()\n",
    "\n",
    "f = open(\"test_file.txt\", \"r\")\n",
    "lines = f.readlines()\n",
    "f.close()\n",
    "count = 0\n",
    "for line in lines:\n",
    "    if line.strip() != \"\":\n",
    "        count = count + 1\n",
    "\n",
    "# Cleanup\n",
    "import os\n",
    "os.remove(\"test_file.txt\")\n",
    "\n",
    "print(f\"High scorers: {high_scorers}\")\n",
    "print(f\"Extensions: {extensions}\")\n",
    "print(f\"Non-empty lines: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solution: Pythonic Refactoring</b></summary>\n",
    "\n",
    "```python\n",
    "# Task 1: zip + list comprehension with filter\n",
    "high_scorers = [(n, s) for n, s in zip(names, scores) if s > 80]\n",
    "\n",
    "# Task 2: dict comprehension with split unpacking\n",
    "extensions = {f.split(\".\")[0]: f.split(\".\")[1] for f in files}\n",
    "# Or cleaner:\n",
    "extensions = {Path(f).stem: Path(f).suffix[1:] for f in files}\n",
    "\n",
    "# Task 3: context manager + sum with generator\n",
    "with open(\"test_file.txt\", \"w\") as f:\n",
    "    f.write(\"line1\\n\\nline2\\nline3\\n\")\n",
    "\n",
    "with open(\"test_file.txt\", \"r\") as f:\n",
    "    count = sum(1 for line in f if line.strip())\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context Managers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context managers ensure cleanup (files close, locks release, etc.)\n",
    "\n",
    "# File I/O - always use 'with'\n",
    "from pathlib import Path\n",
    "\n",
    "# Write\n",
    "with open(\"test.txt\", \"w\") as f:\n",
    "    f.write(\"Hello, World!\")\n",
    "\n",
    "# Read\n",
    "with open(\"test.txt\", \"r\") as f:\n",
    "    content = f.read()\n",
    "print(f\"File content: {content}\")\n",
    "\n",
    "# Clean up\n",
    "Path(\"test.txt\").unlink()\n",
    "\n",
    "# PyTorch example: disable gradients for inference\n",
    "import torch\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    y = x * 2  # No gradient tracking here\n",
    "print(f\"y.requires_grad: {y.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic try/except\n",
    "def safe_divide(a: float, b: float) -> float | None:\n",
    "    try:\n",
    "        return a / b\n",
    "    except ZeroDivisionError:\n",
    "        print(\"Warning: Division by zero\")\n",
    "        return None\n",
    "\n",
    "print(safe_divide(10, 2))\n",
    "print(safe_divide(10, 0))\n",
    "\n",
    "# Multiple exception types with proper chaining\n",
    "def parse_int(s: str) -> int:\n",
    "    try:\n",
    "        return int(s)\n",
    "    except ValueError as e:\n",
    "        raise ValueError(f\"Cannot parse '{s}' as integer\") from e  # Chain exceptions!\n",
    "    except TypeError as e:\n",
    "        raise TypeError(f\"Expected string, got {type(s)}\") from e\n",
    "\n",
    "# finally - always runs (cleanup)\n",
    "def read_with_cleanup(filename: str) -> str:\n",
    "    f = None\n",
    "    try:\n",
    "        f = open(filename, \"r\")\n",
    "        return f.read()\n",
    "    except FileNotFoundError:\n",
    "        return \"\"\n",
    "    finally:\n",
    "        if f:\n",
    "            f.close()\n",
    "            print(\"File closed\")\n",
    "\n",
    "# Demo exception chaining\n",
    "try:\n",
    "    parse_int(\"abc\")\n",
    "except ValueError as e:\n",
    "    print(f\"Caught: {e}\")\n",
    "    print(f\"Original cause: {e.__cause__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Q: When should you catch exceptions vs let them propagate?</b></summary>\n",
    "\n",
    "**A:** \n",
    "- **Catch** when you can handle it meaningfully (retry, default value, cleanup)\n",
    "- **Propagate** when the caller should decide how to handle it\n",
    "\n",
    "**Bad:** Catching everything and hiding errors\n",
    "```python\n",
    "try:\n",
    "    result = do_something()\n",
    "except:  # Never do this!\n",
    "    pass\n",
    "```\n",
    "\n",
    "**Good:** Catch specific exceptions you can handle\n",
    "```python\n",
    "try:\n",
    "    data = load_file(path)\n",
    "except FileNotFoundError:\n",
    "    data = default_data\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 Key Takeaways\n",
    "\n",
    "- **Type hints** (`def f(x: int) -> str`) improve code clarity and IDE support\n",
    "- **Comprehensions** are cleaner than loops for building collections\n",
    "- **Context managers** (`with`) ensure proper resource cleanup\n",
    "- **Exception chaining** (`raise ... from e`) preserves the original error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: NumPy Essentials\n",
    "\n",
    "NumPy is the foundation for all deep learning frameworks. Understanding it is essential.\n",
    "\n",
    "---\n",
    "\n",
    "## 3.1 Why NumPy Matters for Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Vectorisation is MUCH faster than loops\n",
    "size = 1_000_000\n",
    "\n",
    "# Loop version\n",
    "a_list = list(range(size))\n",
    "b_list = list(range(size))\n",
    "\n",
    "start = time.time()\n",
    "c_list = [a + b for a, b in zip(a_list, b_list)]\n",
    "loop_time = time.time() - start\n",
    "\n",
    "# NumPy version\n",
    "a_np = np.arange(size)\n",
    "b_np = np.arange(size)\n",
    "\n",
    "start = time.time()\n",
    "c_np = a_np + b_np\n",
    "numpy_time = time.time() - start\n",
    "\n",
    "print(f\"Loop time: {loop_time:.4f}s\")\n",
    "print(f\"NumPy time: {numpy_time:.4f}s\")\n",
    "print(f\"NumPy is {loop_time/numpy_time:.1f}x faster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Deep Dive: Why is NumPy so fast?</b></summary>\n",
    "\n",
    "NumPy achieves 10-100x speedups through several mechanisms:\n",
    "\n",
    "1. **Contiguous Memory Layout**: Arrays store data in continuous memory blocks, enabling efficient CPU cache utilisation. Python lists store pointers to scattered objects.\n",
    "\n",
    "2. **Compiled C/Fortran Backend**: Core operations are implemented in optimised C code, not interpreted Python.\n",
    "\n",
    "3. **SIMD Vectorisation**: Modern CPUs can process multiple numbers per instruction (Single Instruction, Multiple Data). NumPy operations leverage this automatically.\n",
    "\n",
    "4. **No Type Checking Per Element**: Python lists check types dynamically for each element. NumPy arrays have uniform dtype - no per-element overhead.\n",
    "\n",
    "5. **No Python Object Overhead**: Each Python object has ~28 bytes of overhead (reference count, type pointer, etc.). NumPy stores raw numbers.\n",
    "\n",
    "```python\n",
    "# Memory comparison\n",
    "import sys\n",
    "py_list = [1.0] * 1000\n",
    "np_array = np.ones(1000)\n",
    "print(f\"Python list: {sys.getsizeof(py_list) + sum(sys.getsizeof(x) for x in py_list)} bytes\")\n",
    "print(f\"NumPy array: {np_array.nbytes} bytes\")  # Just 8000 bytes (8 bytes per float64)\n",
    "```\n",
    "\n",
    "**Rule**: If you're looping over array elements in Python, you're probably doing it wrong.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Array Creation & Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Creating arrays\n",
    "a = np.array([1, 2, 3, 4, 5])          # From list\n",
    "b = np.zeros((3, 4))                     # 3x4 zeros\n",
    "c = np.ones((2, 3))                      # 2x3 ones\n",
    "d = np.arange(0, 10, 2)                  # [0, 2, 4, 6, 8]\n",
    "e = np.linspace(0, 1, 5)                 # 5 points from 0 to 1\n",
    "f = np.random.randn(3, 3)                # 3x3 standard normal\n",
    "\n",
    "print(f\"zeros shape: {b.shape}\")\n",
    "print(f\"arange: {d}\")\n",
    "print(f\"linspace: {e}\")\n",
    "\n",
    "# Indexing\n",
    "arr = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "print(f\"\\narr:\\n{arr}\")\n",
    "print(f\"arr[0, 1]: {arr[0, 1]}\")         # Single element\n",
    "print(f\"arr[0, :]: {arr[0, :]}\")         # First row\n",
    "print(f\"arr[:, 1]: {arr[:, 1]}\")         # Second column\n",
    "print(f\"arr[0:2, 1:3]:\\n{arr[0:2, 1:3]}\")  # Subarray\n",
    "\n",
    "# Boolean indexing\n",
    "print(f\"\\narr > 5: {arr[arr > 5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Broadcasting\n",
    "\n",
    "Broadcasting allows operations between arrays of different shapes.\n",
    "\n",
    "### Rules:\n",
    "1. Compare shapes from right to left\n",
    "2. Dimensions match if they're equal OR one of them is 1\n",
    "3. Missing dimensions are treated as 1\n",
    "\n",
    "**Before running each cell below, predict the output shape!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Scalar broadcasts to any shape\n",
    "a = np.array([[1, 2, 3], [4, 5, 6]])  # Shape: (2, 3)\n",
    "print(f\"a + 10:\\n{a + 10}\")  # 10 broadcasts to (2, 3)\n",
    "\n",
    "# Row vector broadcasts across rows\n",
    "row = np.array([100, 200, 300])  # Shape: (3,)\n",
    "print(f\"\\na + row:\\n{a + row}\")  # (3,) -> (2, 3)\n",
    "\n",
    "# Column vector broadcasts across columns\n",
    "col = np.array([[10], [20]])  # Shape: (2, 1)\n",
    "print(f\"\\na + col:\\n{a + col}\")  # (2, 1) -> (2, 3)\n",
    "\n",
    "# Outer product via broadcasting\n",
    "x = np.array([1, 2, 3])[:, np.newaxis]  # Shape: (3, 1)\n",
    "y = np.array([10, 20])                   # Shape: (2,)\n",
    "print(f\"\\nOuter product (x * y):\\n{x * y}\")  # (3, 1) * (2,) -> (3, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Q: Why does `np.array([1,2]) + np.array([[1],[2],[3]])` work?</b></summary>\n",
    "\n",
    "**A:** Let's trace the broadcasting:\n",
    "- Left: shape (2,)\n",
    "- Right: shape (3, 1)\n",
    "\n",
    "Align from right:\n",
    "```\n",
    "     (2,)  ->  (1, 2)  [add dimension]\n",
    "  (3, 1)   ->  (3, 1)\n",
    "  Result:      (3, 2)  [both expand]\n",
    "```\n",
    "\n",
    "Each expands where it has size 1:\n",
    "```python\n",
    "[[1, 2],      [[1, 1],     [[2, 3],\n",
    " [1, 2],  +    [2, 2],  =   [3, 4],\n",
    " [1, 2]]       [3, 3]]      [4, 5]]\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcasting Debugger - useful helper function\n",
    "def broadcast_shapes(*shapes):\n",
    "    \"\"\"Visualise how shapes align and what the result will be.\"\"\"\n",
    "    max_dims = max(len(s) for s in shapes)\n",
    "    \n",
    "    # Pad shapes with 1s on the left\n",
    "    padded = [((1,) * (max_dims - len(s))) + s for s in shapes]\n",
    "    \n",
    "    print(\"Shape alignment (right-aligned):\")\n",
    "    for i, (orig, pad) in enumerate(zip(shapes, padded)):\n",
    "        print(f\"  Array {i+1}: {str(orig):>15} -> {pad}\")\n",
    "    \n",
    "    # Compute result shape\n",
    "    result = []\n",
    "    for dims in zip(*padded):\n",
    "        if len(set(d for d in dims if d != 1)) > 1:\n",
    "            print(f\"\\n❌ INCOMPATIBLE: dimension has {dims} (multiple non-1 values)\")\n",
    "            return None\n",
    "        result.append(max(dims))\n",
    "    \n",
    "    print(f\"\\n✓ Result shape: {tuple(result)}\")\n",
    "    return tuple(result)\n",
    "\n",
    "# Test it\n",
    "print(\"Example 1: (2,3) + (3,)\")\n",
    "broadcast_shapes((2, 3), (3,))\n",
    "\n",
    "print(\"\\nExample 2: (3,1) + (1,4)\")\n",
    "broadcast_shapes((3, 1), (1, 4))\n",
    "\n",
    "print(\"\\nExample 3: Incompatible shapes\")\n",
    "broadcast_shapes((3, 4), (5,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Broadcasting\n",
    "\n",
    "Fix the code to add bias to each sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Data: 100 samples, 784 features (like MNIST flattened)\n",
    "X = np.random.randn(100, 784)\n",
    "bias = np.random.randn(784)\n",
    "\n",
    "# This should add bias to each row\n",
    "result = X + bias  # Does this work?\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"bias shape: {bias.shape}\")\n",
    "print(f\"result shape: {result.shape}\")\n",
    "assert result.shape == (100, 784), \"Shape mismatch!\"\n",
    "print(\"Broadcasting worked!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solution: Broadcasting</b></summary>\n",
    "\n",
    "The code already works! Broadcasting automatically handles this case:\n",
    "\n",
    "```python\n",
    "X.shape     # (100, 784)\n",
    "bias.shape  # (784,)\n",
    "\n",
    "# NumPy aligns from right:\n",
    "#   X:    (100, 784)\n",
    "#   bias:      (784,)  → treated as (1, 784)\n",
    "# Result: (100, 784) ✓\n",
    "```\n",
    "\n",
    "If bias had shape `(100,)` instead, you'd need to reshape:\n",
    "```python\n",
    "bias_wrong = np.random.randn(100)  # Shape (100,)\n",
    "result = X + bias_wrong[:, np.newaxis]  # Reshape to (100, 1) for column broadcast\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Common Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "print(f\"Original shape: {a.shape}\")\n",
    "\n",
    "# Reshape\n",
    "b = a.reshape(3, 2)\n",
    "print(f\"Reshaped to (3,2):\\n{b}\")\n",
    "\n",
    "# Transpose\n",
    "print(f\"Transposed:\\n{a.T}\")\n",
    "\n",
    "# Flatten\n",
    "print(f\"Flattened: {a.flatten()}\")\n",
    "\n",
    "# Concatenate\n",
    "c = np.array([[7, 8, 9]])\n",
    "print(f\"\\nVertical concat:\\n{np.concatenate([a, c], axis=0)}\")\n",
    "\n",
    "d = np.array([[10], [20]])\n",
    "print(f\"\\nHorizontal concat:\\n{np.concatenate([a, d], axis=1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reductions along axes\n",
    "a = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "print(f\"Array:\\n{a}\")\n",
    "\n",
    "print(f\"\\nSum all: {a.sum()}\")\n",
    "print(f\"Sum rows (axis=1): {a.sum(axis=1)}\")     # Sum each row\n",
    "print(f\"Sum cols (axis=0): {a.sum(axis=0)}\")     # Sum each column\n",
    "\n",
    "print(f\"\\nMean all: {a.mean():.2f}\")\n",
    "print(f\"Mean rows: {a.mean(axis=1)}\")\n",
    "\n",
    "# Matrix multiplication\n",
    "W = np.random.randn(3, 4)  # 3x4\n",
    "x = np.random.randn(4, 2)  # 4x2\n",
    "y = W @ x                   # 3x2\n",
    "print(f\"\\nW @ x: {W.shape} @ {x.shape} = {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Q: What's the difference between `axis=0` and `axis=1` in reductions?</b></summary>\n",
    "\n",
    "**A:** The axis parameter specifies which dimension to \"collapse\":\n",
    "- `axis=0`: Collapse rows → result has shape of a single row\n",
    "- `axis=1`: Collapse columns → result has shape of a single column\n",
    "\n",
    "Think of it as: \"sum **along** this axis\" or \"reduce **this** dimension\"\n",
    "\n",
    "```python\n",
    "a = [[1, 2, 3],\n",
    "     [4, 5, 6]]  # Shape (2, 3)\n",
    "\n",
    "a.sum(axis=0)  # [5, 7, 9]   - summed down columns, shape (3,)\n",
    "a.sum(axis=1)  # [6, 15]     - summed across rows, shape (2,)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Shape Prediction\n",
    "\n",
    "**Predict the output shapes before running!** Write your predictions, then verify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# VIEWS: Slicing creates a view (shares memory!)\n",
    "original = np.array([1, 2, 3, 4, 5])\n",
    "view = original[1:4]  # This is a VIEW\n",
    "\n",
    "print(f\"Original: {original}\")\n",
    "print(f\"View: {view}\")\n",
    "\n",
    "# Modifying the view changes the original!\n",
    "view[0] = 999\n",
    "print(f\"After modifying view[0]:\")\n",
    "print(f\"  Original: {original}\")  # Also changed!\n",
    "print(f\"  View: {view}\")\n",
    "\n",
    "# COPIES: Use .copy() to get independent data\n",
    "original = np.array([1, 2, 3, 4, 5])\n",
    "copy = original[1:4].copy()  # Explicit copy\n",
    "\n",
    "copy[0] = 999\n",
    "print(f\"\\nWith .copy():\")\n",
    "print(f\"  Original: {original}\")  # Unchanged!\n",
    "print(f\"  Copy: {copy}\")\n",
    "\n",
    "# How to check: views share memory\n",
    "a = np.array([1, 2, 3])\n",
    "b = a[:]      # View\n",
    "c = a.copy()  # Copy\n",
    "\n",
    "print(f\"\\nShares memory?\")\n",
    "print(f\"  a and b: {np.shares_memory(a, b)}\")  # True\n",
    "print(f\"  a and c: {np.shares_memory(a, c)}\")  # False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Q: Is `arr.reshape(3, 4)` a view or a copy?</b></summary>\n",
    "\n",
    "**A:** It depends! Reshape returns a **view** when possible (if the data is contiguous in memory), but may return a **copy** if the memory layout doesn't allow a view.\n",
    "\n",
    "```python\n",
    "a = np.arange(12).reshape(3, 4)  # Usually a view\n",
    "b = a.T.reshape(6, 2)            # Must be a copy (transpose breaks contiguity)\n",
    "```\n",
    "\n",
    "**Safe approach:** If you need to be sure, use `.copy()` explicitly. If you want to ensure a view (and error otherwise), use `.reshape()` with `order='A'` or `np.ndarray.view()`.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Views vs Copies (Critical!)\n",
    "\n",
    "Understanding when NumPy creates a view vs a copy prevents subtle bugs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3 Key Takeaways\n",
    "\n",
    "- **Vectorise** operations—loops over arrays are slow\n",
    "- **Broadcasting** aligns shapes from the right, expanding size-1 dimensions\n",
    "- **Views** share memory with originals; use `.copy()` for independence\n",
    "- **axis=0** collapses rows, **axis=1** collapses columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: Data Handling with Pandas\n",
    "\n",
    "Pandas is the standard library for tabular data in Python. Essential for loading and preprocessing ML datasets.\n",
    "\n",
    "---\n",
    "\n",
    "## 4.1 Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris, load_wine\n",
    "\n",
    "# Load from sklearn\n",
    "iris = load_iris(as_frame=True)\n",
    "df = iris['data']\n",
    "target = iris['target']\n",
    "\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 DataFrame Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering\n",
    "short_sepals = df[df['sepal length (cm)'] < 5]\n",
    "print(f\"Flowers with sepal < 5cm: {len(short_sepals)}\")\n",
    "\n",
    "# Selecting columns\n",
    "subset = df[['sepal length (cm)', 'petal length (cm)']]\n",
    "print(f\"Subset shape: {subset.shape}\")\n",
    "\n",
    "# Adding columns\n",
    "df_with_target = df.copy()\n",
    "df_with_target['species'] = target\n",
    "df_with_target['species_name'] = df_with_target['species'].map({0: 'setosa', 1: 'versicolour', 2: 'virginica'})\n",
    "\n",
    "# GroupBy\n",
    "print(\"\\nMean by species:\")\n",
    "print(df_with_target.groupby('species_name')[['sepal length (cm)', 'petal length (cm)']].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Q: What's the difference between `df[col]` and `df[[col]]`?</b></summary>\n",
    "\n",
    "**A:** \n",
    "- `df['col']` returns a **Series** (1D)\n",
    "- `df[['col']]` returns a **DataFrame** (2D, single column)\n",
    "\n",
    "```python\n",
    "type(df['sepal length (cm)'])  # pandas.Series\n",
    "type(df[['sepal length (cm)']])  # pandas.DataFrame\n",
    "```\n",
    "\n",
    "Use double brackets when you need to keep the DataFrame structure (e.g., for sklearn).\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data with missing values\n",
    "df_missing = pd.DataFrame({\n",
    "    'A': [1, 2, np.nan, 4, 5],\n",
    "    'B': [np.nan, 2, 3, np.nan, 5],\n",
    "    'C': [1, 2, 3, 4, 5]\n",
    "})\n",
    "\n",
    "print(\"Original:\")\n",
    "print(df_missing)\n",
    "print(f\"\\nMissing values per column:\\n{df_missing.isna().sum()}\")\n",
    "\n",
    "# Option 1: Drop rows with any NaN\n",
    "print(f\"\\nAfter dropna(): {len(df_missing.dropna())} rows\")\n",
    "\n",
    "# Option 2: Fill with value\n",
    "print(f\"\\nFill with 0:\\n{df_missing.fillna(0)}\")\n",
    "\n",
    "# Option 3: Fill with column mean\n",
    "print(f\"\\nFill with mean:\\n{df_missing.fillna(df_missing.mean())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Encoding Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding with pandas\n",
    "df_categories = pd.DataFrame({\n",
    "    'colour': ['red', 'blue', 'green', 'red', 'blue'],\n",
    "    'size': ['S', 'M', 'L', 'M', 'S']\n",
    "})\n",
    "\n",
    "print(\"Original:\")\n",
    "print(df_categories)\n",
    "\n",
    "# One-hot encode\n",
    "encoded = pd.get_dummies(df_categories, columns=['colour', 'size'])\n",
    "print(\"\\nOne-hot encoded:\")\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# Sample data\n",
    "X = np.array([[1, 100], [2, 200], [3, 300], [4, 400]])\n",
    "\n",
    "# StandardScaler: zero mean, unit variance\n",
    "scaler_std = StandardScaler()\n",
    "X_standardised = scaler_std.fit_transform(X)\n",
    "print(\"StandardScaler (mean=0, std=1):\")\n",
    "print(X_standardised)\n",
    "print(f\"Mean: {X_standardised.mean(axis=0)}, Std: {X_standardised.std(axis=0)}\")\n",
    "\n",
    "# MinMaxScaler: scale to [0, 1]\n",
    "scaler_mm = MinMaxScaler()\n",
    "X_minmax = scaler_mm.fit_transform(X)\n",
    "print(\"\\nMinMaxScaler [0, 1]:\")\n",
    "print(X_minmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Q: When should you use StandardScaler vs MinMaxScaler?</b></summary>\n",
    "\n",
    "**A:**\n",
    "- **StandardScaler**: When features follow roughly Gaussian distribution. Works well with most ML algorithms, especially those sensitive to feature magnitudes (SVM, logistic regression, neural networks).\n",
    "\n",
    "- **MinMaxScaler**: When you need bounded values (e.g., [0,1] for image pixels or probabilities). Sensitive to outliers - a single extreme value can compress all other values.\n",
    "\n",
    "**Rule of thumb**: Start with StandardScaler for neural networks. Use MinMaxScaler when interpretability of the scale matters.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4 Key Takeaways\n",
    "\n",
    "- **pandas** is essential for loading and exploring tabular data\n",
    "- Always check for **missing values** before training\n",
    "- **One-hot encoding** converts categories to numeric features\n",
    "- **Scaling** (StandardScaler/MinMaxScaler) improves model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 5: Visualisation with Matplotlib\n",
    "\n",
    "Visualisation is critical for understanding data and debugging models.\n",
    "\n",
    "---\n",
    "\n",
    "## 5.1 Basic Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reload iris for plotting\n",
    "iris = load_iris(as_frame=True)\n",
    "df = iris['data']\n",
    "target = iris['target']\n",
    "\n",
    "# Scatter plot with colour by class\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Left: Sepal dimensions\n",
    "colours = ['red', 'green', 'blue']\n",
    "for i, species in enumerate(['setosa', 'versicolour', 'virginica']):\n",
    "    mask = target == i\n",
    "    axes[0].scatter(df.loc[mask, 'sepal length (cm)'], \n",
    "                   df.loc[mask, 'sepal width (cm)'],\n",
    "                   c=colours[i], label=species, alpha=0.7)\n",
    "axes[0].set_xlabel('Sepal Length (cm)')\n",
    "axes[0].set_ylabel('Sepal Width (cm)')\n",
    "axes[0].set_title('Sepal Dimensions')\n",
    "axes[0].legend()\n",
    "\n",
    "# Right: Petal dimensions\n",
    "for i, species in enumerate(['setosa', 'versicolour', 'virginica']):\n",
    "    mask = target == i\n",
    "    axes[1].scatter(df.loc[mask, 'petal length (cm)'], \n",
    "                   df.loc[mask, 'petal width (cm)'],\n",
    "                   c=colours[i], label=species, alpha=0.7)\n",
    "axes[1].set_xlabel('Petal Length (cm)')\n",
    "axes[1].set_ylabel('Petal Width (cm)')\n",
    "axes[1].set_title('Petal Dimensions')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Histograms and Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of features\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(df.columns):\n",
    "    axes[i].hist(df[col], bins=20, edgecolour='black', alpha=0.7)\n",
    "    axes[i].set_xlabel(col)\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "    axes[i].set_title(f'Distribution of {col}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Plotting for Deep Learning\n",
    "\n",
    "Common plots you'll use when training models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated training history\n",
    "epochs = range(1, 51)\n",
    "train_loss = 2.0 * np.exp(-np.array(epochs) / 10) + 0.1 + np.random.randn(50) * 0.05\n",
    "val_loss = 2.0 * np.exp(-np.array(epochs) / 12) + 0.15 + np.random.randn(50) * 0.08\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Loss curves\n",
    "axes[0].plot(epochs, train_loss, label='Train Loss', colour='blue')\n",
    "axes[0].plot(epochs, val_loss, label='Val Loss', colour='orange')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Curves')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Gradient histogram (simulated)\n",
    "gradients = np.random.randn(1000) * 0.1\n",
    "axes[1].hist(gradients, bins=50, edgecolour='black', alpha=0.7)\n",
    "axes[1].axvline(x=0, colour='red', linestyle='--', label='Zero')\n",
    "axes[1].set_xlabel('Gradient Value')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Gradient Distribution (Healthy)')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Tip: If gradients cluster near 0 -> vanishing gradients\")\n",
    "print(\"     If gradients are huge -> exploding gradients\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Q: What does a bimodal gradient histogram suggest?</b></summary>\n",
    "\n",
    "**A:** A bimodal (two-peaked) gradient histogram often indicates:\n",
    "1. Different layers learning at different rates\n",
    "2. Potential issues with initialisation\n",
    "3. Some weights updating much faster than others\n",
    "\n",
    "Healthy gradient distributions are typically unimodal and centred near zero.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><b>Q: What should you look for in training curves?</b></summary>\n",
    "\n",
    "**A:** Key patterns to watch:\n",
    "- **Train loss decreasing, val loss stable then increasing** → Overfitting, stop earlier\n",
    "- **Both losses plateau high** → Underfitting, increase model capacity\n",
    "- **Loss spikes or oscillates** → Learning rate too high\n",
    "- **Very slow decrease** → Learning rate too low\n",
    "- **Train and val loss track closely** → Good generalisation\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 6: OOP for Deep Learning\n",
    "\n",
    "PyTorch heavily uses OOP. Understanding these patterns is essential.\n",
    "\n",
    "---\n",
    "\n",
    "## 6.1 Classes Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5 Key Takeaways\n",
    "\n",
    "- **Scatter plots** reveal feature relationships and class separability\n",
    "- **Histograms** show feature distributions\n",
    "- **Training curves** diagnose overfitting, underfitting, and learning rate issues\n",
    "- **Gradient histograms** detect vanishing/exploding gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \"\"\"A simple neural network class demonstrating OOP patterns.\"\"\"\n",
    "    \n",
    "    # Class attribute (shared by all instances)\n",
    "    default_activation = \"relu\"\n",
    "    \n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int):\n",
    "        \"\"\"Initialise the network.\"\"\"\n",
    "        # Instance attributes (unique to each instance)\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Simulated weights\n",
    "        self.weights = {\n",
    "            \"W1\": np.random.randn(input_size, hidden_size) * 0.01,\n",
    "            \"W2\": np.random.randn(hidden_size, output_size) * 0.01,\n",
    "        }\n",
    "    \n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        h = x @ self.weights[\"W1\"]\n",
    "        h = np.maximum(0, h)  # ReLU\n",
    "        return h @ self.weights[\"W2\"]\n",
    "\n",
    "# Usage\n",
    "net = NeuralNetwork(784, 128, 10)\n",
    "x = np.random.randn(32, 784)  # Batch of 32\n",
    "output = net.forward(x)\n",
    "print(f\"Input: {x.shape} -> Output: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Naming Conventions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    \"\"\"Demonstrates Python naming conventions.\"\"\"\n",
    "    \n",
    "    def __init__(self, data: list):\n",
    "        self.data = data              # Public: anyone can access\n",
    "        self._cache = {}              # Protected: internal use, but accessible\n",
    "        self.__secret = \"hidden\"      # Private: name-mangled to _DataProcessor__secret\n",
    "    \n",
    "    def process(self):\n",
    "        \"\"\"Public method - part of the API.\"\"\"\n",
    "        return self._preprocess()\n",
    "    \n",
    "    def _preprocess(self):\n",
    "        \"\"\"Protected method - internal helper, but subclasses can override.\"\"\"\n",
    "        return [x * 2 for x in self.data]\n",
    "    \n",
    "    def __validate(self):\n",
    "        \"\"\"Private method - truly internal, not for subclasses.\"\"\"\n",
    "        return all(isinstance(x, (int, float)) for x in self.data)\n",
    "\n",
    "dp = DataProcessor([1, 2, 3])\n",
    "print(f\"Public data: {dp.data}\")\n",
    "print(f\"Protected _cache: {dp._cache}\")  # Works but discouraged\n",
    "# print(dp.__secret)  # AttributeError!\n",
    "print(f\"Mangled name: {dp._DataProcessor__secret}\")  # How to access if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Q: When should you use `_protected` vs `__private`?</b></summary>\n",
    "\n",
    "**A:**\n",
    "- **`_protected`**: Use for internal methods that subclasses might need to override. It's a convention saying \"internal, but accessible.\"\n",
    "\n",
    "- **`__private`**: Use when you truly want to prevent accidental override in subclasses. Python mangles the name to `_ClassName__method`, making it harder (but not impossible) to access.\n",
    "\n",
    "**In practice:** Most Python code uses `_protected`. Use `__private` sparingly.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Dunder Methods\n",
    "\n",
    "Dunder (double underscore) methods let you customise how objects behave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor:\n",
    "    \"\"\"A simple tensor class demonstrating dunder methods.\"\"\"\n",
    "    \n",
    "    def __init__(self, data: list):\n",
    "        self.data = np.array(data)\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        \"\"\"For developers - unambiguous representation.\"\"\"\n",
    "        return f\"Tensor(shape={self.data.shape}, dtype={self.data.dtype})\"\n",
    "    \n",
    "    def __str__(self) -> str:\n",
    "        \"\"\"For users - readable representation.\"\"\"\n",
    "        return f\"Tensor with shape {self.data.shape}\"\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Enable len(tensor).\"\"\"\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Enable tensor[idx].\"\"\"\n",
    "        return self.data[idx]\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        \"\"\"Enable tensor(x) - used heavily in PyTorch!\"\"\"\n",
    "        return self.data @ x\n",
    "\n",
    "t = Tensor([[1, 2], [3, 4]])\n",
    "print(f\"repr: {repr(t)}\")\n",
    "print(f\"str: {str(t)}\")\n",
    "print(f\"len: {len(t)}\")\n",
    "print(f\"t[0]: {t[0]}\")\n",
    "print(f\"t([1, 1]): {t(np.array([1, 1]))}\")  # Callable!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Q: Why does PyTorch use `__call__` for the forward pass?</b></summary>\n",
    "\n",
    "**A:** In PyTorch, `model(x)` calls `model.__call__(x)`, which internally calls `model.forward(x)` but also handles:\n",
    "- Hooks (callbacks before/after forward)\n",
    "- Gradient tracking setup\n",
    "- Module state management\n",
    "\n",
    "This is why you define `forward()` but call `model(x)`, not `model.forward(x)`.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 Inheritance (Essential for PyTorch)\n",
    "\n",
    "PyTorch's `nn.Module` uses inheritance heavily. You'll subclass it for every model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Q: Why do we call `super().__init__()` in subclasses?</b></summary>\n",
    "\n",
    "**A:** `super().__init__()` calls the parent class's `__init__` method, ensuring proper initialisation of inherited attributes. Without it:\n",
    "\n",
    "```python\n",
    "class Linear(Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        # WRONG: forgot super().__init__()\n",
    "        self.weight = ...\n",
    "        \n",
    "layer = Linear(10, 5)\n",
    "print(layer.training)  # AttributeError! .training was never set\n",
    "```\n",
    "\n",
    "In PyTorch, forgetting `super().__init__()` is a common bug that breaks module registration, parameter tracking, and device movement.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified nn.Module-like base class\n",
    "class Module:\n",
    "    \"\"\"Base class demonstrating PyTorch's Module pattern.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._modules = {}\n",
    "        self.training = True\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        \"\"\"When you call model(x), this runs.\"\"\"\n",
    "        return self.forward(x)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Subclasses MUST override this.\"\"\"\n",
    "        raise NotImplementedError(\"Subclasses must implement forward()\")\n",
    "    \n",
    "    def train(self, mode: bool = True):\n",
    "        self.training = mode\n",
    "        return self\n",
    "    \n",
    "    def eval(self):\n",
    "        return self.train(False)\n",
    "\n",
    "\n",
    "# Subclass: A simple linear layer\n",
    "class Linear(Module):\n",
    "    \"\"\"Linear layer: y = x @ W + b\"\"\"\n",
    "    \n",
    "    def __init__(self, in_features: int, out_features: int):\n",
    "        super().__init__()  # Call parent's __init__\n",
    "        self.weight = np.random.randn(in_features, out_features) * 0.01\n",
    "        self.bias = np.zeros(out_features)\n",
    "    \n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        return x @ self.weight + self.bias  # Broadcasting!\n",
    "\n",
    "\n",
    "# Subclass: A two-layer network\n",
    "class TwoLayerNet(Module):\n",
    "    \"\"\"Network that composes multiple layers.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int):\n",
    "        super().__init__()\n",
    "        self.fc1 = Linear(input_size, hidden_size)\n",
    "        self.fc2 = Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        x = self.fc1(x)           # Note: uses __call__, not .forward()\n",
    "        x = np.maximum(0, x)      # ReLU activation\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Usage - this is exactly how you'll use PyTorch!\n",
    "model = TwoLayerNet(784, 128, 10)\n",
    "x = np.random.randn(32, 784)\n",
    "output = model(x)  # Calls __call__ -> forward\n",
    "print(f\"Input: {x.shape} -> Output: {output.shape}\")\n",
    "print(f\"Training mode: {model.training}\")\n",
    "model.eval()\n",
    "print(f\"After eval(): {model.training}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Decorators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, name: str):\n",
    "        self._name = name\n",
    "        self._is_training = True\n",
    "    \n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        \"\"\"Property decorator - access like an attribute.\"\"\"\n",
    "        return self._name\n",
    "    \n",
    "    @property\n",
    "    def is_training(self) -> bool:\n",
    "        return self._is_training\n",
    "    \n",
    "    @is_training.setter\n",
    "    def is_training(self, value: bool):\n",
    "        \"\"\"Setter for property.\"\"\"\n",
    "        self._is_training = value\n",
    "        print(f\"Training mode: {value}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def count_parameters(weights: dict) -> int:\n",
    "        \"\"\"Static method - doesn't need self.\"\"\"\n",
    "        return sum(w.size for w in weights.values())\n",
    "    \n",
    "    @classmethod\n",
    "    def from_config(cls, config: dict):\n",
    "        \"\"\"Class method - alternative constructor.\"\"\"\n",
    "        return cls(name=config.get(\"name\", \"unnamed\"))\n",
    "\n",
    "# Usage\n",
    "m = Model(\"MyModel\")\n",
    "print(f\"Name: {m.name}\")  # Property access\n",
    "m.is_training = False     # Property setter\n",
    "\n",
    "m2 = Model.from_config({\"name\": \"ConfigModel\"})  # Classmethod\n",
    "print(f\"From config: {m2.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 7: sklearn & Linear Regression\n",
    "\n",
    "Before neural networks, understand classical ML. Linear regression is the foundation.\n",
    "\n",
    "---\n",
    "\n",
    "## 7.1 Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "X = np.random.randn(200, 1) * 2\n",
    "y = 3 * X.squeeze() + 2 + np.random.randn(200) * 0.8\n",
    "\n",
    "# Split: 80% train, 20% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Visualise\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(X_train, y_train, alpha=0.5, label='Train')\n",
    "plt.scatter(X_test, y_test, alpha=0.5, label='Test')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Train/Test Split')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Q: Why do we split data into train and test sets?</b></summary>\n",
    "\n",
    "**A:** To evaluate **generalisation** - how well the model performs on unseen data.\n",
    "\n",
    "- **Training set**: Used to fit model parameters\n",
    "- **Test set**: Held out completely, used only for final evaluation\n",
    "\n",
    "If we evaluated on training data, we'd overestimate performance because the model has \"memorised\" those examples. This is called **overfitting**.\n",
    "\n",
    "**Common splits:** 80/20 or 70/30 for train/test.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Linear Regression with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Fit model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Learned parameters\n",
    "print(f\"Learned: y = {model.coef_[0]:.3f}x + {model.intercept_:.3f}\")\n",
    "print(f\"True:    y = 3.000x + 2.000\")\n",
    "\n",
    "# Predictions\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(f\"\\nTrain MSE: {mean_squared_error(y_train, y_pred_train):.4f}\")\n",
    "print(f\"Test MSE:  {mean_squared_error(y_test, y_pred_test):.4f}\")\n",
    "print(f\"Test R²:   {r2_score(y_test, y_pred_test):.4f}\")\n",
    "\n",
    "# Plot fit\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(X_test, y_test, alpha=0.5, label='Test data')\n",
    "X_line = np.linspace(X.min(), X.max(), 100).reshape(-1, 1)\n",
    "plt.plot(X_line, model.predict(X_line), colour='red', linewidth=2, label='Fitted line')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Linear Regression Fit')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Linear Regression from Scratch\n",
    "\n",
    "Implementing gradient descent - the same algorithm that trains neural networks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematical Derivation\n",
    "\n",
    "Before implementing, let's derive the gradient descent update rules.\n",
    "\n",
    "<details>\n",
    "<summary><b>Deep Dive: Deriving MSE Gradients</b></summary>\n",
    "\n",
    "For linear regression with model $\\hat{y} = wx + b$, the Mean Squared Error loss is:\n",
    "\n",
    "$$L = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2 = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - wx_i - b)^2$$\n",
    "\n",
    "**Gradient with respect to $w$:**\n",
    "\n",
    "Using the chain rule:\n",
    "$$\\frac{\\partial L}{\\partial w} = \\frac{1}{n}\\sum_{i=1}^{n} 2(y_i - wx_i - b) \\cdot (-x_i) = -\\frac{2}{n}\\sum_{i=1}^{n} x_i(y_i - \\hat{y}_i)$$\n",
    "\n",
    "**Gradient with respect to $b$:**\n",
    "$$\\frac{\\partial L}{\\partial b} = \\frac{1}{n}\\sum_{i=1}^{n} 2(y_i - wx_i - b) \\cdot (-1) = -\\frac{2}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)$$\n",
    "\n",
    "**Update rule:** Move in the *opposite* direction of the gradient (downhill):\n",
    "$$w_{\\text{new}} = w_{\\text{old}} - \\alpha \\frac{\\partial L}{\\partial w}$$\n",
    "$$b_{\\text{new}} = b_{\\text{old}} - \\alpha \\frac{\\partial L}{\\partial b}$$\n",
    "\n",
    "where $\\alpha$ is the learning rate.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression_gd(X, y, lr=0.1, epochs=100):\n",
    "    \"\"\"\n",
    "    Train linear regression using gradient descent.\n",
    "    \n",
    "    Args:\n",
    "        X: Features, shape (n_samples, 1)\n",
    "        y: Targets, shape (n_samples,)\n",
    "        lr: Learning rate\n",
    "        epochs: Number of iterations\n",
    "    \n",
    "    Returns:\n",
    "        w, b: Learned parameters\n",
    "        history: Loss at each epoch\n",
    "    \"\"\"\n",
    "    # Initialise parameters\n",
    "    w = 0.0\n",
    "    b = 0.0\n",
    "    n = len(y)\n",
    "    history = []\n",
    "    \n",
    "    X_flat = X.squeeze()  # Shape: (n_samples,)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Forward pass: predictions\n",
    "        y_pred = w * X_flat + b\n",
    "        \n",
    "        # Compute loss (MSE)\n",
    "        loss = np.mean((y - y_pred) ** 2)\n",
    "        history.append(loss)\n",
    "        \n",
    "        # Compute gradients (partial derivatives of MSE)\n",
    "        # d(MSE)/dw = -2/n * sum(X * (y - y_pred))\n",
    "        # d(MSE)/db = -2/n * sum(y - y_pred)\n",
    "        dw = (-2/n) * np.sum(X_flat * (y - y_pred))\n",
    "        db = (-2/n) * np.sum(y - y_pred)\n",
    "        \n",
    "        # Update parameters (gradient descent step)\n",
    "        w = w - lr * dw\n",
    "        b = b - lr * db\n",
    "        \n",
    "        if epoch % 20 == 0:\n",
    "            print(f\"Epoch {epoch:3d}: Loss = {loss:.4f}, w = {w:.3f}, b = {b:.3f}\")\n",
    "    \n",
    "    return w, b, history\n",
    "\n",
    "# Train from scratch\n",
    "w, b, history = linear_regression_gd(X_train, y_train, lr=0.1, epochs=100)\n",
    "\n",
    "print(f\"\\nFinal: y = {w:.3f}x + {b:.3f}\")\n",
    "print(f\"sklearn: y = {model.coef_[0]:.3f}x + {model.intercept_:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise gradient descent\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Loss curve\n",
    "axes[0].plot(history)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('MSE Loss')\n",
    "axes[0].set_title('Gradient Descent Convergence')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Compare fits\n",
    "axes[1].scatter(X_test, y_test, alpha=0.5, label='Test data')\n",
    "X_line = np.linspace(X.min(), X.max(), 100)\n",
    "axes[1].plot(X_line, w * X_line + b, colour='red', linewidth=2, label=f'GD: y={w:.2f}x+{b:.2f}')\n",
    "axes[1].plot(X_line, model.coef_[0] * X_line + model.intercept_, \n",
    "             colour='green', linewidth=2, linestyle='--', label='sklearn')\n",
    "axes[1].set_xlabel('X')\n",
    "axes[1].set_ylabel('y')\n",
    "axes[1].set_title('Comparison: Gradient Descent vs sklearn')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Q: What happens if learning rate is too high or too low?</b></summary>\n",
    "\n",
    "**A:**\n",
    "- **Too high**: Loss oscillates or diverges (explodes to infinity). The steps are too big and overshoot the minimum.\n",
    "\n",
    "- **Too low**: Converges very slowly. May get stuck or take forever to train.\n",
    "\n",
    "**Try it**: Change `lr=0.1` to `lr=0.01` (slow) or `lr=1.0` (unstable) and observe.\n",
    "\n",
    "**Rule of thumb**: Start with lr=0.01 or 0.001 for neural networks. Use learning rate schedulers for better results.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 Polynomial Features & Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Generate nonlinear data\n",
    "np.random.seed(42)\n",
    "X_poly = np.random.uniform(-3, 3, 50).reshape(-1, 1)\n",
    "y_poly = 0.5 * X_poly.squeeze()**2 - X_poly.squeeze() + 2 + np.random.randn(50) * 0.5\n",
    "\n",
    "X_train_p, X_test_p, y_train_p, y_test_p = train_test_split(X_poly, y_poly, test_size=0.3, random_state=42)\n",
    "\n",
    "# Fit models of different complexity\n",
    "degrees = [1, 3, 15]\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "X_plot = np.linspace(-3.5, 3.5, 100).reshape(-1, 1)\n",
    "\n",
    "for ax, degree in zip(axes, degrees):\n",
    "    # Create polynomial pipeline\n",
    "    model = make_pipeline(\n",
    "        PolynomialFeatures(degree),\n",
    "        LinearRegression()\n",
    "    )\n",
    "    model.fit(X_train_p, y_train_p)\n",
    "    \n",
    "    # Evaluate\n",
    "    train_mse = mean_squared_error(y_train_p, model.predict(X_train_p))\n",
    "    test_mse = mean_squared_error(y_test_p, model.predict(X_test_p))\n",
    "    \n",
    "    # Plot\n",
    "    ax.scatter(X_train_p, y_train_p, alpha=0.6, label='Train')\n",
    "    ax.scatter(X_test_p, y_test_p, alpha=0.6, label='Test')\n",
    "    ax.plot(X_plot, model.predict(X_plot), colour='red', linewidth=2)\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_title(f'Degree {degree}\\nTrain MSE: {train_mse:.2f}, Test MSE: {test_mse:.2f}')\n",
    "    ax.legend()\n",
    "    ax.set_ylim(-5, 15)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Observe: Degree 15 has LOW train error but HIGH test error = OVERFITTING\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Q: How do you detect overfitting?</b></summary>\n",
    "\n",
    "**A:** Compare train vs test performance:\n",
    "\n",
    "| Scenario | Train Error | Test Error | Diagnosis |\n",
    "|----------|------------|-----------|-----------|\n",
    "| Low | Low | Good fit |\n",
    "| Low | High | **Overfitting** |\n",
    "| High | High | Underfitting |\n",
    "\n",
    "**Solutions for overfitting:**\n",
    "1. More training data\n",
    "2. Simpler model (fewer parameters)\n",
    "3. Regularization (L1/L2)\n",
    "4. Early stopping\n",
    "5. Dropout (for neural networks)\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5 Regularization Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso\n",
    "\n",
    "# Compare regularized models on degree-15 polynomial\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "models = [\n",
    "    ('No Regularization', make_pipeline(PolynomialFeatures(15), LinearRegression())),\n",
    "    ('Ridge (L2)', make_pipeline(PolynomialFeatures(15), Ridge(alpha=1.0))),\n",
    "    ('Lasso (L1)', make_pipeline(PolynomialFeatures(15), Lasso(alpha=0.1))),\n",
    "]\n",
    "\n",
    "for ax, (name, model) in zip(axes, models):\n",
    "    model.fit(X_train_p, y_train_p)\n",
    "    \n",
    "    train_mse = mean_squared_error(y_train_p, model.predict(X_train_p))\n",
    "    test_mse = mean_squared_error(y_test_p, model.predict(X_test_p))\n",
    "    \n",
    "    ax.scatter(X_train_p, y_train_p, alpha=0.6, label='Train')\n",
    "    ax.scatter(X_test_p, y_test_p, alpha=0.6, label='Test')\n",
    "    ax.plot(X_plot, model.predict(X_plot), colour='red', linewidth=2)\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_title(f'{name}\\nTrain: {train_mse:.2f}, Test: {test_mse:.2f}')\n",
    "    ax.legend()\n",
    "    ax.set_ylim(-5, 15)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Ridge/Lasso add penalty terms to prevent overfitting:\")\n",
    "print(\"  Ridge: penalizes large weights (L2 norm)\")\n",
    "print(\"  Lasso: promotes sparsity (L1 norm)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 8: Practical Patterns\n",
    "\n",
    "---\n",
    "\n",
    "## 8.1 Generators & Iterators\n",
    "\n",
    "Generators are crucial for memory-efficient data loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator function - uses yield\n",
    "def count_up_to(n: int):\n",
    "    \"\"\"Generate numbers from 0 to n-1.\"\"\"\n",
    "    i = 0\n",
    "    while i < n:\n",
    "        yield i  # Pauses here, returns value\n",
    "        i += 1\n",
    "\n",
    "# Usage\n",
    "for num in count_up_to(5):\n",
    "    print(num, end=\" \")\n",
    "print()\n",
    "\n",
    "# Generator expression (like list comprehension but lazy)\n",
    "squares_gen = (x**2 for x in range(1000000))  # No memory allocated yet!\n",
    "print(f\"Generator: {squares_gen}\")\n",
    "print(f\"First 5: {[next(squares_gen) for _ in range(5)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why generators matter for DL: memory efficiency\n",
    "import sys\n",
    "\n",
    "# List stores all values in memory\n",
    "big_list = [i**2 for i in range(1000000)]\n",
    "print(f\"List size: {sys.getsizeof(big_list) / 1e6:.1f} MB\")\n",
    "\n",
    "# Generator computes on-demand\n",
    "def big_gen():\n",
    "    for i in range(1000000):\n",
    "        yield i**2\n",
    "\n",
    "gen = big_gen()\n",
    "print(f\"Generator size: {sys.getsizeof(gen)} bytes\")\n",
    "\n",
    "# DataLoader-style batching\n",
    "def batch_generator(data: list, batch_size: int):\n",
    "    \"\"\"Yield batches from data.\"\"\"\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        yield data[i:i + batch_size]\n",
    "\n",
    "data = list(range(100))\n",
    "for batch in batch_generator(data, batch_size=32):\n",
    "    print(f\"Batch: {batch[:3]}... (size {len(batch)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Q: When should you use a generator vs a list?</b></summary>\n",
    "\n",
    "**A:**\n",
    "- **Generator**: When data is large, you only need one pass, or values are computed on-demand\n",
    "- **List**: When you need random access, multiple passes, or the data is small\n",
    "\n",
    "**DataLoaders use generators** because:\n",
    "1. Training data is often huge (can't fit in RAM)\n",
    "2. You only need one batch at a time\n",
    "3. Data can be augmented on-the-fly\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 File I/O with Pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Create paths (cross-platform!)\n",
    "data_dir = Path(\"data\")\n",
    "model_path = data_dir / \"models\" / \"best.pt\"\n",
    "\n",
    "print(f\"Path: {model_path}\")\n",
    "print(f\"Parent: {model_path.parent}\")\n",
    "print(f\"Name: {model_path.name}\")\n",
    "print(f\"Stem: {model_path.stem}\")\n",
    "print(f\"Suffix: {model_path.suffix}\")\n",
    "\n",
    "# Check existence\n",
    "print(f\"\\nExists: {model_path.exists()}\")\n",
    "print(f\"Is file: {model_path.is_file()}\")\n",
    "\n",
    "# Find files\n",
    "current = Path(\".\")\n",
    "print(f\"\\nPython files in current dir: {list(current.glob('*.py'))[:3]}\")\n",
    "print(f\"All .ipynb (recursive): {list(current.glob('**/*.ipynb'))[:3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3 Debugging Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Print debugging with f-strings\n",
    "def debug_forward(x, W):\n",
    "    print(f\"DEBUG: x.shape={x.shape}, W.shape={W.shape}\")\n",
    "    result = x @ W\n",
    "    print(f\"DEBUG: result.shape={result.shape}\")\n",
    "    return result\n",
    "\n",
    "# 2. Assertions - catch bugs early\n",
    "def normalize(x: np.ndarray) -> np.ndarray:\n",
    "    assert x.ndim == 2, f\"Expected 2D array, got {x.ndim}D\"\n",
    "    assert x.shape[0] > 0, \"Empty array\"\n",
    "    return (x - x.mean(axis=0)) / (x.std(axis=0) + 1e-8)\n",
    "\n",
    "# 3. Shape annotations in comments\n",
    "def attention(Q, K, V):\n",
    "    # Q: (batch, heads, seq_len, d_k)\n",
    "    # K: (batch, heads, seq_len, d_k)\n",
    "    # V: (batch, heads, seq_len, d_v)\n",
    "    \n",
    "    scores = Q @ K.transpose(-2, -1)  # (batch, heads, seq_len, seq_len)\n",
    "    weights = scores  # Simplified - normally softmax\n",
    "    output = weights @ V  # (batch, heads, seq_len, d_v)\n",
    "    return output\n",
    "\n",
    "# Test\n",
    "x = np.random.randn(32, 784)\n",
    "W = np.random.randn(784, 128)\n",
    "y = debug_forward(x, W)\n",
    "z = normalize(x)\n",
    "print(f\"\\nNormalized shape: {z.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 9: Summary & Capstone\n",
    "\n",
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "### Python Foundations\n",
    "- Use **type hints** for self-documenting code\n",
    "- Use **comprehensions** for building collections\n",
    "- Use **context managers** (`with`) for resource management\n",
    "\n",
    "### NumPy\n",
    "- **Vectorize** operations - avoid Python loops\n",
    "- **Broadcasting** aligns shapes from the right\n",
    "- **axis=0** collapses rows, **axis=1** collapses columns\n",
    "\n",
    "### Data Handling\n",
    "- **Pandas** for loading and preprocessing tabular data\n",
    "- Always check for **missing values** and handle appropriately\n",
    "- **Scale features** before training ML models\n",
    "\n",
    "### Machine Learning Basics\n",
    "- Always **split** data into train/test sets\n",
    "- **MSE** and **R²** for regression evaluation\n",
    "- Watch for **overfitting**: low train error, high test error\n",
    "- **Regularization** (Ridge/Lasso) prevents overfitting\n",
    "\n",
    "### OOP for DL\n",
    "- **`__call__`** makes objects callable (used by `nn.Module`)\n",
    "- **Inheritance** is fundamental to PyTorch model building\n",
    "\n",
    "### Practical Patterns\n",
    "- **Generators** for memory-efficient iteration (DataLoaders!)\n",
    "- **Pathlib** for cross-platform file paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Assessment Checklist\n",
    "\n",
    "Before proceeding to Lab 2, you should be able to:\n",
    "\n",
    "- [ ] Write a function with type hints and a Google-style docstring\n",
    "- [ ] Predict the output shape of broadcasting `(3,1) + (4,)`\n",
    "- [ ] Load a CSV file with pandas and handle missing values\n",
    "- [ ] Split data into train/test sets using sklearn\n",
    "- [ ] Train a linear regression model and compute MSE\n",
    "- [ ] Explain why high train accuracy + low test accuracy = overfitting\n",
    "- [ ] Explain why `__call__` is used in PyTorch modules\n",
    "- [ ] Write a generator function with `yield`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capstone Exercise: End-to-End ML Pipeline\n",
    "\n",
    "Build a complete pipeline from data loading to model evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capstone Exercise: Complete the pipeline\n",
    "\n",
    "class MLPipeline:\n",
    "    \"\"\"\n",
    "    End-to-end ML pipeline.\n",
    "    \n",
    "    TODO: Implement the following methods:\n",
    "    1. load_data: Load wine dataset from sklearn\n",
    "    2. preprocess: Handle missing values, scale features\n",
    "    3. split: Train/test split\n",
    "    4. train: Fit a Ridge regression model\n",
    "    5. evaluate: Return MSE and R² on test set\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.scaler = None\n",
    "        self.model = None\n",
    "        self.X_train = None\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "    \n",
    "    def load_data(self):\n",
    "        # TODO: Load wine dataset, use first feature as target for regression\n",
    "        pass\n",
    "    \n",
    "    def preprocess(self, X):\n",
    "        # TODO: Scale features using StandardScaler\n",
    "        # Remember: fit on train, transform on both train and test\n",
    "        pass\n",
    "    \n",
    "    def split(self, X, y, test_size=0.2):\n",
    "        # TODO: Split into train/test\n",
    "        pass\n",
    "    \n",
    "    def train(self):\n",
    "        # TODO: Fit Ridge regression\n",
    "        pass\n",
    "    \n",
    "    def evaluate(self):\n",
    "        # TODO: Return dict with 'mse' and 'r2' on test set\n",
    "        pass\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"Run the full pipeline.\"\"\"\n",
    "        X, y = self.load_data()\n",
    "        self.split(X, y)\n",
    "        self.X_train = self.preprocess(self.X_train)\n",
    "        self.X_test = self.preprocess(self.X_test)\n",
    "        self.train()\n",
    "        return self.evaluate()\n",
    "\n",
    "# Test your implementation:\n",
    "# pipeline = MLPipeline()\n",
    "# results = pipeline.run()\n",
    "# print(f\"Test MSE: {results['mse']:.4f}\")\n",
    "# print(f\"Test R²: {results['r2']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solution</b></summary>\n",
    "\n",
    "```python\n",
    "class MLPipeline:\n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.model = Ridge(alpha=1.0)\n",
    "        self.X_train = None\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "        self._fitted = False\n",
    "    \n",
    "    def load_data(self):\n",
    "        wine = load_wine()\n",
    "        X = wine.data[:, 1:]  # Features (all but first)\n",
    "        y = wine.data[:, 0]   # Target (first column: alcohol)\n",
    "        return X, y\n",
    "    \n",
    "    def preprocess(self, X):\n",
    "        if not self._fitted:\n",
    "            self._fitted = True\n",
    "            return self.scaler.fit_transform(X)\n",
    "        return self.scaler.transform(X)\n",
    "    \n",
    "    def split(self, X, y, test_size=0.2):\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=42\n",
    "        )\n",
    "    \n",
    "    def train(self):\n",
    "        self.model.fit(self.X_train, self.y_train)\n",
    "    \n",
    "    def evaluate(self):\n",
    "        y_pred = self.model.predict(self.X_test)\n",
    "        return {\n",
    "            'mse': mean_squared_error(self.y_test, y_pred),\n",
    "            'r2': r2_score(self.y_test, y_pred)\n",
    "        }\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 10: Software Engineering Essentials\n",
    "\n",
    "Professional data science requires more than just modeling skills. This section covers essential tools and practices.\n",
    "\n",
    "> **Prerequisite Course**: For data structures and algorithms foundations, see the [DSA Lab Course](https://github.com/henrycgbaker/data-structures-algorithms-lab-2025-TEACHING).\n",
    "\n",
    "---\n",
    "\n",
    "## 10.1 Python Package Management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Virtual Environments\n",
    "\n",
    "Always use virtual environments to isolate project dependencies:\n",
    "\n",
    "```bash\n",
    "# Using venv (built-in)\n",
    "python -m venv .venv\n",
    "source .venv/bin/activate  # Linux/Mac\n",
    ".venv\\Scripts\\activate   # Windows\n",
    "\n",
    "# Using conda\n",
    "conda create -n myproject python=3.10\n",
    "conda activate myproject\n",
    "```\n",
    "\n",
    "### requirements.txt vs pyproject.toml\n",
    "\n",
    "**requirements.txt** (traditional):\n",
    "```\n",
    "numpy>=1.20.0\n",
    "pandas>=1.3.0\n",
    "torch>=2.0.0\n",
    "```\n",
    "\n",
    "**pyproject.toml** (modern, recommended):\n",
    "```toml\n",
    "[project]\n",
    "name = \"my-dl-project\"\n",
    "version = \"0.1.0\"\n",
    "requires-python = \">=3.10\"\n",
    "dependencies = [\n",
    "    \"numpy>=1.20.0\",\n",
    "    \"pandas>=1.3.0\",\n",
    "    \"torch>=2.0.0\",\n",
    "]\n",
    "\n",
    "[project.optional-dependencies]\n",
    "dev = [\"pytest\", \"ruff\", \"mypy\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poetry (Recommended for Projects)\n",
    "\n",
    "[Poetry](https://python-poetry.org/) provides dependency management and packaging:\n",
    "\n",
    "```bash\n",
    "# Install poetry\n",
    "curl -sSL https://install.python-poetry.org | python3 -\n",
    "\n",
    "# Create new project\n",
    "poetry new my-project\n",
    "cd my-project\n",
    "\n",
    "# Add dependencies\n",
    "poetry add numpy pandas torch\n",
    "poetry add --group dev pytest ruff\n",
    "\n",
    "# Install all dependencies\n",
    "poetry install\n",
    "\n",
    "# Run commands in virtual environment\n",
    "poetry run python train.py\n",
    "poetry run pytest\n",
    "\n",
    "# Export to requirements.txt (for deployment)\n",
    "poetry export -f requirements.txt --output requirements.txt\n",
    "```\n",
    "\n",
    "**Why Poetry?**\n",
    "- Lock file ensures reproducible builds\n",
    "- Separates dev and production dependencies\n",
    "- Handles version conflicts automatically\n",
    "- Modern pyproject.toml format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Q: When should you use pip vs conda vs poetry?</b></summary>\n",
    "\n",
    "**A:**\n",
    "| Tool | Best For |\n",
    "|------|----------|\n",
    "| **pip** | Simple scripts, quick prototypes |\n",
    "| **conda** | Scientific computing, GPU libraries, cross-language deps |\n",
    "| **poetry** | Production projects, packages you'll distribute |\n",
    "\n",
    "**Rule of thumb**: \n",
    "- Colab/quick experiments → pip\n",
    "- Complex ML environments → conda\n",
    "- Serious projects → poetry\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2 Code Quality & Pre-commit Hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-commit hooks run checks automatically before each commit\n",
    "# Install: pip install pre-commit\n",
    "\n",
    "# Example .pre-commit-config.yaml:\n",
    "pre_commit_config = \"\"\"\n",
    "repos:\n",
    "  - repo: https://github.com/astral-sh/ruff-pre-commit\n",
    "    rev: v0.1.6\n",
    "    hooks:\n",
    "      - id: ruff          # Linting\n",
    "        args: [--fix]\n",
    "      - id: ruff-format   # Formatting\n",
    "  \n",
    "  - repo: https://github.com/pre-commit/pre-commit-hooks\n",
    "    rev: v4.5.0\n",
    "    hooks:\n",
    "      - id: trailing-whitespace\n",
    "      - id: end-of-file-fixer\n",
    "      - id: check-yaml\n",
    "      - id: check-added-large-files\n",
    "\"\"\"\n",
    "\n",
    "print(\"Save this as .pre-commit-config.yaml in your repo root\")\n",
    "print(\"Then run: pre-commit install\")\n",
    "print(\"Now every 'git commit' will auto-format and lint your code!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ruff: Modern Python Linter\n",
    "\n",
    "[Ruff](https://github.com/astral-sh/ruff) is extremely fast and replaces multiple tools:\n",
    "\n",
    "```bash\n",
    "# Install\n",
    "pip install ruff\n",
    "\n",
    "# Lint (check for issues)\n",
    "ruff check .\n",
    "\n",
    "# Fix auto-fixable issues\n",
    "ruff check --fix .\n",
    "\n",
    "# Format (like black)\n",
    "ruff format .\n",
    "```\n",
    "\n",
    "**pyproject.toml configuration:**\n",
    "```toml\n",
    "[tool.ruff]\n",
    "line-length = 100\n",
    "target-version = \"py310\"\n",
    "\n",
    "[tool.ruff.lint]\n",
    "select = [\"E\", \"F\", \"I\", \"UP\"]  # Error, pyflakes, isort, pyupgrade\n",
    "ignore = [\"E501\"]  # Line too long (handled by formatter)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3 GitHub Actions (CI/CD)\n",
    "\n",
    "Automatically run tests and checks on every push:\n",
    "\n",
    "```yaml\n",
    "# .github/workflows/ci.yml\n",
    "name: CI\n",
    "\n",
    "on: [push, pull_request]\n",
    "\n",
    "jobs:\n",
    "  test:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - uses: actions/checkout@v4\n",
    "      \n",
    "      - name: Set up Python\n",
    "        uses: actions/setup-python@v5\n",
    "        with:\n",
    "          python-version: '3.10'\n",
    "      \n",
    "      - name: Install dependencies\n",
    "        run: |\n",
    "          pip install -r requirements.txt\n",
    "          pip install pytest ruff\n",
    "      \n",
    "      - name: Lint with ruff\n",
    "        run: ruff check .\n",
    "      \n",
    "      - name: Run tests\n",
    "        run: pytest tests/\n",
    "```\n",
    "\n",
    "This ensures:\n",
    "- Every PR is automatically tested\n",
    "- Code style is enforced\n",
    "- Broken code can't be merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Q: Why copy requirements.txt before copying code in Dockerfile?</b></summary>\n",
    "\n",
    "**A:** Docker caches each layer. If you copy code first, ANY code change invalidates the cache for `pip install`. By copying requirements.txt first:\n",
    "- Requirements layer is cached if dependencies are unchanged\n",
    "- Code changes only rebuild the final copy layer\n",
    "\n",
    "This can save minutes on each build when dependencies are stable.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><b>Q: What's the difference between `poetry install` and `pip install -r requirements.txt`?</b></summary>\n",
    "\n",
    "**A:**\n",
    "- **`poetry install`**: Uses lock file (`poetry.lock`) for exact versions. Creates isolated virtual environment. Handles dependency resolution.\n",
    "\n",
    "- **`pip install -r`**: Uses version ranges from requirements.txt. May get different versions on different machines. No built-in environment management.\n",
    "\n",
    "Poetry is more reproducible; pip is simpler for quick setups.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.4 Docker Basics\n",
    "\n",
    "Docker containers ensure your code runs the same everywhere.\n",
    "\n",
    "> **Full Docker Course**: See [DS Hub Docker Guide](https://github.com/hertie-data-science-lab/ds01-hub/tree/main)\n",
    "\n",
    "### Essential Dockerfile for ML\n",
    "\n",
    "```dockerfile\n",
    "# Use official Python image\n",
    "FROM python:3.10-slim\n",
    "\n",
    "# Set working directory\n",
    "WORKDIR /app\n",
    "\n",
    "# Copy requirements first (for caching)\n",
    "COPY requirements.txt .\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Copy code\n",
    "COPY . .\n",
    "\n",
    "# Run training script\n",
    "CMD [\"python\", \"train.py\"]\n",
    "```\n",
    "\n",
    "### Common Commands\n",
    "\n",
    "```bash\n",
    "# Build image\n",
    "docker build -t my-ml-project .\n",
    "\n",
    "# Run container\n",
    "docker run my-ml-project\n",
    "\n",
    "# Run with GPU (NVIDIA)\n",
    "docker run --gpus all my-ml-project\n",
    "\n",
    "# Interactive shell\n",
    "docker run -it my-ml-project /bin/bash\n",
    "\n",
    "# Mount local directory\n",
    "docker run -v $(pwd)/data:/app/data my-ml-project\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Docker Compose for Multi-Container Apps\n",
    "\n",
    "```yaml\n",
    "# docker-compose.yml\n",
    "version: '3.8'\n",
    "services:\n",
    "  training:\n",
    "    build: .\n",
    "    volumes:\n",
    "      - ./data:/app/data\n",
    "      - ./models:/app/models\n",
    "    deploy:\n",
    "      resources:\n",
    "        reservations:\n",
    "          devices:\n",
    "            - driver: nvidia\n",
    "              count: 1\n",
    "              capabilities: [gpu]\n",
    "  \n",
    "  tensorboard:\n",
    "    image: tensorflow/tensorflow\n",
    "    ports:\n",
    "      - \"6006:6006\"\n",
    "    volumes:\n",
    "      - ./logs:/logs\n",
    "    command: tensorboard --logdir=/logs --host=0.0.0.0\n",
    "```\n",
    "\n",
    "```bash\n",
    "# Start all services\n",
    "docker-compose up\n",
    "\n",
    "# Run in background\n",
    "docker-compose up -d\n",
    "\n",
    "# Stop all\n",
    "docker-compose down\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Q: When should you use Docker for ML projects?</b></summary>\n",
    "\n",
    "**A:** Use Docker when:\n",
    "- **Reproducibility matters**: Ensure exact same environment\n",
    "- **Deployment**: Serving models in production\n",
    "- **Collaboration**: Share exact environments with team\n",
    "- **GPU clusters**: Many HPC systems require containers\n",
    "\n",
    "**Skip Docker when:**\n",
    "- Quick experiments in Colab\n",
    "- Simple scripts with few dependencies\n",
    "- Learning/prototyping phase\n",
    "\n",
    "**Rule**: Start without Docker, add it when you need reproducibility or deployment.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.5 Project Structure\n",
    "\n",
    "Recommended structure for ML projects:\n",
    "\n",
    "```\n",
    "my-ml-project/\n",
    "├── .github/\n",
    "│   └── workflows/\n",
    "│       └── ci.yml          # GitHub Actions\n",
    "├── data/\n",
    "│   ├── raw/                # Original data (gitignored)\n",
    "│   └── processed/          # Cleaned data\n",
    "├── models/                 # Saved model checkpoints\n",
    "├── notebooks/              # Jupyter notebooks for exploration\n",
    "├── src/\n",
    "│   ├── __init__.py\n",
    "│   ├── data.py            # Data loading/preprocessing\n",
    "│   ├── model.py           # Model architecture\n",
    "│   ├── train.py           # Training loop\n",
    "│   └── evaluate.py        # Evaluation metrics\n",
    "├── tests/\n",
    "│   └── test_model.py      # Unit tests\n",
    "├── .gitignore\n",
    "├── .pre-commit-config.yaml\n",
    "├── pyproject.toml         # Dependencies & config\n",
    "├── README.md\n",
    "└── Dockerfile\n",
    "```\n",
    "\n",
    "**Key principles:**\n",
    "- Separate code (src/) from experiments (notebooks/)\n",
    "- Never commit raw data or model weights to git\n",
    "- Use pyproject.toml for all configuration\n",
    "- Write tests for critical functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.6 Further Resources\n",
    "\n",
    "### Courses & Links\n",
    "- [DSA Lab Course](https://github.com/henrycgbaker/data-structures-algorithms-lab-2025-TEACHING) - Data structures & algorithms\n",
    "- [DS Hub Docker Guide](https://github.com/hertie-data-science-lab/ds01-hub/tree/main) - Docker for data science\n",
    "- [Poetry Documentation](https://python-poetry.org/docs/)\n",
    "- [GitHub Actions Guide](https://docs.github.com/en/actions)\n",
    "- [Ruff Documentation](https://docs.astral.sh/ruff/)\n",
    "\n",
    "### Books\n",
    "- *The Good Research Code Handbook* - Patrick Mineault\n",
    "- *Software Engineering for Data Scientists* - Andrew Trevett"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "### Python & NumPy\n",
    "1. [Python Type Hints Cheat Sheet](https://mypy.readthedocs.io/en/stable/cheat_sheet_py3.html)\n",
    "2. [NumPy Broadcasting Rules](https://numpy.org/doc/stable/user/basics.broadcasting.html)\n",
    "3. [Google Python Style Guide](https://google.github.io/styleguide/pyguide.html)\n",
    "\n",
    "### Data Science & ML\n",
    "4. [Pandas Documentation](https://pandas.pydata.org/docs/)\n",
    "5. [sklearn User Guide](https://scikit-learn.org/stable/user_guide.html)\n",
    "6. [PyTorch nn.Module Source](https://github.com/pytorch/pytorch/blob/main/torch/nn/modules/module.py)\n",
    "\n",
    "### Software Engineering\n",
    "7. [DSA Lab Course](https://github.com/henrycgbaker/data-structures-algorithms-lab-2025-TEACHING) - Prerequisites\n",
    "8. [DS Hub Docker Guide](https://github.com/hertie-data-science-lab/ds01-hub/tree/main)\n",
    "9. [Poetry Documentation](https://python-poetry.org/docs/)\n",
    "10. [Ruff Documentation](https://docs.astral.sh/ruff/)\n",
    "11. [GitHub Actions Guide](https://docs.github.com/en/actions)\n",
    "\n",
    "---\n",
    "\n",
    "**Next:** Lab 2 - Introduction to Feedforward Neural Networks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
